<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CLIP: Contrastive Vision-Language Learning</title>
  <style>
    body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:1200px;margin:0 auto;padding:20px;background:linear-gradient(135deg,#1a1a1a 0%,#2d2d2d 100%);color:#e0e0e0;line-height:1.6}
    .container{background:#fff;color:#2d2d2d;border-radius:20px;padding:30px;margin:20px 0;border:1px solid #e0e0e0;box-shadow:0 4px 20px rgba(0,0,0,.1)}
    .nav-bar{background:#2d2d2d;color:#fff;padding:15px 30px;border-radius:15px;margin:20px 0;display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:15px}
    .nav-home,.nav-prev,.nav-next{background:#fff;color:#2d2d2d;padding:8px 16px;border-radius:6px;text-decoration:none;font-weight:bold;transition:all .3s}
    .nav-next{background:#28a745;color:#fff}
    .nav-home:hover,.nav-prev:hover,.nav-next:hover{transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    .nav-title{font-size:1.2em;font-weight:bold;flex:1;min-width:300px}
    .step{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;margin:15px 0;border-radius:15px;border-left:4px solid #2d2d2d}
    .interactive-demo{background:#f8f9fa;border:2px solid #e9ecef;border-radius:15px;padding:25px;margin:20px 0}
    .demo-title{font-size:1.3em;font-weight:bold;margin-bottom:20px;text-align:center;color:#2d2d2d}
    .controls{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .control-group{display:flex;flex-direction:column;gap:5px}
    .control-group label{font-weight:bold;font-size:14px;color:#2d2d2d}
    button{background:#2d2d2d;border:none;color:#fff;padding:12px 24px;border-radius:8px;cursor:pointer;font-weight:bold;transition:all .3s;margin:5px}
    button:hover{background:#1a1a1a;transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    button.primary{background:#28a745}
    button.primary:hover{background:#218838}
    input,select,textarea{padding:8px 12px;border:1px solid #dadce0;border-radius:6px;background:#fff;color:#2d2d2d}
    textarea{min-height:70px;resize:vertical}
    .math-formula{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;text-align:center;font-size:16px;box-shadow:0 2px 8px rgba(0,0,0,.1)}
    .code-block{background:#2d2d2d;color:#e0e0e0;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;font-size:14px;overflow:auto;position:relative}
    .code-header{background:#1a1a1a;color:#28a745;padding:8px 15px;margin:-20px -20px 15px -20px;border-radius:8px 8px 0 0;font-weight:bold;font-size:12px}
    .copy-button{position:absolute;top:10px;right:10px;background:#28a745;color:#fff;border:none;padding:4px 8px;border-radius:4px;cursor:pointer;font-size:10px}
    .similarity-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(120px,1fr));gap:10px;margin:15px 0;padding:15px;background:#fff;border-radius:8px;border:1px solid #dee2e6}
    .similarity-cell{background:#f8f9fa;border:2px solid #e9ecef;border-radius:8px;padding:10px;text-align:center;font-size:12px;transition:all .3s}
    .similarity-cell:hover{transform:scale(1.05)}
    .embedding-space{width:100%;height:400px;background:#fff;border:2px solid #e9ecef;border-radius:8px;margin:15px 0;position:relative;overflow:hidden}
    .embedding-canvas{width:100%;height:100%}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(150px,1fr));gap:15px;margin:20px 0}
    .metric-card{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center}
    .metric-value{font-size:1.5em;font-weight:bold;color:#2d2d2d;margin-bottom:5px}
    .metric-label{font-size:12px;color:#666}
    .architecture-flow{display:flex;justify-content:space-between;align-items:center;margin:20px 0;padding:20px;background:#fff;border-radius:12px;border:2px solid #e9ecef;flex-wrap:wrap;gap:15px}
    .arch-component{background:#f8f9fa;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center;min-width:120px;flex:1}
    .arch-component.vision{border-color:#dc3545}
    .arch-component.text{border-color:#007bff}
    .arch-component.shared{border-color:#28a745}
    .arch-arrow{font-size:24px;color:#28a745;font-weight:bold}
    .contrastive-matrix{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;margin:15px 0}
    .matrix-grid{display:grid;gap:2px;margin:10px 0}
    .matrix-cell{padding:8px;text-align:center;font-size:12px;border-radius:4px;font-weight:bold}
    .matrix-positive{background:#d4edda;color:#155724}
    .matrix-negative{background:#f8d7da;color:#721c24}
    .matrix-header{background:#e9ecef;color:#495057;font-weight:bold}
    .training-simulation{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:20px;margin:15px 0}
    .loss-chart{width:100%;height:300px;background:#f8f9fa;border:1px solid #dee2e6;border-radius:4px;margin:10px 0}
    .dataset-preview{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .dataset-item{background:#f8f9fa;border:1px solid #dee2e6;border-radius:8px;padding:15px}
    .dataset-image{width:100%;height:120px;background:linear-gradient(45deg,#e9ecef,#dee2e6);border-radius:4px;display:flex;align-items:center;justify-content:center;margin-bottom:10px;font-weight:bold;color:#495057}
    .dataset-text{font-size:12px;color:#666;font-style:italic}
    .info{background:#d1ecf1;border-left:4px solid #17a2b8;color:#0c5460;padding:15px;border-radius:8px;margin:15px 0}
    .success{background:#d4edda;border-left:4px solid #28a745;color:#155724;padding:15px;border-radius:8px;margin:15px 0}
    .warning{background:#fff3cd;border-left:4px solid #ffc107;color:#856404;padding:15px;border-radius:8px;margin:15px 0}
    .breakthrough-highlight{background:linear-gradient(135deg,#28a745,#20c997);color:#fff;padding:20px;border-radius:12px;text-align:center;margin:20px 0;font-size:18px;font-weight:bold;box-shadow:0 4px 15px rgba(40,167,69,.3)}
    .tabs{display:flex;margin-bottom:20px;border-bottom:2px solid #e9ecef}
    .tab{padding:12px 24px;cursor:pointer;border-bottom:3px solid transparent;font-weight:bold;transition:all .3s}
    .tab:hover{background:#f8f9fa}
    .tab.active{border-bottom-color:#28a745;background:#d4edda;color:#155724}
    .tab-content{display:none}
    .tab-content.active{display:block}
    .zero-shot-demo{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0}
    .class-options{display:grid;grid-template-columns:repeat(auto-fit,minmax(120px,1fr));gap:8px;margin:15px 0}
    .class-option{padding:8px 12px;background:#f8f9fa;border:2px solid #e9ecef;border-radius:6px;cursor:pointer;text-align:center;font-size:12px;transition:all .3s}
    .class-option:hover{background:#e9ecef}
    .class-option.selected{background:#d4edda;border-color:#28a745;color:#155724}
    .progress-bar{width:100%;height:20px;background:#e9ecef;border-radius:10px;overflow:hidden;margin:10px 0}
    .progress-fill{height:100%;background:linear-gradient(135deg,#28a745,#20c997);transition:width 1s ease;display:flex;align-items:center;justify-content:center;color:#fff;font-size:11px;font-weight:bold}
    @keyframes pulse{0%{transform:scale(1);opacity:.8}50%{transform:scale(1.05);opacity:1}100%{transform:scale(1);opacity:.8}}
    .training-animation{animation:pulse 2s ease-in-out infinite}
    .temperature-slider{margin:15px 0}
    .temperature-display{font-weight:bold;color:#28a745}
  </style>
</head>
<body>
  <div class="nav-bar">
    <div class="nav-title">üîó CLIP: Contrastive Vision-Language Learning</div>
    <a href="index.html" class="nav-home">üè† Home</a>
    <a href="training-finetuning-vits.html" class="nav-prev">‚Üê Training ViTs</a>
    <a href="vision-language-models.html" class="nav-next">Next: Vision-Language Models ‚Üí</a>
  </div>

  <div class="container">
    <h1>üîó CLIP: Connecting Vision and Language</h1>
    <p>CLIP (Contrastive Language-Image Pre-training) revolutionized AI by learning to connect images and text in a shared embedding space. Unlike traditional vision models trained on fixed categories, CLIP learns from natural language descriptions, enabling <strong>zero-shot classification</strong>, <strong>image search with text</strong>, and the foundation for modern multimodal AI systems like GPT-4V and DALL-E.</p>
    
    <div class="breakthrough-highlight">
      üèÜ CLIP's Revolutionary Impact: First model to achieve human-level performance on ImageNet zero-shot classification using only natural language supervision!
    </div>
  </div>

  <div class="container">
    <h2>üèóÔ∏è CLIP Architecture: Two Towers, One Goal</h2>
    <div class="step">
      <h3>üéØ The Core Architecture</h3>
      <p>CLIP uses a <strong>"dual encoder"</strong> architecture with separate but parallel processing towers for images and text. Both encoders map their inputs to the same high-dimensional space where similar concepts cluster together.</p>

      <div class="architecture-flow">
        <div class="arch-component vision">
          <h4>üñºÔ∏è Image Encoder</h4>
          <div>Vision Transformer</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ ViT-B/32, ViT-B/16, or ViT-L/14<br>
            ‚Ä¢ ResNet variants also supported<br>
            ‚Ä¢ Output: 512D or 768D vector
          </div>
        </div>
        
        <div class="arch-arrow">‚Üí</div>
        
        <div class="arch-component shared">
          <h4>üåê Shared Space</h4>
          <div>Joint Embedding</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Same dimensionality<br>
            ‚Ä¢ L2 normalized vectors<br>
            ‚Ä¢ Cosine similarity
          </div>
        </div>
        
        <div class="arch-arrow">‚Üê</div>
        
        <div class="arch-component text">
          <h4>üìù Text Encoder</h4>
          <div>Transformer</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ 12-layer Transformer<br>
            ‚Ä¢ 63M parameters<br>
            ‚Ä¢ Output: same 512D/768D
          </div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üèóÔ∏è Interactive Architecture Explorer</div>
        
        <div class="controls">
          <div class="control-group">
            <label>CLIP Model Variant:</label>
            <select id="clipVariant">
              <option value="vit-b-32" selected>ViT-B/32 (Fastest)</option>
              <option value="vit-b-16">ViT-B/16 (Balanced)</option>
              <option value="vit-l-14">ViT-L/14 (Best Quality)</option>
              <option value="resnet-50">ResNet-50 (Baseline)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Embedding Dimension:</label>
            <select id="embeddingDim">
              <option value="512" selected>512D (Standard)</option>
              <option value="768">768D (Large)</option>
              <option value="1024">1024D (Huge)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Analysis Type:</label>
            <select id="analysisType">
              <option value="parameters" selected>Parameter Count</option>
              <option value="memory">Memory Usage</option>
              <option value="speed">Inference Speed</option>
            </select>
          </div>
        </div>

        <button onclick="analyzeArchitecture()">üîç Analyze Architecture</button>
        <div id="architectureAnalysis"></div>
      </div>
    </div>

<div class="step">
      <h3>üßÆ Mathematical Foundation: Joint Embedding Space</h3>
      <p><strong>The Big Idea:</strong> CLIP's revolutionary insight is creating a <strong>shared mathematical space</strong> where both images and text live as vectors. Think of it like a universal translator that converts both "a photo of a dog" and an actual dog photo into the same mathematical language - vectors of numbers that can be directly compared.</p>
      
      <div class="info">
        <strong>üéØ Why Joint Embeddings Matter:</strong><br>
        Before CLIP, image models output "dog, cat, bird" categories, while text models output word vectors. These lived in completely different mathematical universes and couldn't talk to each other. CLIP creates one shared space where "dog" (text) and üêï (image) become nearly identical vectors.
      </div>

<div class="step">
      <h3>üßÆ Mathematical Foundation: Joint Embedding Space</h3>
      <p><strong>The Big Idea:</strong> CLIP's revolutionary insight is creating a <strong>shared mathematical space</strong> where both images and text live as vectors. Think of it like a universal translator that converts both "a photo of a dog" and an actual dog photo into the same mathematical language - vectors of numbers that can be directly compared.</p>
      
      <div class="info">
        <strong>üéØ Why Joint Embeddings Matter:</strong><br>
        Before CLIP, image models output "dog, cat, bird" categories, while text models output word vectors. These lived in completely different mathematical universes and couldn't talk to each other. CLIP creates one shared space where "dog" (text) and üêï (image) become nearly identical vectors.
      </div>

      <div class="math-formula">
        <strong>Step-by-Step: From Images and Text to Comparable Vectors</strong><br><br>
        
        <strong>Step 1 - Encoding (Translation to Vectors):</strong><br>
        I = f<sub>image</sub>(x<sub>img</sub>) ‚àà ‚Ñù<sup>d</sup> &nbsp;&nbsp;&nbsp; T = f<sub>text</sub>(x<sub>txt</sub>) ‚àà ‚Ñù<sup>d</sup><br>
        <em>Transform a dog photo ‚Üí [0.2, -0.5, 0.8, ...] (512 numbers)</em><br>
        <em>Transform "a dog" text ‚Üí [0.3, -0.4, 0.9, ...] (same 512 numbers)</em><br><br>
        
        <strong>Step 2 - Normalization (Make All Vectors Same Length):</strong><br>
        √é = I / ||I||<sub>2</sub> &nbsp;&nbsp;&nbsp; TÃÇ = T / ||T||<sub>2</sub><br>
        <em>Why: So similarity only depends on direction, not magnitude</em><br>
        <em>Like: All arrows point from origin but have length = 1</em><br><br>
        
        <strong>Step 3 - Similarity (How Alike Are They?):</strong><br>
        s<sub>ij</sub> = √é<sub>i</sub> ¬∑ TÃÇ<sub>j</sub> = cos(Œ∏<sub>ij</sub>)<br>
        <em>Dot product = cosine similarity (angle between vectors)</em><br>
        <em>+1 = identical, 0 = unrelated, -1 = opposite</em><br><br>
        
        <strong>Step 4 - Temperature Scaling (Fine-tune Confidence):</strong><br>
        logit<sub>ij</sub> = s<sub>ij</sub> / œÑ<br><br>
        
        <strong>üî• Deep Dive: What This Division Actually Does</strong><br>
        <em>Logits = "raw scores" that get fed into softmax to produce probabilities</em><br><br>
        
        <strong>Example with œÑ = 0.07 (CLIP's magic number):</strong><br>
        ‚Ä¢ Similarity 0.8 ‚Üí Logit 0.8 √∑ 0.07 = <strong>11.4</strong> (amplified!)<br>
        ‚Ä¢ Similarity 0.3 ‚Üí Logit 0.3 √∑ 0.07 = <strong>4.3</strong><br>
        ‚Ä¢ Similarity 0.0 ‚Üí Logit 0.0 √∑ 0.07 = <strong>0</strong><br>
        ‚Ä¢ Similarity -0.2 ‚Üí Logit -0.2 √∑ 0.07 = <strong>-2.9</strong><br><br>
        
        <strong>üéØ The Amplification Effect:</strong><br>
        Small œÑ = Amplifies differences ‚Üí "I'm confident it's a dog!"<br>
        Large œÑ = Dampens differences ‚Üí "Could be dog... or cat... unsure"<br><br>
        
        <strong>üéõÔ∏è Think of œÑ as CLIP's "confidence dial":</strong><br>
        ‚Ä¢ œÑ = 0.01: üì¢ Overconfident (99.99% sure)<br>
        ‚Ä¢ œÑ = 0.07: ‚úÖ Just right (87% confident)<br>
        ‚Ä¢ œÑ = 0.5: ü§∑ Wishy-washy (40% vs 30% vs 20%)<br><br>
        
        <strong>üèÜ Why œÑ = 0.07 works:</strong> Strong gradients + good calibration + clear decisions without overconfidence
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üßÆ Vector Mathematics Playground</div>
        <p><strong>See the math in action:</strong> Enter different similarity values and see how normalization and temperature scaling affect the final predictions. This is exactly what happens inside CLIP millions of times during training!</p>

        <div class="controls">
          <div class="control-group">
            <label>Raw Image Vector (4D simplified):</label>
            <input type="text" id="rawImageVector" value="2.0, -1.5, 3.0, 1.0" placeholder="e.g. 2.0, -1.5, 3.0, 1.0">
            <small style="display:block;color:#666;margin-top:3px">In reality, CLIP uses 512 dimensions. Here we use 4 for visualization.</small>
          </div>
          <div class="control-group">
            <label>Raw Text Vector (4D simplified):</label>
            <input type="text" id="rawTextVector" value="1.8, -1.2, 2.9, 0.8" placeholder="e.g. 1.8, -1.2, 2.9, 0.8">
            <small style="display:block;color:#666;margin-top:3px">Similar vectors = similar concepts. Try making them more/less similar!</small>
          </div>
          <div class="control-group">
            <label>Temperature (œÑ):</label>
            <input type="range" id="mathTemperature" min="0.01" max="0.2" value="0.07" step="0.01">
            <span id="mathTempValue" class="temperature-display">0.07</span>
            <small style="display:block;color:#666;margin-top:3px">Lower = more confident predictions, higher = more uncertain predictions.</small>
          </div>
        </div>

        <button onclick="calculateVectorMath()" class="primary">üßÆ Calculate Step-by-Step</button>
        <div id="vectorMathResults"></div>
      </div>

      <div class="step" style="background: #fff3cd; border-left-color: #ffc107;">
        <h4>ü§î Why L2 Normalization? The Geometric Insight</h4>
        <p><strong>The Problem:</strong> Without normalization, a vector like [1000, 2000, 3000] would have much higher similarity scores than [1, 2, 3] even if they point in the exact same direction!</p>
        
        <div class="math-formula">
          <strong>L2 Norm (Vector Length):</strong><br>
          ||v||<sub>2</sub> = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + v‚ÇÉ¬≤ + ... + v_d¬≤)<br><br>
          
          <strong>L2 Normalization:</strong><br>
          vÃÇ = v / ||v||<sub>2</sub><br><br>
          
          <strong>Result:</strong> All vectors have length = 1, so similarity only depends on <em>direction</em><br>
          <strong>Geometric meaning:</strong> All points lie on a unit hypersphere (circle in 2D, sphere in 3D, hypersphere in 512D)
        </div>

        <div class="interactive-demo">
          <div class="demo-title">üìê Normalization Visualizer</div>
          
          <div class="controls">
            <div class="control-group">
              <label>Vector A (before normalization):</label>
              <input type="text" id="vectorA" value="3, 4" placeholder="e.g. 3, 4">
            </div>
            <div class="control-group">
              <label>Vector B (before normalization):</label>
              <input type="text" id="vectorB" value="30, 40" placeholder="e.g. 30, 40">
            </div>
          </div>

          <button onclick="visualizeNormalization()">üìê Visualize Normalization Effect</button>
          <div id="normalizationViz"></div>
        </div>
      </div>

      <div class="step" style="background: #d1ecf1; border-left-color: #17a2b8;">
        <h4>üå°Ô∏è Temperature Scaling: The Confidence Knob</h4>
        <p><strong>What it does:</strong> Temperature (œÑ) controls how "confident" or "uncertain" the model's predictions become. It's like adjusting the contrast on a photo - lower temperature = higher contrast (more decisive), higher temperature = lower contrast (more uncertain).</p>

        <div class="math-formula">
          <strong>Temperature Effect on Softmax:</strong><br><br>
          
          <strong>Without temperature:</strong> p<sub>i</sub> = exp(s<sub>i</sub>) / ‚àë exp(s<sub>j</sub>)<br>
          <strong>With temperature:</strong> p<sub>i</sub> = exp(s<sub>i</sub>/œÑ) / ‚àë exp(s<sub>j</sub>/œÑ)<br><br>
          
          <strong>Effect of œÑ values:</strong><br>
          ‚Ä¢ œÑ ‚Üí 0: Picks highest similarity with probability ‚âà 1 (overconfident)<br>
          ‚Ä¢ œÑ = 1: Standard softmax (baseline)<br>
          ‚Ä¢ œÑ ‚Üí ‚àû: All probabilities approach equal (uniform, no confidence)<br><br>
          
          <strong>CLIP's œÑ ‚âà 0.07:</strong> Makes the model quite confident in its matches
        </div>

        <div class="interactive-demo">
          <div class="demo-title">üå°Ô∏è Temperature Effects Explorer</div>
          <p><strong>Experiment:</strong> See how the same similarity scores produce very different probability distributions with different temperatures. This is crucial for CLIP's training stability!</p>

          <div class="controls">
            <div class="control-group">
              <label>Similarity Scores (5 text candidates):</label>
              <input type="text" id="tempSimilarities" value="0.8, 0.3, 0.1, -0.2, -0.5" placeholder="e.g. 0.8, 0.3, 0.1, -0.2, -0.5">
              <small style="display:block;color:#666;margin-top:3px">These represent how similar an image is to 5 different text descriptions.</small>
            </div>
            <div class="control-group">
              <label>Temperature (œÑ):</label>
              <input type="range" id="tempScaleSlider" min="0.01" max="1.0" value="0.07" step="0.01">
              <span id="tempScaleValue" class="temperature-display">0.07</span>
            </div>
          </div>

          <button onclick="exploreTemperature()">üå°Ô∏è Explore Temperature Effects</button>
          <div id="temperatureExploration"></div>
        </div>
      </div>

      <div class="success">
        <strong>üéØ Key Mathematical Insights:</strong><br><br>
        
        <strong>üîó Joint Space Magic:</strong><br>
        ‚Ä¢ Both images and text become vectors in the same 512D space<br>
        ‚Ä¢ Similar concepts cluster together regardless of modality<br>
        ‚Ä¢ Distance in this space = semantic similarity<br><br>
        
        <strong>üìê Normalization Benefits:</strong><br>
        ‚Ä¢ Focuses on direction (semantic meaning), not magnitude<br>
        ‚Ä¢ All vectors lie on unit hypersphere for fair comparison<br>
        ‚Ä¢ Cosine similarity becomes simple dot product<br><br>
        
        <strong>üå°Ô∏è Temperature Tuning:</strong><br>
        ‚Ä¢ Controls prediction confidence during training<br>
        ‚Ä¢ œÑ = 0.07 found optimal through experimentation<br>
        ‚Ä¢ Too low = overconfident, too high = indecisive<br><br>
        
        <strong>üí° The Breakthrough:</strong><br>
        This mathematical framework enables zero-shot classification, image search, and is the foundation for GPT-4V, DALL-E, and modern multimodal AI!
      </div>
    </div>

      <div class="interactive-demo">
        <div class="demo-title">üßÆ Vector Mathematics Playground</div>
        <p><strong>See the math in action:</strong> Enter different similarity values and see how normalization and temperature scaling affect the final predictions. This is exactly what happens inside CLIP millions of times during training!</p>

        <div class="controls">
          <div class="control-group">
            <label>Raw Image Vector (4D simplified):</label>
            <input type="text" id="rawImageVector" value="2.0, -1.5, 3.0, 1.0" placeholder="e.g. 2.0, -1.5, 3.0, 1.0">
            <small style="display:block;color:#666;margin-top:3px">In reality, CLIP uses 512 dimensions. Here we use 4 for visualization.</small>
          </div>
          <div class="control-group">
            <label>Raw Text Vector (4D simplified):</label>
            <input type="text" id="rawTextVector" value="1.8, -1.2, 2.9, 0.8" placeholder="e.g. 1.8, -1.2, 2.9, 0.8">
            <small style="display:block;color:#666;margin-top:3px">Similar vectors = similar concepts. Try making them more/less similar!</small>
          </div>
          <div class="control-group">
            <label>Temperature (œÑ):</label>
            <input type="range" id="mathTemperature" min="0.01" max="0.2" value="0.07" step="0.01">
            <span id="mathTempValue" class="temperature-display">0.07</span>
            <small style="display:block;color:#666;margin-top:3px">Lower = more confident predictions, higher = more uncertain predictions.</small>
          </div>
        </div>

        <button onclick="calculateVectorMath()" class="primary">üßÆ Calculate Step-by-Step</button>
        <div id="vectorMathResults"></div>
      </div>

      <div class="step" style="background: #fff3cd; border-left-color: #ffc107;">
        <h4>ü§î Why L2 Normalization? The Geometric Insight</h4>
        <p><strong>The Problem:</strong> Without normalization, a vector like [1000, 2000, 3000] would have much higher similarity scores than [1, 2, 3] even if they point in the exact same direction!</p>
        
        <div class="math-formula">
          <strong>L2 Norm (Vector Length):</strong><br>
          ||v||<sub>2</sub> = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + v‚ÇÉ¬≤ + ... + v_d¬≤)<br><br>
          
          <strong>L2 Normalization:</strong><br>
          vÃÇ = v / ||v||<sub>2</sub><br><br>
          
          <strong>Result:</strong> All vectors have length = 1, so similarity only depends on <em>direction</em><br>
          <strong>Geometric meaning:</strong> All points lie on a unit hypersphere (circle in 2D, sphere in 3D, hypersphere in 512D)
        </div>

        <div class="interactive-demo">
          <div class="demo-title">üìê Normalization Visualizer</div>
          
          <div class="controls">
            <div class="control-group">
              <label>Vector A (before normalization):</label>
              <input type="text" id="vectorA" value="3, 4" placeholder="e.g. 3, 4">
            </div>
            <div class="control-group">
              <label>Vector B (before normalization):</label>
              <input type="text" id="vectorB" value="30, 40" placeholder="e.g. 30, 40">
            </div>
          </div>

          <button onclick="visualizeNormalization()">üìê Visualize Normalization Effect</button>
          <div id="normalizationViz"></div>
        </div>
      </div>

      <div class="step" style="background: #d1ecf1; border-left-color: #17a2b8;">
        <h4>üå°Ô∏è Temperature Scaling: The Confidence Knob</h4>
        <p><strong>What it does:</strong> Temperature (œÑ) controls how "confident" or "uncertain" the model's predictions become. It's like adjusting the contrast on a photo - lower temperature = higher contrast (more decisive), higher temperature = lower contrast (more uncertain).</p>

        <div class="math-formula">
          <strong>Temperature Effect on Softmax:</strong><br><br>
          
          <strong>Without temperature:</strong> p<sub>i</sub> = exp(s<sub>i</sub>) / ‚àë exp(s<sub>j</sub>)<br>
          <strong>With temperature:</strong> p<sub>i</sub> = exp(s<sub>i</sub>/œÑ) / ‚àë exp(s<sub>j</sub>/œÑ)<br><br>
          
          <strong>Effect of œÑ values:</strong><br>
          ‚Ä¢ œÑ ‚Üí 0: Picks highest similarity with probability ‚âà 1 (overconfident)<br>
          ‚Ä¢ œÑ = 1: Standard softmax (baseline)<br>
          ‚Ä¢ œÑ ‚Üí ‚àû: All probabilities approach equal (uniform, no confidence)<br><br>
          
          <strong>CLIP's œÑ ‚âà 0.07:</strong> Makes the model quite confident in its matches
        </div>

        <div class="interactive-demo">
          <div class="demo-title">üå°Ô∏è Temperature Effects Explorer</div>
          <p><strong>Experiment:</strong> See how the same similarity scores produce very different probability distributions with different temperatures. This is crucial for CLIP's training stability!</p>

          <div class="controls">
            <div class="control-group">
              <label>Similarity Scores (5 text candidates):</label>
              <input type="text" id="tempSimilarities" value="0.8, 0.3, 0.1, -0.2, -0.5" placeholder="e.g. 0.8, 0.3, 0.1, -0.2, -0.5">
              <small style="display:block;color:#666;margin-top:3px">These represent how similar an image is to 5 different text descriptions.</small>
            </div>
            <div class="control-group">
              <label>Temperature (œÑ):</label>
              <input type="range" id="tempScaleSlider" min="0.01" max="1.0" value="0.07" step="0.01">
              <span id="tempScaleValue" class="temperature-display">0.07</span>
            </div>
          </div>

          <button onclick="exploreTemperature()">üå°Ô∏è Explore Temperature Effects</button>
          <div id="temperatureExploration"></div>
        </div>
      </div>

      <div class="success">
        <strong>üéØ Key Mathematical Insights:</strong><br><br>
        
        <strong>üîó Joint Space Magic:</strong><br>
        ‚Ä¢ Both images and text become vectors in the same 512D space<br>
        ‚Ä¢ Similar concepts cluster together regardless of modality<br>
        ‚Ä¢ Distance in this space = semantic similarity<br><br>
        
        <strong>üìê Normalization Benefits:</strong><br>
        ‚Ä¢ Focuses on direction (semantic meaning), not magnitude<br>
        ‚Ä¢ All vectors lie on unit hypersphere for fair comparison<br>
        ‚Ä¢ Cosine similarity becomes simple dot product<br><br>
        
        <strong>üå°Ô∏è Temperature Tuning:</strong><br>
        ‚Ä¢ Controls prediction confidence during training<br>
        ‚Ä¢ œÑ = 0.07 found optimal through experimentation<br>
        ‚Ä¢ Too low = overconfident, too high = indecisive<br><br>
        
        <strong>üí° The Breakthrough:</strong><br>
        This mathematical framework enables zero-shot classification, image search, and is the foundation for GPT-4V, DALL-E, and modern multimodal AI!
      </div>
    </div>

  <div class="container">
    <h2>üéØ Contrastive Learning: The Training Magic</h2>
    <div class="step">
          <h3>üìä InfoNCE Loss: Learning Through Contrast</h3>
          <p>CLIP learns by seeing <strong>millions of (image, text) pairs</strong> from the internet and learning to maximize similarity between correct pairs while minimizing similarity between incorrect pairs. This creates a rich, semantic embedding space.</p>
    
          <div class="info">
            <strong>üîç InfoNCE Full Form:</strong><br>
            <strong>Info</strong>rmation <strong>N</strong>oise <strong>C</strong>ontrastive <strong>E</strong>stimation<br><br>
            ‚Ä¢ <strong>Information:</strong> Maximizes mutual information between matched image-text pairs<br>
            ‚Ä¢ <strong>Noise:</strong> Uses negative/incorrect examples as "noise" to create contrast<br>
            ‚Ä¢ <strong>Contrastive:</strong> Learns by comparing positive pairs vs negative pairs<br>
            ‚Ä¢ <strong>Estimation:</strong> Approximates the true relationship through sampling<br><br>
            <strong>üí° The Core Idea:</strong> Learn by distinguishing correct matches from incorrect ones - like a multiple choice test where each image has thousands of possible text answers!
          </div>
    
          <div class="math-formula">
            <strong>InfoNCE Loss Function:</strong><br><br>
            
            ‚Ñì<sub>i‚Üít</sub> = -log( exp(s<sub>ii</sub>/œÑ) / ‚àë<sub>j=1</sub><sup>N</sup> exp(s<sub>ij</sub>/œÑ) )<br><br>
            
            ‚Ñì<sub>t‚Üíi</sub> = -log( exp(s<sub>ii</sub>/œÑ) / ‚àë<sub>j=1</sub><sup>N</sup> exp(s<sub>ji</sub>/œÑ) )<br><br>
            
            <strong>Total Loss:</strong> ‚Ñì = (‚Ñì<sub>i‚Üít</sub> + ‚Ñì<sub>t‚Üíi</sub>) / 2<br><br>
            
            <strong>üßÆ Formula Breakdown:</strong><br>
            ‚Ä¢ <strong>Numerator:</strong> exp(s<sub>ii</sub>/œÑ) = strength of correct match (maximize this!)<br>
            ‚Ä¢ <strong>Denominator:</strong> ‚àë exp(s<sub>ij</sub>/œÑ) = sum over ALL possible matches (minimize incorrect ones)<br>
            ‚Ä¢ <strong>Division:</strong> Creates probability "How likely is the correct match?"<br>
            ‚Ä¢ <strong>Negative log:</strong> High probability ‚Üí low loss (good!), Low probability ‚Üí high loss (bad!)<br><br>
            
            Where:<br>
            ‚Ä¢ s<sub>ij</sub> = cosine similarity between image i and text j<br>
            ‚Ä¢ œÑ = temperature parameter (~0.07 makes model decisive)<br>
            ‚Ä¢ N = batch size (CLIP uses 32,768 = millions of negative examples!)
          </div>
    
          <div class="warning">
            <strong>üéØ Why Two Directions Matter:</strong><br>
            ‚Ä¢ <strong>‚Ñì<sub>i‚Üít</sub> (Image‚ÜíText):</strong> "Given this image, find the correct text description"<br>
            ‚Ä¢ <strong>‚Ñì<sub>t‚Üíi</sub> (Text‚ÜíImage):</strong> "Given this text, find the matching image"<br>
            ‚Ä¢ <strong>Both must work:</strong> Ensures bidirectional search and zero-shot classification<br>
            ‚Ä¢ <strong>Symmetry:</strong> Image-to-text and text-to-image become equally strong
          </div>

      <div class="interactive-demo">
        <div class="demo-title">üéì Contrastive Learning Simulator</div>

        <div class="controls">
          <div class="control-group">
            <label>Batch Size:</label>
            <select id="batchSize">
              <option value="4">4 (Demo)</option>
              <option value="8" selected>8 (Small)</option>
              <option value="16">16 (Medium)</option>
              <option value="32">32 (Large)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Positive Pair Similarity:</label>
            <input type="range" id="positiveSim" min="0.1" max="1.0" value="0.8" step="0.1">
            <span id="positiveSimValue">0.8</span>
          </div>
          <div class="control-group">
            <label>Negative Pair Similarity:</label>
            <input type="range" id="negativeSim" min="-0.5" max="0.5" value="0.1" step="0.1">
            <span id="negativeSimValue">0.1</span>
          </div>
          <div class="control-group">
            <label>Temperature:</label>
            <input type="range" id="contrastiveTemp" min="0.01" max="0.2" value="0.07" step="0.01">
            <span id="contrastiveTempValue">0.07</span>
          </div>
        </div>

        <button onclick="simulateContrastiveLearning()">üéØ Simulate Contrastive Training</button>
        <div id="contrastiveDemo"></div>
      </div>
    </div>

    <div class="step">
      <h3>üåê The Training Data: Learning from the Web</h3>
      <p>CLIP was trained on <strong>400 million (image, text) pairs</strong> collected from the internet. This massive, diverse dataset is key to CLIP's remarkable generalization abilities.</p>

      <div class="dataset-preview">
        <div class="dataset-item">
          <div class="dataset-image">üêï Dog Photo</div>
          <div class="dataset-text">"A golden retriever playing fetch in a park"</div>
        </div>
        <div class="dataset-item">
          <div class="dataset-image">üèîÔ∏è Landscape</div>
          <div class="dataset-text">"Snow-capped mountains reflected in a crystal clear lake"</div>
        </div>
        <div class="dataset-item">
          <div class="dataset-image">üçï Food</div>
          <div class="dataset-text">"Delicious pepperoni pizza with melted cheese"</div>
        </div>
        <div class="dataset-item">
          <div class="dataset-image">üöó Vehicle</div>
          <div class="dataset-text">"Red sports car parked on a city street"</div>
        </div>
      </div>

      <div class="info">
        <strong>üåç Dataset Scale & Diversity:</strong><br>
        ‚Ä¢ <strong>400M image-text pairs</strong> from the web (vs ImageNet's 1.3M labeled images)<br>
        ‚Ä¢ <strong>Natural language supervision</strong> instead of fixed categories<br>
        ‚Ä¢ <strong>Incredible diversity:</strong> Art, photos, memes, diagrams, charts, screenshots<br>
        ‚Ä¢ <strong>Multiple languages</strong> though primarily English<br>
        ‚Ä¢ <strong>No manual labeling</strong> - uses existing alt-text and captions
      </div>
    </div>
  </div>

  <div class="container">
    <h2>‚ö° Zero-Shot Classification: The Superpower</h2>
    <div class="step">
      <h3>üéØ How Zero-Shot Works</h3>
      <p>CLIP can classify images into categories it has never explicitly seen during training. Instead of learning fixed categories, it learned the <strong>relationship between visual concepts and language</strong>. Give it any text description, and it can find matching images!</p>

      <div class="math-formula">
        <strong>Zero-Shot Classification Process:</strong><br><br>
        
        <strong>1. Create text prompts:</strong><br>
        "A photo of a {class}" for each possible class<br><br>
        
        <strong>2. Encode image and all text prompts:</strong><br>
        I = f<sub>image</sub>(x), &nbsp; T<sub>k</sub> = f<sub>text</sub>("A photo of a {class_k}")<br><br>
        
        <strong>3. Calculate similarities:</strong><br>
        p(y=k|x) = exp(cos(I, T<sub>k</sub>)/œÑ) / ‚àë<sub>j</sub> exp(cos(I, T<sub>j</sub>)/œÑ)<br><br>
        
        <strong>4. Predict highest similarity class</strong>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üîÆ Zero-Shot Classification Demo</div>

        <div class="controls">
          <div class="control-group">
            <label>Select Test Image:</label>
            <select id="testImage">
              <option value="dog" selected>üêï Golden Retriever</option>
              <option value="cat">üê± Siamese Cat</option>
              <option value="car">üöó Red Sports Car</option>
              <option value="plane">‚úàÔ∏è Commercial Airplane</option>
              <option value="flower">üå∏ Cherry Blossom</option>
              <option value="food">üçï Pepperoni Pizza</option>
            </select>
          </div>
          <div class="control-group">
            <label>Prompt Template:</label>
            <select id="promptTemplate">
              <option value="photo" selected>A photo of a {class}</option>
              <option value="picture">A picture of a {class}</option>
              <option value="image">An image of a {class}</option>
              <option value="drawing">A drawing of a {class}</option>
            </select>
          </div>
        </div>

        <div class="class-options" id="classOptions">
          <div class="class-option" onclick="toggleClass(this)">dog</div>
          <div class="class-option" onclick="toggleClass(this)">cat</div>
          <div class="class-option" onclick="toggleClass(this)">car</div>
          <div class="class-option" onclick="toggleClass(this)">airplane</div>
          <div class="class-option" onclick="toggleClass(this)">flower</div>
          <div class="class-option" onclick="toggleClass(this)">pizza</div>
          <div class="class-option" onclick="toggleClass(this)">bird</div>
          <div class="class-option" onclick="toggleClass(this)">tree</div>
        </div>

        <button onclick="performZeroShot()" class="primary">üîÆ Perform Zero-Shot Classification</button>
        <div id="zeroShotResults"></div>
      </div>
    </div>

    <div class="step">
      <h3>üìä CLIP's Remarkable Performance</h3>
      <p>CLIP achieved unprecedented zero-shot performance, often matching or exceeding supervised models trained specifically on target datasets.</p>

      <div class="metric-grid">
        <div class="metric-card">
          <div class="metric-value">76.2%</div>
          <div class="metric-label">ImageNet Zero-Shot Top-1</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">95.0%</div>
          <div class="metric-label">ImageNet Zero-Shot Top-5</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">88.8%</div>
          <div class="metric-label">CIFAR-10 Zero-Shot</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">400M</div>
          <div class="metric-label">Training Pairs</div>
        </div>
      </div>

      <div class="success">
        <strong>üèÜ CLIP's Breakthrough Achievements:</strong><br>
        ‚Ä¢ <strong>Human-level ImageNet performance</strong> without seeing ImageNet training data<br>
        ‚Ä¢ <strong>Generalizes across domains</strong> - medical images, satellite imagery, art<br>
        ‚Ä¢ <strong>Robust to distribution shift</strong> - performs well on different image styles<br>
        ‚Ä¢ <strong>Interpretable failures</strong> - when wrong, the mistakes make sense<br>
        ‚Ä¢ <strong>Multilingual capabilities</strong> - works with text in multiple languages
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üîç Embedding Space Visualization</h2>
    <div class="step">
      <h3>üåå Exploring the Joint Embedding Space</h3>
      <p>CLIP creates a rich embedding space where semantically similar images and text cluster together. This space is the foundation for all of CLIP's capabilities.</p>

      <div class="interactive-demo">
        <div class="demo-title">üó∫Ô∏è Interactive Embedding Space Explorer</div>

        <div class="controls">
          <div class="control-group">
            <label>Visualization Mode:</label>
            <select id="vizMode">
              <option value="semantic" selected>Semantic Clusters</option>
              <option value="similarity">Similarity Heatmap</option>
              <option value="nearest">Nearest Neighbors</option>
            </select>
          </div>
          <div class="control-group">
            <label>Query Type:</label>
            <select id="queryType">
              <option value="image" selected>Image Query</option>
              <option value="text">Text Query</option>
            </select>
          </div>
          <div class="control-group">
            <label>Query:</label>
            <select id="queryContent">
              <option value="dog" selected>Dog</option>
              <option value="cat">Cat</option>
              <option value="car">Car</option>
              <option value="landscape">Landscape</option>
            </select>
          </div>
        </div>

        <button onclick="visualizeEmbedding()">üó∫Ô∏è Visualize Embedding Space</button>
        <div class="embedding-space">
          <canvas id="embeddingCanvas" class="embedding-canvas"></canvas>
        </div>
        <div id="embeddingAnalysis"></div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üõ†Ô∏è Implementation & Code Examples</h2>
    <div class="step">
      <h3>üíª Using CLIP with OpenAI's API</h3>

      <div class="tabs">
        <div class="tab active" onclick="switchTab('basic', this)">Basic Usage</div>
        <div class="tab" onclick="switchTab('zeroshot', this)">Zero-Shot</div>
        <div class="tab" onclick="switchTab('similarity', this)">Similarity Search</div>
        <div class="tab" onclick="switchTab('custom', this)">Custom Training</div>
      </div>

      <div id="basic" class="tab-content active">
        <div class="code-block">
          <div class="code-header">üöÄ Basic CLIP Usage (OpenAI API)</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import torch
import clip
from PIL import Image

# Load model and preprocessing
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Load and preprocess image
image = preprocess(Image.open("photo.jpg")).unsqueeze(0).to(device)

# Tokenize text
text = clip.tokenize(["a dog", "a cat", "a car"]).to(device)

# Get features
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    
    # Normalize features
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    
    # Calculate similarities
    similarities = (image_features @ text_features.T).squeeze(0)
    probs = similarities.softmax(dim=-1)
    
print(f"Probabilities: {probs}")
for i, text_input in enumerate(["a dog", "a cat", "a car"]):
    print(f"{text_input}: {probs[i].item():.3f}")
</pre>
        </div>
      </div>

      <div id="zeroshot" class="tab-content">
        <div class="code-block">
          <div class="code-header">üîÆ Zero-Shot Classification</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import torch
import clip
from PIL import Image
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

def zero_shot_classify(image_path, classes, template="A photo of a {}"):
    """
    Perform zero-shot classification on an image
    """
    # Load and preprocess image
    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
    
    # Create text prompts for each class
    text_prompts = [template.format(cls) for cls in classes]
    text = clip.tokenize(text_prompts).to(device)
    
    with torch.no_grad():
        # Get features
        image_features = model.encode_image(image)
        text_features = model.encode_text(text)
        
        # Normalize
        image_features /= image_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # Calculate similarities and probabilities
        similarities = (image_features @ text_features.T).squeeze(0)
        probs = similarities.softmax(dim=-1)
    
    # Return results
    results = []
    for i, cls in enumerate(classes):
        results.append({
            'class': cls,
            'probability': probs[i].item(),
            'similarity': similarities[i].item()
        })
    
    return sorted(results, key=lambda x: x['probability'], reverse=True)

# Example usage
classes = ['dog', 'cat', 'bird', 'car', 'airplane', 'ship']
results = zero_shot_classify('test_image.jpg', classes)

print("Zero-shot classification results:")
for result in results:
    print(f"{result['class']}: {result['probability']:.3f}")
</pre>
        </div>
      </div>

      <div id="similarity" class="tab-content">
        <div class="code-block">
          <div class="code-header">üîç Image-Text Similarity Search</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import torch
import clip
from PIL import Image
import os
from pathlib import Path

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

class CLIPSearchEngine:
    def __init__(self, image_dir):
        self.image_dir = Path(image_dir)
        self.image_features = []
        self.image_paths = []
        self.build_index()
    
    def build_index(self):
        """Build searchable index of image features"""
        print("Building CLIP index...")
        
        for img_path in self.image_dir.glob("*.jpg"):
            try:
                image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)
                
                with torch.no_grad():
                    features = model.encode_image(image)
                    features /= features.norm(dim=-1, keepdim=True)
                
                self.image_features.append(features.cpu())
                self.image_paths.append(img_path)
            except Exception as e:
                print(f"Error processing {img_path}: {e}")
        
        self.image_features = torch.cat(self.image_features, dim=0)
        print(f"Indexed {len(self.image_paths)} images")
    
    def search(self, query_text, top_k=5):
        """Search for images matching text query"""
        # Encode query text
        text = clip.tokenize([query_text]).to(device)
        
        with torch.no_grad():
            text_features = model.encode_text(text)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # Calculate similarities
        similarities = (self.image_features @ text_features.T).squeeze()
        top_indices = similarities.topk(top_k).indices
        
        results = []
        for idx in top_indices:
            results.append({
                'path': self.image_paths[idx],
                'similarity': similarities[idx].item()
            })
        
        return results

# Example usage
search_engine = CLIPSearchEngine("./image_database/")

# Search for images
results = search_engine.search("a beautiful sunset over mountains", top_k=10)
print("Search results:")
for result in results:
    print(f"{result['path']}: {result['similarity']:.3f}")
</pre>
        </div>
      </div>

      <div id="custom" class="tab-content">
        <div class="code-block">
          <div class="code-header">üéì Custom CLIP Training (Simplified)</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import clip

class CLIPLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, image_features, text_features):
        # Normalize features
        image_features = F.normalize(image_features, dim=1)
        text_features = F.normalize(text_features, dim=1)
        
        # Calculate similarity matrix
        similarities = torch.matmul(image_features, text_features.T) / self.temperature
        
        # Create labels (diagonal matrix)
        batch_size = similarities.size(0)
        labels = torch.arange(batch_size).to(similarities.device)
        
        # Calculate contrastive losses
        loss_img_to_text = F.cross_entropy(similarities, labels)
        loss_text_to_img = F.cross_entropy(similarities.T, labels)
        
        return (loss_img_to_text + loss_text_to_img) / 2

def train_clip_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    
    for batch_idx, (images, texts) in enumerate(dataloader):
        images, texts = images.to(device), texts.to(device)
        
        # Forward pass
        image_features = model.encode_image(images)
        text_features = model.encode_text(texts)
        
        # Calculate loss
        loss = criterion(image_features, text_features)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if batch_idx % 100 == 0:
            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')
    
    return total_loss / len(dataloader)

# Training setup
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)
criterion = CLIPLoss(temperature=0.07)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.1)

# Note: You'll need to implement your own dataset class
# that returns (image, text) pairs
# train_dataset = CustomCLIPDataset(...)
# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

print("üöÄ CLIP training setup complete!")
print("Remember: CLIP training requires massive datasets (400M+ pairs)")
print("Consider fine-tuning on smaller domain-specific datasets instead")
</pre>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>üéØ Production Deployment Tips</h3>
      <div class="success">
        <strong>‚úÖ CLIP Production Best Practices:</strong><br><br>
        
        <strong>üîß Model Selection:</strong><br>
        ‚Ä¢ <strong>ViT-B/32:</strong> Fastest inference, good for real-time applications<br>
        ‚Ä¢ <strong>ViT-B/16:</strong> Best balance of speed and accuracy<br>
        ‚Ä¢ <strong>ViT-L/14:</strong> Highest quality, use for offline processing<br><br>
        
        <strong>‚ö° Performance Optimization:</strong><br>
        ‚Ä¢ Cache text embeddings for repeated queries<br>
        ‚Ä¢ Batch process images when possible<br>
        ‚Ä¢ Use mixed precision (FP16) for 2x speedup<br>
        ‚Ä¢ Consider ONNX conversion for deployment<br><br>
        
        <strong>üéØ Application Patterns:</strong><br>
        ‚Ä¢ <strong>Image Search:</strong> Encode images once, search with text queries<br>
        ‚Ä¢ <strong>Content Moderation:</strong> Zero-shot classification for inappropriate content<br>
        ‚Ä¢ <strong>Product Matching:</strong> Find visually similar products<br>
        ‚Ä¢ <strong>Creative Tools:</strong> Foundation for text-to-image generation
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üî¨ Advanced Topics & Research Insights</h2>
    <div class="step">
      <h3>üß† What Makes CLIP Work So Well?</h3>
      <div class="warning">
        <strong>üîç Key Research Insights:</strong><br>
        ‚Ä¢ <strong>Scale is crucial:</strong> Performance scales log-linearly with dataset size<br>
        ‚Ä¢ <strong>Natural language supervision:</strong> Web text is incredibly rich and diverse<br>
        ‚Ä¢ <strong>Contrastive learning:</strong> More efficient than predicting exact text<br>
        ‚Ä¢ <strong>Temperature parameter:</strong> Critical for training stability and performance<br>
        ‚Ä¢ <strong>Prompt engineering matters:</strong> "A photo of a [class]" works better than just "[class]"
      </div>

      <div class="info">
        <strong>üé≠ CLIP's Limitations:</strong><br>
        ‚Ä¢ <strong>Compositional reasoning:</strong> Struggles with complex spatial relationships<br>
        ‚Ä¢ <strong>Fine-grained classification:</strong> Difficulty with very similar classes<br>
        ‚Ä¢ <strong>Counting:</strong> Cannot reliably count objects in images<br>
        ‚Ä¢ <strong>Text reading:</strong> Limited OCR capabilities<br>
        ‚Ä¢ <strong>Abstract concepts:</strong> Works best with concrete, visual concepts
      </div>
    </div>

    <div class="step">
      <h3>üöÄ CLIP's Impact on AI</h3>
      <div class="breakthrough-highlight">
        üåü CLIP sparked the multimodal AI revolution: GPT-4V, DALL-E 2/3, Stable Diffusion, and countless applications all build on CLIP's foundation!
      </div>

      <div class="success">
        <strong>üèÜ Revolutionary Applications Enabled by CLIP:</strong><br>
        ‚Ä¢ <strong>Text-to-Image Generation:</strong> DALL-E, Stable Diffusion, Midjourney<br>
        ‚Ä¢ <strong>Vision-Language Models:</strong> GPT-4V, Flamingo, BLIP<br>
        ‚Ä¢ <strong>Image Editing:</strong> CLIPDraw, StyleCLIP, semantic image editing<br>
        ‚Ä¢ <strong>Robotics:</strong> CLIPort for language-guided robot manipulation<br>
        ‚Ä¢ <strong>3D Understanding:</strong> CLIP-guided NeRF, 3D shape retrieval<br>
        ‚Ä¢ <strong>Video Understanding:</strong> VideoCLIP, ActionCLIP for video analysis
      </div>
    </div>
  </div>

  <script>
    let activeTab = 'basic';

    // Initialize on page load
    document.addEventListener('DOMContentLoaded', () => {
      initializeCharts();
      setupEventListeners();
      analyzeArchitecture();
      demonstrateTemperature();
      simulateContrastiveLearning();
      visualizeEmbedding();
      performZeroShot();
    });

    function setupEventListeners() {
      // Temperature slider
      const tempSlider = document.getElementById('temperatureSlider');
      if (tempSlider) {
        tempSlider.addEventListener('input', () => {
          document.getElementById('temperatureValue').textContent = tempSlider.value;
        });
      }

      // Contrastive learning sliders
      const posSlider = document.getElementById('positiveSim');
      const negSlider = document.getElementById('negativeSim');
      const ctempSlider = document.getElementById('contrastiveTemp');
      
      if (posSlider) {
        posSlider.addEventListener('input', () => {
          document.getElementById('positiveSimValue').textContent = posSlider.value;
        });
      }
      if (negSlider) {
        negSlider.addEventListener('input', () => {
          document.getElementById('negativeSimValue').textContent = negSlider.value;
        });
      }
      if (ctempSlider) {
        ctempSlider.addEventListener('input', () => {
          document.getElementById('contrastiveTempValue').textContent = ctempSlider.value;
        });
      }
    }

    function initializeCharts() {
      const canvas = document.getElementById('embeddingCanvas');
      if (canvas) {
        canvas.width = canvas.offsetWidth;
        canvas.height = canvas.offsetHeight;
      }
    }

    function analyzeArchitecture() {
      const variant = document.getElementById('clipVariant')?.value || 'vit-b-32';
      const embeddingDim = document.getElementById('embeddingDim')?.value || '512';
      const analysisType = document.getElementById('analysisType')?.value || 'parameters';

      const specs = {
        'vit-b-32': { params: 151, flops: 4.4, memory: 2.5, speed: 100 },
        'vit-b-16': { params: 149, flops: 17.6, memory: 3.2, speed: 40 },
        'vit-l-14': { params: 427, flops: 81.1, memory: 8.7, speed: 15 },
        'resnet-50': { params: 102, flops: 4.1, memory: 2.1, speed: 120 }
      };

      const spec = specs[variant];
      const analysis = document.getElementById('architectureAnalysis');
      
      if (analysis) {
        let content = `
          <div class="metric-grid">
            <div class="metric-card">
              <div class="metric-value">${spec.params}M</div>
              <div class="metric-label">Total Parameters</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${embeddingDim}D</div>
              <div class="metric-label">Embedding Dimension</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${spec.flops}G</div>
              <div class="metric-label">FLOPs per Image</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${spec.speed}</div>
              <div class="metric-label">Images/Second (A100)</div>
            </div>
          </div>`;

        if (analysisType === 'parameters') {
          content += `
            <div class="info">
              <strong>üìä Parameter Breakdown:</strong><br>
              ‚Ä¢ <strong>Vision Encoder:</strong> ~${Math.round(spec.params * 0.6)}M parameters<br>
              ‚Ä¢ <strong>Text Encoder:</strong> ~${Math.round(spec.params * 0.4)}M parameters<br>
              ‚Ä¢ <strong>Projection Layers:</strong> Minimal (~1M parameters)<br>
              ‚Ä¢ <strong>Trade-offs:</strong> ${variant.includes('32') ? 'Faster but lower resolution patches' : variant.includes('16') ? 'Balanced resolution and speed' : 'Highest quality, slowest inference'}
            </div>`;
        }
        
        analysis.innerHTML = content;
      }
    }

    function demonstrateTemperature() {
      const temperature = parseFloat(document.getElementById('temperatureSlider')?.value || '0.07');
      const baseSims = document.getElementById('baseSimilarities')?.value || '0.8, 0.3, 0.1, -0.2, -0.5';
      
      const similarities = baseSims.split(',').map(x => parseFloat(x.trim()));
      const scaledSims = similarities.map(s => s / temperature);
      const softmax = scaledSims.map(s => Math.exp(s));
      const sumExp = softmax.reduce((a, b) => a + b, 0);
      const probabilities = softmax.map(s => s / sumExp);

      const demo = document.getElementById('temperatureDemo');
      if (demo) {
        demo.innerHTML = `
          <div class="metric-grid">
            <div class="metric-card">
              <div class="metric-value">${temperature}</div>
              <div class="metric-label">Temperature (œÑ)</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${Math.max(...probabilities).toFixed(3)}</div>
              <div class="metric-label">Max Probability</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${(-probabilities.reduce((sum, p) => sum + p * Math.log(p), 0)).toFixed(2)}</div>
              <div class="metric-label">Entropy</div>
            </div>
          </div>
          <div class="info">
            <strong>üìä Before/After Temperature Scaling:</strong><br>
            ${similarities.map((sim, i) => 
              `‚Ä¢ Similarity ${i+1}: ${sim.toFixed(2)} ‚Üí Probability: ${probabilities[i].toFixed(3)}`
            ).join('<br>')}
            <br><br>
            <strong>üí° Effect:</strong> ${temperature < 0.1 ? 'Low temperature makes predictions more confident/peaked' : 'High temperature makes predictions more uniform/uncertain'}
          </div>`;
      }
    }

    function simulateContrastiveLearning() {
      const batchSize = parseInt(document.getElementById('batchSize')?.value || '8');
      const positiveSim = parseFloat(document.getElementById('positiveSim')?.value || '0.8');
      const negativeSim = parseFloat(document.getElementById('negativeSim')?.value || '0.1');
      const temperature = parseFloat(document.getElementById('contrastiveTemp')?.value || '0.07');

      // Create similarity matrix
      const matrix = [];
      for (let i = 0; i < batchSize; i++) {
        const row = [];
        for (let j = 0; j < batchSize; j++) {
          if (i === j) {
            row.push(positiveSim + (Math.random() - 0.5) * 0.1); // positive pairs
          } else {
            row.push(negativeSim + (Math.random() - 0.5) * 0.2); // negative pairs
          }
        }
        matrix.push(row);
      }

      // Calculate InfoNCE loss
      let totalLoss = 0;
      for (let i = 0; i < batchSize; i++) {
        const numerator = Math.exp(matrix[i][i] / temperature);
        const denominator = matrix[i].reduce((sum, sim) => sum + Math.exp(sim / temperature), 0);
        totalLoss += -Math.log(numerator / denominator);
      }
      totalLoss /= batchSize;

      const demo = document.getElementById('contrastiveDemo');
      if (demo) {
        // Create similarity matrix visualization
        let matrixHtml = `<div class="contrastive-matrix">
          <h4>Similarity Matrix (${batchSize}√ó${batchSize})</h4>
          <div class="matrix-grid" style="grid-template-columns: repeat(${batchSize + 1}, 1fr);">
            <div class="matrix-header">I\\T</div>`;
        
        // Header row
        for (let j = 0; j < batchSize; j++) {
          matrixHtml += `<div class="matrix-header">T${j}</div>`;
        }
        
        // Data rows
        for (let i = 0; i < batchSize; i++) {
          matrixHtml += `<div class="matrix-header">I${i}</div>`;
          for (let j = 0; j < batchSize; j++) {
            const value = matrix[i][j];
            const cellClass = i === j ? 'matrix-positive' : 'matrix-negative';
            matrixHtml += `<div class="matrix-cell ${cellClass}">${value.toFixed(2)}</div>`;
          }
        }
        
        matrixHtml += '</div></div>';

        demo.innerHTML = matrixHtml + `
          <div class="metric-grid">
            <div class="metric-card">
              <div class="metric-value">${totalLoss.toFixed(3)}</div>
              <div class="metric-label">InfoNCE Loss</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${positiveSim.toFixed(2)}</div>
              <div class="metric-label">Positive Similarity</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${negativeSim.toFixed(2)}</div>
              <div class="metric-label">Negative Similarity</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${temperature}</div>
              <div class="metric-label">Temperature</div>
            </div>
          </div>
          <div class="info">
            <strong>üéØ Contrastive Learning Insights:</strong><br>
            ‚Ä¢ <strong>Green diagonal:</strong> Correct image-text pairs (maximize these)<br>
            ‚Ä¢ <strong>Red off-diagonal:</strong> Incorrect pairs (minimize these)<br>
            ‚Ä¢ <strong>Loss interpretation:</strong> ${totalLoss < 1 ? 'Good separation between positive/negative pairs' : 'Need better separation - increase positive similarity or decrease negative'}<br>
            ‚Ä¢ <strong>Large batch benefits:</strong> More negative examples per positive pair
          </div>`;
      }
    }

    function toggleClass(element) {
      element.classList.toggle('selected');
    }

    function performZeroShot() {
      const testImage = document.getElementById('testImage')?.value || 'dog';
      const promptTemplate = document.getElementById('promptTemplate')?.value || 'photo';
      const selectedClasses = Array.from(document.querySelectorAll('.class-option.selected')).map(el => el.textContent);
      
      if (selectedClasses.length === 0) {
        // Auto-select some classes for demo
        const defaultClasses = ['dog', 'cat', 'car', 'airplane'];
        document.querySelectorAll('.class-option').forEach(el => {
          if (defaultClasses.includes(el.textContent)) {
            el.classList.add('selected');
          }
        });
        performZeroShot();
        return;
      }

      // Simulate CLIP similarities (in reality these would come from the model)
      const groundTruth = {
        'dog': { dog: 0.9, cat: 0.3, car: 0.1, airplane: 0.05, flower: 0.1, pizza: 0.1 },
        'cat': { cat: 0.85, dog: 0.4, car: 0.1, airplane: 0.05, flower: 0.15, pizza: 0.1 },
        'car': { car: 0.9, airplane: 0.2, dog: 0.05, cat: 0.05, flower: 0.1, pizza: 0.1 },
        'plane': { airplane: 0.9, car: 0.3, dog: 0.05, cat: 0.05, flower: 0.1, pizza: 0.1 },
        'flower': { flower: 0.9, dog: 0.1, cat: 0.1, car: 0.05, airplane: 0.05, pizza: 0.1 },
        'food': { pizza: 0.9, flower: 0.1, dog: 0.1, cat: 0.1, car: 0.05, airplane: 0.05 }
      };

      const baseSims = groundTruth[testImage] || groundTruth['dog'];
      const similarities = selectedClasses.map(cls => baseSims[cls] || 0.1);
      
      // Add some noise and template effect
      const templateBoost = {
        'photo': 1.0,
        'picture': 0.95,
        'image': 0.9,
        'drawing': 0.7
      };
      
      const adjustedSims = similarities.map(s => s * (templateBoost[promptTemplate] || 1.0) + (Math.random() - 0.5) * 0.1);
      
      // Softmax to get probabilities
      const expSims = adjustedSims.map(s => Math.exp(s / 0.07));
      const sumExp = expSims.reduce((a, b) => a + b, 0);
      const probabilities = expSims.map(e => e / sumExp);

      const results = selectedClasses.map((cls, i) => ({
        class: cls,
        similarity: adjustedSims[i],
        probability: probabilities[i]
      })).sort((a, b) => b.probability - a.probability);

      const resultsDiv = document.getElementById('zeroShotResults');
      if (resultsDiv) {
        let html = `
          <div class="zero-shot-demo">
            <h4>üîÆ Zero-Shot Classification Results</h4>
            <p><strong>Test Image:</strong> ${testImage} | <strong>Prompt:</strong> "${promptTemplate.replace('{}', '{class}')}"</p>`;

        results.forEach((result, i) => {
          const barWidth = result.probability * 100;
          const isCorrect = result.class === testImage || (testImage === 'food' && result.class === 'pizza') || (testImage === 'plane' && result.class === 'airplane');
          html += `
            <div style="margin: 10px 0;">
              <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 5px;">
                <span style="font-weight: bold; color: ${isCorrect ? '#28a745' : '#2d2d2d'}">${i + 1}. ${result.class}</span>
                <span>${(result.probability * 100).toFixed(1)}%</span>
              </div>
              <div class="progress-bar">
                <div class="progress-fill" style="width: ${barWidth}%; background: ${isCorrect ? '#28a745' : '#6c757d'}">
                  ${barWidth > 20 ? (result.probability * 100).toFixed(1) + '%' : ''}
                </div>
              </div>
            </div>`;
        });

        html += `
            <div class="success" style="margin-top: 20px;">
              <strong>‚úÖ Zero-Shot Success:</strong> ${results[0].class === testImage || (testImage === 'food' && results[0].class === 'pizza') || (testImage === 'plane' && results[0].class === 'airplane') ? 'Correct classification!' : 'Misclassification - but note the reasoning pattern'}
              <br><strong>Top prediction confidence:</strong> ${(results[0].probability * 100).toFixed(1)}%
            </div>
          </div>`;

        resultsDiv.innerHTML = html;
      }
    }

    function visualizeEmbedding() {
      const canvas = document.getElementById('embeddingCanvas');
      if (!canvas) return;
      
      const ctx = canvas.getContext('2d');
      const width = canvas.width;
      const height = canvas.height;

      // Clear canvas
      ctx.fillStyle = '#f8f9fa';
      ctx.fillRect(0, 0, width, height);

      // Generate sample embeddings for visualization
      const concepts = [
        { name: 'dog', type: 'animal', x: width * 0.3, y: height * 0.3, color: '#dc3545' },
        { name: 'cat', type: 'animal', x: width * 0.4, y: height * 0.35, color: '#dc3545' },
        { name: 'bird', type: 'animal', x: width * 0.35, y: height * 0.25, color: '#dc3545' },
        { name: 'car', type: 'vehicle', x: width * 0.7, y: height * 0.6, color: '#007bff' },
        { name: 'truck', type: 'vehicle', x: width * 0.75, y: height * 0.65, color: '#007bff' },
        { name: 'airplane', type: 'vehicle', x: width * 0.8, y: height * 0.55, color: '#007bff' },
        { name: 'rose', type: 'plant', x: width * 0.2, y: height * 0.7, color: '#28a745' },
        { name: 'tree', type: 'plant', x: width * 0.25, y: height * 0.75, color: '#28a745' },
        { name: 'flower', type: 'plant', x: width * 0.15, y: height * 0.65, color: '#28a745' },
      ];

      // Draw concept clusters
      concepts.forEach(concept => {
        // Draw concept circle
        ctx.fillStyle = concept.color;
        ctx.beginPath();
        ctx.arc(concept.x, concept.y, 8, 0, 2 * Math.PI);
        ctx.fill();

        // Draw label
        ctx.fillStyle = '#2d2d2d';
        ctx.font = '12px Arial';
        ctx.fillText(concept.name, concept.x + 12, concept.y + 4);
      });

      // Draw cluster boundaries
      const clusters = {
        'animal': { x: width * 0.35, y: height * 0.3, radius: 80, color: '#dc354540' },
        'vehicle': { x: width * 0.75, y: height * 0.6, radius: 70, color: '#007bff40' },
        'plant': { x: width * 0.2, y: height * 0.7, radius: 60, color: '#28a74540' }
      };

      Object.values(clusters).forEach(cluster => {
        ctx.strokeStyle = cluster.color;
        ctx.lineWidth = 2;
        ctx.setLineDash([5, 5]);
        ctx.beginPath();
        ctx.arc(cluster.x, cluster.y, cluster.radius, 0, 2 * Math.PI);
        ctx.stroke();
        ctx.setLineDash([]);
      });

      // Add title and axes labels
      ctx.fillStyle = '#2d2d2d';
      ctx.font = 'bold 16px Arial';
      ctx.fillText('CLIP Embedding Space (2D Projection)', 10, 25);
      
      ctx.font = '12px Arial';
      ctx.fillText('Dimension 1', width - 80, height - 10);
      ctx.save();
      ctx.translate(15, height - 50);
      ctx.rotate(-Math.PI/2);
      ctx.fillText('Dimension 2', 0, 0);
      ctx.restore();

      // Update analysis
      const analysis = document.getElementById('embeddingAnalysis');
      if (analysis) {
        analysis.innerHTML = `
          <div class="info">
            <strong>üó∫Ô∏è Embedding Space Insights:</strong><br>
            ‚Ä¢ <strong>Semantic Clustering:</strong> Similar concepts naturally group together<br>
            ‚Ä¢ <strong>Cross-Modal Alignment:</strong> Text "dog" and dog images occupy same space<br>
            ‚Ä¢ <strong>Hierarchical Structure:</strong> Animals, vehicles, plants form distinct clusters<br>
            ‚Ä¢ <strong>Compositional Properties:</strong> "Small dog" would lie between dog cluster and size concepts<br><br>
            <strong>üí° This 2D projection shows how CLIP's 512D space organizes knowledge semantically!</strong>
          </div>`;
      }
    }

    function switchTab(tabName, element) {
      // Remove active class from all tabs and content
      document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
      document.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));
      
      // Add active class to clicked tab and corresponding content
      if (element) element.classList.add('active');
      const content = document.getElementById(tabName);
      if (content) content.classList.add('active');
      
      activeTab = tabName;
    }

    function copyCode(button) {
      const codeBlock = button.nextElementSibling;
      if (codeBlock && codeBlock.tagName === 'PRE') {
        const text = codeBlock.textContent;
        navigator.clipboard.writeText(text).then(() => {
          const originalText = button.textContent;
          button.textContent = '‚úÖ Copied';
          setTimeout(() => {
            button.textContent = originalText;
          }, 2000);
        });
      }
    }
    // Vector Mathematics Functions for CLIP Tutorial

function calculateVectorMath() {
  const imageVectorStr = document.getElementById('rawImageVector')?.value || '2.0, -1.5, 3.0, 1.0';
  const textVectorStr = document.getElementById('rawTextVector')?.value || '1.8, -1.2, 2.9, 0.8';
  const temperature = parseFloat(document.getElementById('mathTemperature')?.value || '0.07');
  
  // Update temperature display
  document.getElementById('mathTempValue').textContent = temperature.toFixed(2);
  
  // Parse vectors
  const imageVector = imageVectorStr.split(',').map(x => parseFloat(x.trim()));
  const textVector = textVectorStr.split(',').map(x => parseFloat(x.trim()));
  
  // Validate vectors
  if (imageVector.length !== textVector.length || imageVector.some(isNaN) || textVector.some(isNaN)) {
    document.getElementById('vectorMathResults').innerHTML = `
      <div class="warning">‚ö†Ô∏è Please enter valid vectors of the same length (e.g., "2.0, -1.5, 3.0, 1.0")</div>`;
    return;
  }
  
  // Step 1: Calculate L2 norms
  const imageNorm = Math.sqrt(imageVector.reduce((sum, val) => sum + val * val, 0));
  const textNorm = Math.sqrt(textVector.reduce((sum, val) => sum + val * val, 0));
  
  // Step 2: Normalize vectors
  const imageNormalized = imageVector.map(val => val / imageNorm);
  const textNormalized = textVector.map(val => val / textNorm);
  
  // Step 3: Calculate cosine similarity (dot product of normalized vectors)
  const similarity = imageNormalized.reduce((sum, val, i) => sum + val * textNormalized[i], 0);
  
  // Step 4: Apply temperature scaling
  const logit = similarity / temperature;
  
  // Step 5: Convert to probability (simplified - assuming this is the positive case)
  const probability = 1 / (1 + Math.exp(-logit)); // Sigmoid for demo
  
  const results = document.getElementById('vectorMathResults');
  if (results) {
    results.innerHTML = `
      <div class="training-simulation">
        <h4>üßÆ Step-by-Step Mathematical Transformation</h4>
        
        <div style="display: grid; gap: 15px; margin: 20px 0;">
          <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #dc3545;">
            <strong>üì• Step 1: Raw Vectors</strong><br>
            <strong>Image:</strong> [${imageVector.map(v => v.toFixed(2)).join(', ')}]<br>
            <strong>Text:</strong> [${textVector.map(v => v.toFixed(2)).join(', ')}]
          </div>
          
          <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007bff;">
            <strong>üìè Step 2: Calculate L2 Norms (Vector Lengths)</strong><br>
            <strong>Image norm:</strong> ‚àö(${imageVector.map(v => v.toFixed(1) + '¬≤').join(' + ')}) = ${imageNorm.toFixed(3)}<br>
            <strong>Text norm:</strong> ‚àö(${textVector.map(v => v.toFixed(1) + '¬≤').join(' + ')}) = ${textNorm.toFixed(3)}
          </div>
          
          <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #28a745;">
            <strong>üéØ Step 3: L2 Normalization (Make Length = 1)</strong><br>
            <strong>Image normalized:</strong> [${imageNormalized.map(v => v.toFixed(3)).join(', ')}]<br>
            <strong>Text normalized:</strong> [${textNormalized.map(v => v.toFixed(3)).join(', ')}]<br>
            <em>Now both vectors have length = 1.000</em>
          </div>
          
          <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #ffc107;">
            <strong>üìê Step 4: Cosine Similarity (Dot Product)</strong><br>
            <strong>Similarity:</strong> ${imageNormalized.map((v, i) => `${v.toFixed(2)}√ó${textNormalized[i].toFixed(2)}`).join(' + ')} = <strong>${similarity.toFixed(4)}</strong><br>
            <em>Range: -1 (opposite) to +1 (identical)</em>
          </div>
          
          <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #6f42c1;">
            <strong>üå°Ô∏è Step 5: Temperature Scaling</strong><br>
            <strong>Logit:</strong> ${similarity.toFixed(4)} √∑ ${temperature} = <strong>${logit.toFixed(2)}</strong><br>
            <em>Temperature amplifies the similarity by ${(1/temperature).toFixed(1)}√ó</em>
          </div>
          
          <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #17a2b8;">
            <strong>üéØ Step 6: Final Probability</strong><br>
            <strong>Probability:</strong> sigmoid(${logit.toFixed(2)}) = <strong>${(probability * 100).toFixed(1)}%</strong><br>
            <em>This represents how likely these vectors represent the same concept</em>
          </div>
        </div>
        
        <div class="metric-grid">
          <div class="metric-card">
            <div class="metric-value">${similarity.toFixed(3)}</div>
            <div class="metric-label">Cosine Similarity</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">${logit.toFixed(2)}</div>
            <div class="metric-label">Temperature-Scaled Logit</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">${(probability * 100).toFixed(1)}%</div>
            <div class="metric-label">Match Probability</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">${(1/temperature).toFixed(1)}√ó</div>
            <div class="metric-label">Amplification Factor</div>
          </div>
        </div>
        
        <div class="info" style="margin-top: 15px;">
          <strong>üí° Key Insights:</strong><br>
          ‚Ä¢ <strong>Normalization effect:</strong> Without it, similarity would be ${(imageVector.reduce((sum, val, i) => sum + val * textVector[i], 0)).toFixed(4)} (magnitude-biased)<br>
          ‚Ä¢ <strong>Temperature effect:</strong> Raw similarity ${similarity.toFixed(3)} becomes logit ${logit.toFixed(2)} (${logit > 0 ? 'amplified' : 'dampened'})<br>
          ‚Ä¢ <strong>Interpretation:</strong> ${similarity > 0.7 ? 'Very similar concepts' : similarity > 0.3 ? 'Somewhat related concepts' : similarity > 0 ? 'Weakly related concepts' : 'Different/opposite concepts'}
        </div>
      </div>`;
  }
}

function visualizeNormalization() {
  const vectorAStr = document.getElementById('vectorA')?.value || '3, 4';
  const vectorBStr = document.getElementById('vectorB')?.value || '30, 40';
  
  // Parse vectors (assuming 2D for visualization)
  const vectorA = vectorAStr.split(',').map(x => parseFloat(x.trim()));
  const vectorB = vectorBStr.split(',').map(x => parseFloat(x.trim()));
  
  if (vectorA.length !== 2 || vectorB.length !== 2 || vectorA.some(isNaN) || vectorB.some(isNaN)) {
    document.getElementById('normalizationViz').innerHTML = `
      <div class="warning">‚ö†Ô∏è Please enter valid 2D vectors (e.g., "3, 4")</div>`;
    return;
  }
  
  // Calculate norms
  const normA = Math.sqrt(vectorA[0] * vectorA[0] + vectorA[1] * vectorA[1]);
  const normB = Math.sqrt(vectorB[0] * vectorB[0] + vectorB[1] * vectorB[1]);
  
  // Normalize
  const normalizedA = [vectorA[0] / normA, vectorA[1] / normA];
  const normalizedB = [vectorB[0] / normB, vectorB[1] / normB];
  
  // Calculate similarities
  const rawSimilarity = vectorA[0] * vectorB[0] + vectorA[1] * vectorB[1];
  const normalizedSimilarity = normalizedA[0] * normalizedB[0] + normalizedA[1] * normalizedB[1];
  
  // Calculate angle
  const angle = Math.acos(normalizedSimilarity) * 180 / Math.PI;
  
  const viz = document.getElementById('normalizationViz');
  if (viz) {
    viz.innerHTML = `
      <div class="training-simulation">
        <h4>üìê Before vs After Normalization</h4>
        
        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
          <div style="background: #fff3cd; padding: 15px; border-radius: 8px;">
            <h5>‚ùå Before Normalization</h5>
            <strong>Vector A:</strong> [${vectorA.join(', ')}] (length: ${normA.toFixed(2)})<br>
            <strong>Vector B:</strong> [${vectorB.join(', ')}] (length: ${normB.toFixed(2)})<br>
            <strong>Dot Product:</strong> ${rawSimilarity.toFixed(2)}<br>
            <em>‚ö†Ô∏è Biased by vector lengths!</em>
          </div>
          
          <div style="background: #d4edda; padding: 15px; border-radius: 8px;">
            <h5>‚úÖ After Normalization</h5>
            <strong>Vector A:</strong> [${normalizedA.map(v => v.toFixed(3)).join(', ')}] (length: 1.000)<br>
            <strong>Vector B:</strong> [${normalizedB.map(v => v.toFixed(3)).join(', ')}] (length: 1.000)<br>
            <strong>Cosine Similarity:</strong> ${normalizedSimilarity.toFixed(3)}<br>
            <em>‚úÖ Only depends on direction!</em>
          </div>
        </div>
        
        <div class="metric-grid">
          <div class="metric-card">
            <div class="metric-value">${angle.toFixed(1)}¬∞</div>
            <div class="metric-label">Angle Between Vectors</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">${normalizedSimilarity.toFixed(3)}</div>
            <div class="metric-label">Cosine Similarity</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">${(normB / normA).toFixed(1)}√ó</div>
            <div class="metric-label">Length Ratio (B/A)</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">${Math.abs(rawSimilarity / normalizedSimilarity).toFixed(1)}√ó</div>
            <div class="metric-label">Bias Factor</div>
          </div>
        </div>
        
        <div class="info">
          <strong>üéØ Key Insight:</strong><br>
          Vectors [${vectorA.join(', ')}] and [${vectorB.join(', ')}] point in the ${angle < 10 ? 'same' : angle < 45 ? 'similar' : angle < 90 ? 'somewhat different' : 'very different'} direction (${angle.toFixed(1)}¬∞ apart).<br><br>
          
          <strong>Without normalization:</strong> Dot product = ${rawSimilarity.toFixed(2)} (magnitude-biased)<br>
          <strong>With normalization:</strong> Cosine similarity = ${normalizedSimilarity.toFixed(3)} (pure direction)<br><br>
          
          <strong>üí° This is why CLIP normalizes:</strong> We want similarity based on <em>semantic direction</em>, not vector magnitude!
        </div>
      </div>`;
  }
}

function exploreTemperature() {
  const similaritiesStr = document.getElementById('tempSimilarities')?.value || '0.8, 0.3, 0.1, -0.2, -0.5';
  const temperature = parseFloat(document.getElementById('tempScaleSlider')?.value || '0.07');
  
  // Update temperature display
  document.getElementById('tempScaleValue').textContent = temperature.toFixed(2);
  
  // Parse similarities
  const similarities = similaritiesStr.split(',').map(x => parseFloat(x.trim()));
  
  if (similarities.some(isNaN)) {
    document.getElementById('temperatureExploration').innerHTML = `
      <div class="warning">‚ö†Ô∏è Please enter valid similarity values (e.g., "0.8, 0.3, 0.1, -0.2, -0.5")</div>`;
    return;
  }
  
  // Calculate logits
  const logits = similarities.map(s => s / temperature);
  
  // Calculate softmax probabilities
  const expLogits = logits.map(logit => Math.exp(logit));
  const sumExp = expLogits.reduce((sum, exp) => sum + exp, 0);
  const probabilities = expLogits.map(exp => exp / sumExp);
  
  // Calculate entropy
  const entropy = -probabilities.reduce((sum, p) => sum + (p > 0 ? p * Math.log2(p) : 0), 0);
  
  // For comparison, also calculate with different temperatures
  const temperatures = [0.01, 0.07, 0.2, 0.5, 1.0];
  const tempComparisons = temperatures.map(temp => {
    const tempLogits = similarities.map(s => s / temp);
    const tempExpLogits = tempLogits.map(logit => Math.exp(logit));
    const tempSumExp = tempExpLogits.reduce((sum, exp) => sum + exp, 0);
    const tempProbs = tempExpLogits.map(exp => exp / tempSumExp);
    const tempEntropy = -tempProbs.reduce((sum, p) => sum + (p > 0 ? p * Math.log2(p) : 0), 0);
    
    return {
      temperature: temp,
      probabilities: tempProbs,
      entropy: tempEntropy,
      maxProb: Math.max(...tempProbs)
    };
  });
  
  const exploration = document.getElementById('temperatureExploration');
  if (exploration) {
    exploration.innerHTML = `
      <div class="training-simulation">
        <h4>üå°Ô∏è Temperature Effects on Same Similarities</h4>
        
        <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 15px 0;">
          <strong>üìä Current Temperature œÑ = ${temperature}</strong><br>
          <div style="display: grid; grid-template-columns: repeat(5, 1fr); gap: 10px; margin: 10px 0; font-family: monospace; font-size: 12px;">
            <strong>Similarity</strong>
            <strong>Logit (s/œÑ)</strong>
            <strong>exp(logit)</strong>
            <strong>Probability</strong>
            <strong>Percentage</strong>
            
            ${similarities.map((sim, i) => `
              <div>${sim.toFixed(2)}</div>
              <div>${logits[i].toFixed(2)}</div>
              <div>${expLogits[i].toFixed(2)}</div>
              <div>${probabilities[i].toFixed(4)}</div>
              <div style="font-weight: bold; color: ${i === 0 ? '#28a745' : '#666'}">${(probabilities[i] * 100).toFixed(1)}%</div>
            `).join('')}
          </div>
        </div>
        
        <div class="metric-grid">
          <div class="metric-card">
            <div class="metric-value">${(probabilities[0] * 100).toFixed(1)}%</div>
            <div class="metric-label">Top Prediction</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">${entropy.toFixed(2)}</div>
            <div class="metric-label">Entropy (bits)</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">${(1/temperature).toFixed(1)}√ó</div>
            <div class="metric-label">Amplification Factor</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">${Math.max(...logits).toFixed(1)}</div>
            <div class="metric-label">Max Logit</div>
          </div>
        </div>
        
        <h5>üîç Temperature Comparison Table</h5>
        <div style="background: white; padding: 15px; border-radius: 8px; overflow-x: auto;">
          <table style="width: 100%; border-collapse: collapse; font-size: 12px;">
            <tr style="background: #e9ecef;">
              <th style="padding: 8px; border: 1px solid #dee2e6;">Temperature (œÑ)</th>
              <th style="padding: 8px; border: 1px solid #dee2e6;">Max Probability</th>
              <th style="padding: 8px; border: 1px solid #dee2e6;">Entropy</th>
              <th style="padding: 8px; border: 1px solid #dee2e6;">Interpretation</th>
            </tr>
            ${tempComparisons.map(comp => `
              <tr style="background: ${Math.abs(comp.temperature - temperature) < 0.01 ? '#d4edda' : 'white'};">
                <td style="padding: 8px; border: 1px solid #dee2e6; font-weight: bold;">${comp.temperature}</td>
                <td style="padding: 8px; border: 1px solid #dee2e6;">${(comp.maxProb * 100).toFixed(1)}%</td>
                <td style="padding: 8px; border: 1px solid #dee2e6;">${comp.entropy.toFixed(2)}</td>
                <td style="padding: 8px; border: 1px solid #dee2e6;">
                  ${comp.temperature < 0.05 ? 'üì¢ Overconfident' : 
                    comp.temperature < 0.15 ? '‚úÖ Confident' : 
                    comp.temperature < 0.4 ? 'ü§î Moderate' : 'ü§∑ Uncertain'}
                </td>
              </tr>
            `).join('')}
          </table>
        </div>
        
        <div class="success" style="margin-top: 15px;">
          <strong>üéØ Temperature Analysis:</strong><br>
          ‚Ä¢ <strong>Current setting (œÑ=${temperature}):</strong> ${temperature < 0.1 ? 'High confidence, sharp decisions' : temperature < 0.3 ? 'Balanced confidence' : 'Low confidence, spread out probabilities'}<br>
          ‚Ä¢ <strong>Entropy = ${entropy.toFixed(2)} bits:</strong> ${entropy < 1 ? 'Very focused distribution' : entropy < 2 ? 'Moderately focused' : entropy < 3 ? 'Somewhat spread out' : 'Very uncertain'}<br>
          ‚Ä¢ <strong>Why œÑ=0.07 works:</strong> ${Math.abs(temperature - 0.07) < 0.02 ? 'Perfect balance of confidence and calibration!' : 'Compare with œÑ=0.07 for optimal balance'}<br><br>
          
          <strong>üí° Key Insight:</strong> Temperature doesn't change the <em>ranking</em> of probabilities, only their <em>sharpness</em>. The best similarity (${Math.max(...similarities).toFixed(2)}) always wins, but œÑ controls by how much!
        </div>
      </div>`;
  }
}

// Update temperature display when slider moves
document.addEventListener('DOMContentLoaded', () => {
  const tempSlider = document.getElementById('tempScaleSlider');
  if (tempSlider) {
    tempSlider.addEventListener('input', () => {
      document.getElementById('tempScaleValue').textContent = tempSlider.value;
    });
  }
});
  </script>
</body>
</html>
