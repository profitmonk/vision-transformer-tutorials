<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Deploying VLAs: Hardware, Integration & Production</title>
  <style>
    body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:1200px;margin:0 auto;padding:20px;background:linear-gradient(135deg,#1a1a1a 0%,#2d2d2d 100%);color:#e0e0e0;line-height:1.6}
    .container{background:#fff;color:#2d2d2d;border-radius:20px;padding:30px;margin:20px 0;border:1px solid #e0e0e0;box-shadow:0 4px 20px rgba(0,0,0,.1)}
    .nav-bar{background:#2d2d2d;color:#fff;padding:15px 30px;border-radius:15px;margin:20px 0;display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:15px}
    .nav-home,.nav-prev,.nav-next{background:#fff;color:#2d2d2d;padding:8px 16px;border-radius:6px;text-decoration:none;font-weight:bold;transition:all .3s}
    .nav-next{background:#28a745;color:#fff}
    .nav-home:hover,.nav-prev:hover,.nav-next:hover{transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    .nav-title{font-size:1.2em;font-weight:bold;flex:1;min-width:300px}
    .step{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;margin:15px 0;border-radius:15px;border-left:4px solid #2d2d2d}
    .interactive-demo{background:#f8f9fa;border:2px solid #e9ecef;border-radius:15px;padding:25px;margin:20px 0}
    .demo-title{font-size:1.3em;font-weight:bold;margin-bottom:20px;text-align:center;color:#2d2d2d}
    .controls{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .control-group{display:flex;flex-direction:column;gap:5px}
    .control-group label{font-weight:bold;font-size:14px;color:#2d2d2d}
    button{background:#2d2d2d;border:none;color:#fff;padding:12px 24px;border-radius:8px;cursor:pointer;font-weight:bold;transition:all .3s;margin:5px}
    button:hover{background:#1a1a1a;transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    button.primary{background:#28a745}
    button.primary:hover{background:#218838}
    button.danger{background:#dc3545}
    button.danger:hover{background:#c82333}
    input,select,textarea{padding:8px 12px;border:1px solid #dadce0;border-radius:6px;background:#fff;color:#2d2d2d}
    textarea{min-height:70px;resize:vertical}
    .math-formula{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;text-align:center;font-size:16px;box-shadow:0 2px 8px rgba(0,0,0,.1)}
    .code-block{background:#2d2d2d;color:#e0e0e0;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;font-size:14px;overflow:auto;position:relative}
    .code-header{background:#1a1a1a;color:#28a745;padding:8px 15px;margin:-20px -20px 15px -20px;border-radius:8px 8px 0 0;font-weight:bold;font-size:12px}
    .copy-button{position:absolute;top:10px;right:10px;background:#28a745;color:#fff;border:none;padding:4px 8px;border-radius:4px;cursor:pointer;font-size:10px}
    .hardware-flow{display:flex;justify-content:space-between;align-items:center;margin:20px 0;padding:20px;background:#fff;border-radius:12px;border:2px solid #e9ecef;flex-wrap:wrap;gap:15px}
    .hardware-component{background:#f8f9fa;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center;min-width:120px;flex:1}
    .hardware-component.cloud{border-color:#007bff}
    .hardware-component.edge{border-color:#28a745}
    .hardware-component.mobile{border-color:#fd7e14}
    .hardware-component.jetson{border-color:#6f42c1}
    .hardware-arrow{font-size:24px;color:#28a745;font-weight:bold}
    .deployment-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:20px;margin:20px 0}
    .deployment-card{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;transition:all .3s;cursor:pointer;position:relative}
    .deployment-card:hover{border-color:#28a745;transform:translateY(-3px);box-shadow:0 6px 20px rgba(0,0,0,.1)}
    .deployment-card.selected{border-color:#28a745;background:#d4edda}
    .deployment-title{font-size:1.3em;font-weight:bold;margin-bottom:10px;color:#2d2d2d}
    .deployment-specs{background:#f8f9fa;padding:12px;border-radius:6px;margin:10px 0;font-size:12px;font-family:'Courier New',monospace}
    .deployment-metrics{margin:15px 0}
    .metric{display:flex;align-items:center;margin:5px 0;font-size:14px}
    .metric-icon{margin-right:8px;font-size:16px}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(150px,1fr));gap:15px;margin:20px 0}
    .metric-card{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center}
    .metric-value{font-size:1.5em;font-weight:bold;color:#2d2d2d;margin-bottom:5px}
    .metric-label{font-size:12px;color:#666}
    .robot-integration{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0}
    .integration-stage{background:#f8f9fa;border:1px solid #e9ecef;border-radius:8px;padding:15px;margin:10px 0;position:relative}
    .stage-number{background:#28a745;color:#fff;width:30px;height:30px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;position:absolute;left:-15px;top:50%;transform:translateY(-50%)}
    .integration-content{margin-left:30px}
    .tabs{display:flex;margin-bottom:20px;border-bottom:2px solid #e9ecef;flex-wrap:wrap}
    .tab{padding:12px 20px;cursor:pointer;border-bottom:3px solid transparent;font-weight:bold;transition:all .3s;margin-bottom:-2px}
    .tab:hover{background:#f8f9fa}
    .tab.active{border-bottom-color:#28a745;background:#d4edda;color:#155724}
    .tab-content{display:none}
    .tab-content.active{display:block}
    .info{background:#d1ecf1;border-left:4px solid #17a2b8;color:#0c5460;padding:15px;border-radius:8px;margin:15px 0}
    .success{background:#d4edda;border-left:4px solid #28a745;color:#155724;padding:15px;border-radius:8px;margin:15px 0}
    .warning{background:#fff3cd;border-left:4px solid #ffc107;color:#856404;padding:15px;border-radius:8px;margin:15px 0}
    .danger{background:#f8d7da;border-left:4px solid #dc3545;color:#721c24;padding:15px;border-radius:8px;margin:15px 0}
    .breakthrough-highlight{background:linear-gradient(135deg,#28a745,#20c997);color:#fff;padding:20px;border-radius:12px;text-align:center;margin:20px 0;font-size:18px;font-weight:bold;box-shadow:0 4px 15px rgba(40,167,69,.3)}
    .deployment-simulator{background:linear-gradient(135deg,#e3f2fd,#f3e5f5);border:3px solid #28a745;border-radius:15px;padding:25px;margin:20px 0}
    .sim-controls{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:20px;margin:20px 0}
    .sim-output{background:#fff;border:2px solid #28a745;border-radius:10px;padding:20px;margin:15px 0}
    .cost-breakdown{background:#fff;border:1px solid #e9ecef;border-radius:8px;margin:10px 0;overflow:hidden}
    .cost-header{background:#2d2d2d;color:#fff;padding:10px 15px;font-weight:bold}
    .cost-item{display:flex;justify-content:space-between;padding:8px 15px;border-bottom:1px solid #e9ecef}
    .cost-total{background:#28a745;color:#fff;padding:10px 15px;font-weight:bold}
    .progress-bar{background:#e9ecef;border-radius:10px;height:8px;margin:10px 0;overflow:hidden}
    .progress-fill{background:#28a745;height:100%;transition:width 1s ease}
    .benchmark-table{width:100%;border-collapse:collapse;margin:15px 0;background:#fff;border:1px solid #e9ecef;border-radius:8px;overflow:hidden}
    .benchmark-table th{background:#2d2d2d;color:#fff;padding:12px;text-align:center;font-weight:bold}
    .benchmark-table td{padding:12px;text-align:center;border-bottom:1px solid #e9ecef}
    .score-excellent{background:#d4edda;color:#155724;font-weight:bold}
    .score-good{background:#d1ecf1;color:#0c5460;font-weight:bold}
    .score-average{background:#fff3cd;color:#856404;font-weight:bold}
    .score-poor{background:#f8d7da;color:#721c24;font-weight:bold}
    .performance-indicator{display:inline-block;width:20px;height:20px;border-radius:50%;margin:0 5px;position:relative}
    .perf-excellent{background:#28a745}
    .perf-good{background:#17a2b8}
    .perf-average{background:#ffc107}
    .perf-poor{background:#dc3545}
    .robot-workspace{background:#fff;border:2px dashed #28a745;border-radius:10px;padding:20px;margin:15px 0;min-height:200px;position:relative;text-align:center}
    .robot-avatar{font-size:4em;margin:20px 0;animation:robotDeploy 3s ease-in-out infinite}
    @keyframes robotDeploy{0%{transform:scale(1) rotate(0deg)}50%{transform:scale(1.05) rotate(5deg)}100%{transform:scale(1) rotate(0deg)}}
    .optimization-panel{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0}
    .optimization-metric{background:#f8f9fa;border:1px solid #e9ecef;border-radius:6px;padding:10px;margin:8px 0;display:flex;justify-content:space-between;align-items:center}
    .optimization-before{color:#dc3545;font-weight:bold}
    .optimization-after{color:#28a745;font-weight:bold}
    .safety-checklist{background:#fff3cd;border:2px solid #ffc107;border-radius:10px;padding:20px;margin:15px 0}
    .checklist-item{display:flex;align-items:center;margin:10px 0;padding:8px;background:#fff;border-radius:6px}
    .checklist-item input[type="checkbox"]{margin-right:10px;transform:scale(1.2)}
    .production-status{background:#f8f9fa;border:2px solid #e9ecef;border-radius:10px;padding:20px;margin:15px 0;text-align:center}
    .status-indicator{display:inline-block;width:15px;height:15px;border-radius:50%;margin:0 8px}
    .status-online{background:#28a745}
    .status-warning{background:#ffc107}
    .status-offline{background:#dc3545}
    .jetson-specs{background:#6f42c1;color:#fff;border-radius:10px;padding:20px;margin:15px 0}
    .jetson-spec{display:flex;justify-content:space-between;align-items:center;padding:5px 0;border-bottom:1px solid rgba(255,255,255,0.2)}
    .jetson-spec:last-child{border-bottom:none}
    .case-study{background:#f8f9fa;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0}
    .case-title{font-size:1.2em;font-weight:bold;color:#2d2d2d;margin-bottom:10px}
    .case-company{background:#28a745;color:#fff;padding:4px 8px;border-radius:4px;font-size:12px;font-weight:bold;display:inline-block;margin-bottom:10px}
    .case-metrics{display:grid;grid-template-columns:repeat(auto-fit,minmax(120px,1fr));gap:10px;margin:10px 0}
    .case-metric{text-align:center;padding:8px;background:#fff;border-radius:6px;border:1px solid #e9ecef}
    .case-value{font-size:1.2em;font-weight:bold;color:#28a745}
    .case-label{font-size:10px;color:#666}
    .integration-wizard{background:#fff;border:2px solid #28a745;border-radius:12px;padding:20px;margin:15px 0}
    .wizard-step{display:none}
    .wizard-step.active{display:block}
    .wizard-navigation{display:flex;justify-content:space-between;align-items:center;margin-top:20px}
    .wizard-progress{background:#e9ecef;border-radius:10px;height:8px;margin:20px 0;overflow:hidden}
    .wizard-progress-fill{background:#28a745;height:100%;transition:width 0.3s ease}
    .terminal-output{background:#000;color:#00ff00;padding:15px;border-radius:8px;margin:10px 0;font-family:'Courier New',monospace;font-size:12px;min-height:100px;overflow-y:auto}
    .terminal-line{margin:2px 0}
    .terminal-prompt{color:#ffff00}
    .terminal-success{color:#00ff00}
    .terminal-error{color:#ff0000}
    .monitoring-dashboard{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .monitor-widget{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center}
    .monitor-value{font-size:2em;font-weight:bold;margin:10px 0}
    .monitor-label{font-size:12px;color:#666}
    .monitor-trend{font-size:12px;margin-top:5px}
    .trend-up{color:#28a745}
    .trend-down{color:#dc3545}
    .trend-stable{color:#6c757d}
  </style>
</head>
<body>
  <div class="nav-bar">
    <div class="nav-title">🚀 Deploying VLAs: Hardware, Integration & Production</div>
    <a href="index.html" class="nav-home">🏠 Home</a>
    <a href="training-vlas.html" class="nav-prev">← Training VLAs</a>
    <a href="advanced-vla-robotics.html" class="nav-next">Next: Advanced VLA & Future →</a>
  </div>

  <div class="container">
    <h1>🚀 From Training to Production: Deploying VLA Models on Real Robots</h1>
    <p>Taking your trained VLA model from GPU clusters to real robots requires mastering hardware optimization, safety systems, and production deployment strategies. This tutorial covers the complete deployment pipeline: from edge AI hardware selection to production monitoring, with real-world case studies and hands-on integration guides.</p>
    
    <div class="breakthrough-highlight">
      🎯 The Mission: Deploy VLA models that can safely and reliably control real robots in production environments
    </div>
  </div>

  <div class="container">
    <h2>🖥️ Section 1: Hardware Deployment Strategies</h2>
    
    <div class="step">
      <h3>⚖️ Cloud vs Edge vs Mobile: The Deployment Spectrum</h3>
      <p>VLA deployment success depends on choosing the right hardware platform for your specific requirements. Each approach offers different trade-offs between performance, latency, cost, and scalability.</p>

      <div class="hardware-flow">
        <div class="hardware-component cloud">
          <h4>☁️ Cloud Deployment</h4>
          <div>H100/A100 Clusters</div>
          <div style="font-size:12px;margin-top:5px">
            • Unlimited compute<br>• Highest performance<br>• Network dependent
          </div>
        </div>
        <div class="hardware-arrow">↔</div>
        <div class="hardware-component edge">
          <h4>🎯 Edge Computing</h4>
          <div>Jetson Thor/Orin</div>
          <div style="font-size:12px;margin-top:5px">
            • Local processing<br>• Low latency<br>• Power efficient
          </div>
        </div>
        <div class="hardware-arrow">↔</div>
        <div class="hardware-component mobile">
          <h4>📱 Mobile/Embedded</h4>
          <div>ARM/x86 SoCs</div>
          <div style="font-size:12px;margin-top:5px">
            • Ultra-low power<br>• Consumer cost<br>• Limited capability
          </div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">⚙️ Hardware Selection Calculator</div>
        <p><strong>Find the optimal hardware for your VLA deployment:</strong></p>

        <div class="controls">
          <div class="control-group">
            <label>Robot Application:</label>
            <select id="robotApplication">
              <option value="industrial" selected>Industrial Automation</option>
              <option value="service">Service Robotics</option>
              <option value="consumer">Consumer/Home</option>
              <option value="research">Research & Development</option>
              <option value="warehouse">Warehouse/Logistics</option>
            </select>
          </div>
          <div class="control-group">
            <label>Latency Requirements:</label>
            <select id="latencyReq">
              <option value="realtime">Real-time (&lt;10ms)</option>
              <option value="fast" selected>Fast (&lt;50ms)</option>
              <option value="moderate">Moderate (&lt;200ms)</option>
              <option value="relaxed">Relaxed (&lt;1s)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Power Constraints:</label>
            <select id="powerConstraints">
              <option value="unlimited">Unlimited (AC Power)</option>
              <option value="high" selected>High (200W+)</option>
              <option value="medium">Medium (50-200W)</option>
              <option value="low">Low (&lt;50W)</option>
              <option value="battery">Battery (&lt;15W)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Budget Range:</label>
            <select id="budgetRange">
              <option value="enterprise">Enterprise ($10K+)</option>
              <option value="professional" selected>Professional ($1K-10K)</option>
              <option value="consumer">Consumer ($100-1K)</option>
              <option value="minimal">Minimal (&lt;$100)</option>
            </select>
          </div>
        </div>

        <button onclick="calculateHardware()" class="primary">🔍 Find Optimal Hardware</button>
        <div id="hardwareRecommendation"></div>
      </div>
    </div>

    <div class="step">
      <h3>🚀 NVIDIA Jetson Thor: The Edge AI Revolution</h3>
      <p>NVIDIA's Jetson Thor represents a breakthrough in edge AI for robotics, delivering 2070 TFLOPS of AI performance in a 130W package. This enables real-time VLA inference directly on robots without cloud connectivity.</p>

      <div class="jetson-specs">
        <h4 style="margin-top:0">🏆 Jetson Thor Specifications</h4>
        <div class="jetson-spec">
          <span>AI Performance:</span>
          <span>2070 TFLOPS (FP8)</span>
        </div>
        <div class="jetson-spec">
          <span>CPU:</span>
          <span>12-core ARM Cortex-A78AE</span>
        </div>
        <div class="jetson-spec">
          <span>GPU:</span>
          <span>Next-gen GPU with 4096 CUDA cores</span>
        </div>
        <div class="jetson-spec">
          <span>Memory:</span>
          <span>64GB LPDDR5x</span>
        </div>
        <div class="jetson-spec">
          <span>Power:</span>
          <span>130W total system power</span>
        </div>
        <div class="jetson-spec">
          <span>Price:</span>
          <span>~$3,000 (development kit)</span>
        </div>
        <div class="jetson-spec">
          <span>Availability:</span>
          <span>H2 2025</span>
        </div>
      </div>

      <div class="math-formula">
        <strong>Jetson Thor VLA Performance Analysis:</strong><br><br>
        <strong>Model Capacity:</strong><br>
        Memory: 64GB → Support models up to ~45B parameters (INT8)<br>
        Inference: 2070 TFLOPS → OpenVLA-7B at ~100 tokens/second<br><br>
        <strong>Real-time Constraints:</strong><br>
        Robot control frequency: 50-200Hz → 5-20ms per action<br>
        VLA inference: 10-50ms → Real-time capable with optimization<br><br>
        <strong>Power Efficiency:</strong><br>
        Performance/Watt: 15.9 TFLOPS/W → 10x more efficient than data center GPUs<br>
        Battery operation: 64GB×0.5W + 130W ≈ 162W total → 2-4 hours on large battery
      </div>

      <div class="interactive-demo">
        <div class="demo-title">🎛️ Jetson Performance Simulator</div>
        <div class="controls">
          <div class="control-group">
            <label>VLA Model Size:</label>
            <select id="jetsonModel">
              <option value="smol">SmolVLA (600M)</option>
              <option value="openvla" selected>OpenVLA (7B)</option>
              <option value="large">Large VLA (13B)</option>
              <option value="groot">GR00T-style (20B)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Quantization:</label>
            <select id="jetsonQuant">
              <option value="fp16">FP16 (Full Precision)</option>
              <option value="int8" selected>INT8 (Quantized)</option>
              <option value="int4">INT4 (Ultra Quantized)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Robot Control Frequency:</label>
            <select id="controlFreq">
              <option value="10">10 Hz (Slow)</option>
              <option value="50" selected>50 Hz (Standard)</option>
              <option value="100">100 Hz (Fast)</option>
              <option value="200">200 Hz (Real-time)</option>
            </select>
          </div>
        </div>
        <button onclick="simulateJetsonPerformance()" class="primary">⚡ Simulate Performance</button>
        <div id="jetsonSimResults"></div>
      </div>
    </div>

    <div class="step">
      <h3>💰 Cost-Performance Analysis</h3>
      <div class="deployment-grid">
        <div class="deployment-card" onclick="selectDeployment('cloud', this)">
          <div class="deployment-title">☁️ Cloud Deployment</div>
          <div class="deployment-specs">
            <strong>Hardware:</strong> H100/A100 clusters<br>
            <strong>Latency:</strong> 50-200ms (network dependent)<br>
            <strong>Throughput:</strong> 1000+ robots per cluster<br>
            <strong>Cost:</strong> $5-15/hour per robot<br>
            <strong>Reliability:</strong> 99.9% uptime SLA
          </div>
          <div class="deployment-metrics">
            <div class="metric"><span class="metric-icon">⚡</span><span>Unlimited compute scaling</span></div>
            <div class="metric"><span class="metric-icon">🌐</span><span>Global accessibility</span></div>
            <div class="metric"><span class="metric-icon">🔧</span><span>Easy updates & maintenance</span></div>
            <div class="metric"><span class="metric-icon">💸</span><span>High operational costs</span></div>
          </div>
        </div>

        <div class="deployment-card" onclick="selectDeployment('edge', this)">
          <div class="deployment-title">🎯 Edge Computing</div>
          <div class="deployment-specs">
            <strong>Hardware:</strong> Jetson Thor/Orin, Intel NUC<br>
            <strong>Latency:</strong> 5-25ms (local processing)<br>
            <strong>Throughput:</strong> 1-10 robots per device<br>
            <strong>Cost:</strong> $2K-5K one-time + power<br>
            <strong>Reliability:</strong> 99.5% uptime (local)
          </div>
          <div class="deployment-metrics">
            <div class="metric"><span class="metric-icon">🚀</span><span>Low latency processing</span></div>
            <div class="metric"><span class="metric-icon">🔒</span><span>Data privacy & security</span></div>
            <div class="metric"><span class="metric-icon">📶</span><span>Network independence</span></div>
            <div class="metric"><span class="metric-icon">🔌</span><span>Power consumption (50-200W)</span></div>
          </div>
        </div>

        <div class="deployment-card" onclick="selectDeployment('mobile', this)">
          <div class="deployment-title">📱 Mobile/Embedded</div>
          <div class="deployment-specs">
            <strong>Hardware:</strong> ARM SoCs, x86 mini-PCs<br>
            <strong>Latency:</strong> 20-100ms (limited compute)<br>
            <strong>Throughput:</strong> 1 robot per device<br>
            <strong>Cost:</strong> $100-500 one-time<br>
            <strong>Reliability:</strong> 99% uptime (battery backup)
          </div>
          <div class="deployment-metrics">
            <div class="metric"><span class="metric-icon">🔋</span><span>Ultra-low power (5-25W)</span></div>
            <div class="metric"><span class="metric-icon">💰</span><span>Consumer-grade pricing</span></div>
            <div class="metric"><span class="metric-icon">📦</span><span>Compact form factor</span></div>
            <div class="metric"><span class="metric-icon">⚠️</span><span>Limited model complexity</span></div>
          </div>
        </div>
      </div>
      <div id="deploymentAnalysis"></div>
    </div>

    <div class="step">
      <h3>⚡ Model Optimization for Production</h3>
      <div class="tabs">
        <div class="tab active" onclick="switchOptTab('quantization', this)">🔢 Quantization</div>
        <div class="tab" onclick="switchOptTab('pruning', this)">✂️ Pruning</div>
        <div class="tab" onclick="switchOptTab('tensorrt', this)">🚀 TensorRT</div>
        <div class="tab" onclick="switchOptTab('distillation', this)">🎓 Distillation</div>
      </div>

      <div id="quantization" class="tab-content active">
        <div class="success">
          <strong>🔢 Model Quantization:</strong><br>
          Reduce precision from FP32 to INT8/INT4 while maintaining accuracy. Critical for edge deployment on resource-constrained hardware.
        </div>

        <div class="optimization-panel">
          <h4>Quantization Impact Analysis</h4>
          <div class="optimization-metric">
            <span>Model Size (7B VLA):</span>
            <span><span class="optimization-before">28GB</span> → <span class="optimization-after">7GB</span> (4x reduction)</span>
          </div>
          <div class="optimization-metric">
            <span>Inference Speed:</span>
            <span><span class="optimization-before">45ms</span> → <span class="optimization-after">12ms</span> (3.8x faster)</span>
          </div>
          <div class="optimization-metric">
            <span>Power Consumption:</span>
            <span><span class="optimization-before">180W</span> → <span class="optimization-after">65W</span> (2.8x reduction)</span>
          </div>
          <div class="optimization-metric">
            <span>Accuracy (Task Success):</span>
            <span><span class="optimization-before">84%</span> → <span class="optimization-after">82%</span> (2% drop)</span>
          </div>
        </div>

        <div class="code-block">
          <div class="code-header">🔢 VLA Model Quantization</div>
          <button class="copy-button" onclick="copyCode(this)">📋 Copy</button>
<pre>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch.quantization as quant

class VLAQuantizer:
    """
    Production quantization pipeline for VLA models
    Optimizes models for edge deployment while preserving accuracy
    """
    
    def __init__(self, model_path, target_platform='jetson'):
        self.model_path = model_path
        self.target_platform = target_platform
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """Load pre-trained VLA model"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float32,  # Start with FP32
            device_map='auto'
        )
        print(f"📊 Loaded model: {self.model.num_parameters():,} parameters")
        
    def calibrate_quantization(self, calibration_data, num_samples=100):
        """
        Calibrate quantization using representative robot data
        
        Args:
            calibration_data: Robot demonstration dataset
            num_samples: Number of samples for calibration
        """
        self.model.eval()
        
        # Prepare calibration dataset
        calibration_inputs = []
        for i, sample in enumerate(calibration_data):
            if i >= num_samples:
                break
                
            # Convert robot demonstration to model input format
            input_text = f"Robot task: {sample['instruction']} Image: [visual_tokens] Actions:"
            inputs = self.tokenizer(input_text, return_tensors='pt', truncation=True)
            calibration_inputs.append(inputs)
        
        # Set up post-training quantization
        quantization_config = quant.get_default_qconfig('fbgemm')
        
        if self.target_platform == 'jetson':
            # Optimize for ARM/CUDA inference
            quantization_config = quant.QConfig(
                activation=quant.MinMaxObserver.with_args(dtype=torch.qint8),
                weight=quant.MinMaxObserver.with_args(dtype=torch.qint8)
            )
        
        # Apply quantization-aware training preparation
        self.model.qconfig = quantization_config
        quant.prepare(self.model, inplace=True)
        
        # Calibration pass
        print("🔍 Running calibration...")
        with torch.no_grad():
            for inputs in calibration_inputs:
                _ = self.model(**inputs)
        
        print("✅ Calibration completed")
        
    def quantize_model(self, quantization_mode='dynamic'):
        """
        Apply quantization to the model
        
        Args:
            quantization_mode: 'dynamic', 'static', or 'qat'
        """
        if quantization_mode == 'dynamic':
            # Dynamic quantization (easiest, good for CPU)
            quantized_model = torch.quantization.quantize_dynamic(
                self.model,
                {torch.nn.Linear, torch.nn.MultiheadAttention},
                dtype=torch.qint8
            )
            
        elif quantization_mode == 'static':
            # Static quantization (best for inference)
            quantized_model = torch.quantization.convert(self.model, inplace=False)
            
        else:
            raise ValueError(f"Unsupported quantization mode: {quantization_mode}")
        
        # Measure quantization impact
        original_size = self.get_model_size(self.model)
        quantized_size = self.get_model_size(quantized_model)
        compression_ratio = original_size / quantized_size
        
        print(f"📊 Quantization Results:")
        print(f"  Original size: {original_size:.1f} MB")
        print(f"  Quantized size: {quantized_size:.1f} MB")
        print(f"  Compression ratio: {compression_ratio:.1f}x")
        
        return quantized_model
    
    def get_model_size(self, model):
        """Calculate model size in MB"""
        total_params = 0
        for param in model.parameters():
            total_params += param.numel()
        
        # Estimate size (assuming 4 bytes per parameter for FP32, 1 byte for INT8)
        if hasattr(model, 'qconfig') and model.qconfig is not None:
            size_mb = total_params * 1 / (1024 * 1024)  # INT8
        else:
            size_mb = total_params * 4 / (1024 * 1024)  # FP32
            
        return size_mb
    
    def benchmark_performance(self, quantized_model, test_inputs, num_trials=50):
        """
        Benchmark inference performance
        
        Returns:
            dict: Performance metrics
        """
        import time
        
        # Warmup
        with torch.no_grad():
            for _ in range(5):
                _ = quantized_model(**test_inputs)
        
        # Measure inference time
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        
        inference_times = []
        with torch.no_grad():
            for _ in range(num_trials):
                start_time = time.time()
                outputs = quantized_model(**test_inputs)
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                end_time = time.time()
                
                inference_times.append((end_time - start_time) * 1000)  # Convert to ms
        
        avg_latency = sum(inference_times) / len(inference_times)
        std_latency = (sum((t - avg_latency) ** 2 for t in inference_times) / len(inference_times)) ** 0.5
        
        return {
            'avg_latency_ms': avg_latency,
            'std_latency_ms': std_latency,
            'p95_latency_ms': sorted(inference_times)[int(0.95 * len(inference_times))],
            'throughput_hz': 1000 / avg_latency
        }
    
    def validate_accuracy(self, quantized_model, validation_data):
        """
        Validate that quantization doesn't significantly hurt accuracy
        
        Returns:
            dict: Accuracy metrics before and after quantization
        """
        def evaluate_model(model, data):
            model.eval()
            correct_predictions = 0
            total_predictions = 0
            
            with torch.no_grad():
                for sample in data:
                    inputs = self.tokenizer(sample['input'], return_tensors='pt')
                    outputs = model(**inputs)
                    
                    # Simplified accuracy check (in practice, use task-specific metrics)
                    predictions = torch.argmax(outputs.logits, dim=-1)
                    targets = self.tokenizer(sample['target'], return_tensors='pt')['input_ids']
                    
                    # Compare predictions with targets (simplified)
                    matches = (predictions == targets).float().mean()
                    correct_predictions += matches.item()
                    total_predictions += 1
            
            return correct_predictions / total_predictions
        
        original_accuracy = evaluate_model(self.model, validation_data)
        quantized_accuracy = evaluate_model(quantized_model, validation_data)
        
        accuracy_drop = original_accuracy - quantized_accuracy
        
        return {
            'original_accuracy': original_accuracy,
            'quantized_accuracy': quantized_accuracy,
            'accuracy_drop': accuracy_drop,
            'relative_drop_percent': (accuracy_drop / original_accuracy) * 100
        }

# Example usage for VLA quantization
def quantize_vla_for_production():
    """Complete quantization pipeline for production VLA deployment"""
    
    # Initialize quantizer
    quantizer = VLAQuantizer(
        model_path="openvla-7b-model", 
        target_platform="jetson"
    )
    
    # Load model
    quantizer.load_model()
    
    # Load calibration data (robot demonstrations)
    calibration_data = load_robot_calibration_data()  # Your dataset here
    
    # Calibrate quantization
    quantizer.calibrate_quantization(calibration_data, num_samples=200)
    
    # Apply quantization
    quantized_model = quantizer.quantize_model(quantization_mode='static')
    
    # Benchmark performance
    test_input = quantizer.tokenizer("Pick up the red cube", return_tensors='pt')
    performance = quantizer.benchmark_performance(quantized_model, test_input)
    
    print(f"\n🚀 Production Deployment Metrics:")
    print(f"  Average Latency: {performance['avg_latency_ms']:.1f}ms")
    print(f"  P95 Latency: {performance['p95_latency_ms']:.1f}ms") 
    print(f"  Throughput: {performance['throughput_hz']:.1f} Hz")
    
    # Validate accuracy
    validation_data = load_robot_validation_data()  # Your validation set
    accuracy = quantizer.validate_accuracy(quantized_model, validation_data)
    
    print(f"  Accuracy Drop: {accuracy['relative_drop_percent']:.1f}%")
    
    # Save quantized model for deployment
    torch.jit.save(torch.jit.script(quantized_model), "vla_quantized_production.pt")
    print("💾 Quantized model saved for production deployment")
    
    return quantized_model, performance, accuracy

# Run the quantization pipeline
if __name__ == "__main__":
    model, perf, acc = quantize_vla_for_production()</pre>
        </div>
      </div>

      <div id="pruning" class="tab-content">
        <div class="info">
          <strong>✂️ Model Pruning:</strong><br>
          Remove unnecessary parameters and connections while preserving model performance. Structured pruning removes entire neurons/layers, while unstructured pruning removes individual weights.
        </div>

        <div class="warning">
          <strong>⚠️ Pruning Considerations for VLA Models:</strong><br>
          VLA models have complex cross-modal connections between vision, language, and action components. Aggressive pruning can disproportionately harm cross-embodiment transfer and instruction following capabilities.<br><br>
          <strong>Recommended Approach:</strong><br>
          • Start with magnitude-based pruning (10-30% sparsity)<br>
          • Preserve attention layers (critical for multimodal reasoning)<br>
          • Use gradual pruning with fine-tuning<br>
          • Validate on diverse robot tasks, not just perplexity
        </div>
      </div>

      <div id="tensorrt" class="tab-content">
        <div class="success">
          <strong>🚀 TensorRT Optimization:</strong><br>
          NVIDIA TensorRT provides automatic optimization for GPU inference, including kernel fusion, precision calibration, and memory optimization specifically for transformer architectures.
        </div>

        <div class="optimization-panel">
          <h4>TensorRT Performance Gains</h4>
          <div class="optimization-metric">
            <span>Inference Latency:</span>
            <span><span class="optimization-before">45ms</span> → <span class="optimization-after">18ms</span> (2.5x faster)</span>
          </div>
          <div class="optimization-metric">
            <span>Memory Usage:</span>
            <span><span class="optimization-before">14GB</span> → <span class="optimization-after">9GB</span> (36% reduction)</span>
          </div>
          <div class="optimization-metric">
            <span>Throughput:</span>
            <span><span class="optimization-before">22 fps</span> → <span class="optimization-after">56 fps</span> (2.5x increase)</span>
          </div>
          <div class="optimization-metric">
            <span>Power Efficiency:</span>
            <span><span class="optimization-before">180W</span> → <span class="optimization-after">140W</span> (22% reduction)</span>
          </div>
        </div>

        <div class="info">
          <strong>🎯 TensorRT Best Practices for VLA:</strong><br>
          • Use FP16 precision for 2x speedup with minimal accuracy loss<br>
          • Enable kernel fusion for transformer blocks<br>
          • Optimize for specific input shapes (batch size, sequence length)<br>
          • Use dynamic shapes carefully (can hurt performance)<br>
          • Profile thoroughly on target hardware (Jetson vs data center)
        </div>
      </div>

      <div id="distillation" class="tab-content">
        <div class="warning">
          <strong>🎓 Knowledge Distillation:</strong><br>
          Train a smaller "student" VLA model to mimic a larger "teacher" model. Particularly effective for creating efficient edge-deployable versions.
        </div>

        <div class="math-formula">
          <strong>VLA Knowledge Distillation:</strong><br><br>
          <strong>Standard Training:</strong> ℒ<sub>student</sub> = CrossEntropy(y<sub>pred</sub>, y<sub>true</sub>)<br><br>
          <strong>Distillation Loss:</strong> ℒ<sub>distill</sub> = KL_Divergence(softmax(z<sub>student</sub>/T), softmax(z<sub>teacher</sub>/T))<br><br>
          <strong>Combined Objective:</strong> ℒ<sub>total</sub> = α × ℒ<sub>student</sub> + (1-α) × ℒ<sub>distill</sub><br><br>
          <strong>Where:</strong> T = temperature parameter, α = balance weight
        </div>

        <div class="success">
          <strong>📊 Typical Distillation Results:</strong><br>
          • <strong>Model Size:</strong> 7B → 1.5B (4.7x smaller)<br>
          • <strong>Inference Speed:</strong> 45ms → 12ms (3.8x faster)<br>
          • <strong>Performance Retention:</strong> 84% → 79% (94% of original)<br>
          • <strong>Memory Usage:</strong> 14GB → 3GB (4.7x reduction)<br><br>
          <strong>🎯 Best Use Cases:</strong> Consumer robotics, battery-powered systems, cost-sensitive deployments
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>🤖 Section 2: Real Robot Integration</h2>

    <div class="step">
      <h3>🔧 Robot Integration Pipeline</h3>
      <p>Integrating VLA models with real robots requires careful attention to control interfaces, safety systems, and real-time constraints. This section covers the complete integration pipeline with working examples.</p>

      <div class="robot-integration">
        <div class="integration-stage">
          <div class="stage-number">1</div>
          <div class="integration-content">
            <h4>🔌 Hardware Interface Setup</h4>
            <p>Establish communication between VLA inference system and robot controllers (ROS, EtherCAT, CAN bus)</p>
          </div>
        </div>
        <div class="integration-stage">
          <div class="stage-number">2</div>
          <div class="integration-content">
            <h4>📷 Sensor Integration</h4>
            <p>Configure cameras, depth sensors, IMUs, and other sensors for real-time data streaming</p>
          </div>
        </div>
        <div class="integration-stage">
          <div class="stage-number">3</div>
          <div class="integration-content">
            <h4>🛡️ Safety System Implementation</h4>
            <p>Deploy emergency stops, workspace boundaries, collision detection, and fail-safe mechanisms</p>
          </div>
        </div>
        <div class="integration-stage">
          <div class="stage-number">4</div>
          <div class="integration-content">
            <h4>⚡ Real-Time Control Loop</h4>
            <p>Implement real-time action execution with proper timing, interpolation, and error handling</p>
          </div>
        </div>
        <div class="integration-stage">
          <div class="stage-number">5</div>
          <div class="integration-content">
            <h4>📊 Monitoring & Logging</h4>
            <p>Setup comprehensive monitoring, telemetry, and data logging for production operation</p>
          </div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">🧙‍♂️ Robot Integration Wizard</div>
        <div class="integration-wizard">
          <div class="wizard-progress">
            <div class="wizard-progress-fill" id="wizardProgress" style="width: 0%"></div>
          </div>
          
          <div class="wizard-step active" id="step1">
            <h4>Step 1: Robot Selection</h4>
            <div class="controls">
              <div class="control-group">
                <label>Robot Platform:</label>
                <select id="wizardRobot">
                  <option value="franka" selected>Franka Emika Panda</option>
                  <option value="ur5">Universal Robot UR5</option>
                  <option value="aloha">ALOHA Bimanual</option>
                  <option value="mobile">Mobile Manipulator</option>
                  <option value="custom">Custom Robot</option>
                </select>
              </div>
              <div class="control-group">
                <label>Control Interface:</label>
                <select id="wizardInterface">
                  <option value="ros" selected>ROS/ROS2</option>
                  <option value="ethercat">EtherCAT</option>
                  <option value="canbus">CAN Bus</option>
                  <option value="tcp">TCP/IP</option>
                  <option value="serial">Serial/UART</option>
                </select>
              </div>
            </div>
          </div>

          <div class="wizard-step" id="step2">
            <h4>Step 2: VLA Model Configuration</h4>
            <div class="controls">
              <div class="control-group">
                <label>VLA Model:</label>
                <select id="wizardModel">
                  <option value="openvla" selected>OpenVLA (7B)</option>
                  <option value="smolvla">SmolVLA (600M)</option>
                  <option value="custom">Custom Model</option>
                </select>
              </div>
              <div class="control-group">
                <label>Deployment Hardware:</label>
                <select id="wizardHardware">
                  <option value="jetson" selected>Jetson Orin/Thor</option>
                  <option value="cloud">Cloud GPU</option>
                  <option value="workstation">Local Workstation</option>
                </select>
              </div>
            </div>
          </div>

          <div class="wizard-step" id="step3">
            <h4>Step 3: Safety Configuration</h4>
            <div class="safety-checklist">
              <div class="checklist-item">
                <input type="checkbox" id="emergencyStop">
                <label for="emergencyStop">Emergency Stop System</label>
              </div>
              <div class="checklist-item">
                <input type="checkbox" id="workspaceBounds">
                <label for="workspaceBounds">Workspace Boundaries</label>
              </div>
              <div class="checklist-item">
                <input type="checkbox" id="collisionDetection">
                <label for="collisionDetection">Collision Detection</label>
              </div>
              <div class="checklist-item">
                <input type="checkbox" id="forceLimit">
                <label for="forceLimit">Force/Torque Limits</label>
              </div>
              <div class="checklist-item">
                <input type="checkbox" id="humanDetection">
                <label for="humanDetection">Human Presence Detection</label>
              </div>
            </div>
          </div>

          <div class="wizard-step" id="step4">
            <h4>Step 4: Integration Code Generation</h4>
            <div class="terminal-output" id="integrationCode"></div>
          </div>

          <div class="wizard-navigation">
            <button onclick="previousWizardStep()" id="prevBtn" disabled>← Previous</button>
            <span id="stepIndicator">Step 1 of 4</span>
            <button onclick="nextWizardStep()" id="nextBtn" class="primary">Next →</button>
          </div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>🦾 Platform-Specific Integration Guides</h3>
      <div class="tabs">
        <div class="tab active" onclick="switchPlatformTab('franka', this)">🦾 Franka Panda</div>
        <div class="tab" onclick="switchPlatformTab('ur5', this)">🤖 UR5</div>
        <div class="tab" onclick="switchPlatformTab('aloha', this)">👐 ALOHA</div>
        <div class="tab" onclick="switchPlatformTab('mobile', this)">🚛 Mobile</div>
      </div>

      <div id="franka" class="tab-content active">
        <div class="code-block">
          <div class="code-header">🦾 Franka Panda VLA Integration</div>
          <button class="copy-button" onclick="copyCode(this)">📋 Copy</button>
<pre>#!/usr/bin/env python3
"""
Franka Panda VLA Integration
Real-time robot control using VLA model inference
"""

import rospy
import numpy as np
import torch
from franka_msgs.msg import FrankaState
from geometry_msgs.msg import PoseStamped
from sensor_msgs.msg import JointState, Image
from your_vla_model import VLAModel  # Your trained VLA model

class FrankaVLAController:
    """
    Production-ready VLA controller for Franka Panda
    Handles real-time inference, safety, and control
    """
    
    def __init__(self):
        # Initialize ROS
        rospy.init_node('franka_vla_controller')
        
        # Load VLA model (quantized for edge deployment)
        self.vla_model = VLAModel.from_pretrained(
            'openvla-7b-quantized',
            device='cuda',
            torch_dtype=torch.float16
        )
        self.vla_model.eval()
        
        # Robot state
        self.current_joint_states = np.zeros(7)
        self.current_pose = None
        self.current_image = None
        self.task_instruction = ""
        
        # Safety limits (Franka Panda specifications)
        self.joint_limits = {
            'lower': np.array([-2.8973, -1.7628, -2.8973, -3.0718, -2.8973, -0.0175, -2.8973]),
            'upper': np.array([2.8973, 1.7628, 2.8973, -0.0698, 2.8973, 3.7525, 2.8973])
        }
        self.max_joint_velocity = np.array([2.1750, 2.1750, 2.1750, 2.1750, 2.6100, 2.6100, 2.6100])
        self.workspace_bounds = {
            'x': [0.3, 0.8], 'y': [-0.3, 0.3], 'z': [0.0, 0.8]
        }
        
        # Control parameters
        self.control_frequency = 50  # Hz
        self.emergency_stop = False
        self.last_action_time = rospy.Time.now()
        
        # ROS publishers and subscribers
        self.joint_cmd_pub = rospy.Publisher(
            '/franka_ros_interface/motion_controller/arm/joint_commands',
            JointState, queue_size=1
        )
        
        rospy.Subscriber('/franka_ros_interface/franka_states', 
                        FrankaState, self.franka_state_callback)
        rospy.Subscriber('/camera/color/image_raw', 
                        Image, self.image_callback)
        rospy.Subscriber('/vla_task_instruction', 
                        String, self.instruction_callback)
        
        # Emergency stop service
        rospy.Service('/emergency_stop', SetBool, self.emergency_stop_callback)
        
        # Main control loop
        self.control_timer = rospy.Timer(
            rospy.Duration(1.0 / self.control_frequency), 
            self.control_loop
        )
        
        rospy.loginfo("🚀 Franka VLA Controller initialized")
    
    def franka_state_callback(self, msg):
        """Process robot state updates"""
        self.current_joint_states = np.array(msg.q)
        
        # Extract end-effector pose
        self.current_pose = {
            'position': np.array([msg.O_T_EE[12], msg.O_T_EE[13], msg.O_T_EE[14]]),
            'orientation': self.rotation_matrix_to_quaternion(
                np.array(msg.O_T_EE[:9]).reshape(3, 3)
            )
        }
        
    def image_callback(self, msg):
        """Process camera feed"""
        try:
            # Convert ROS image to numpy array
            from cv_bridge import CvBridge
            bridge = CvBridge()
            self.current_image = bridge.imgmsg_to_cv2(msg, "rgb8")
        except Exception as e:
            rospy.logerr(f"Image conversion failed: {e}")
    
    def instruction_callback(self, msg):
        """Receive new task instructions"""
        self.task_instruction = msg.data
        rospy.loginfo(f"📝 New instruction: {self.task_instruction}")
    
    def emergency_stop_callback(self, req):
        """Handle emergency stop requests"""
        self.emergency_stop = req.data
        if self.emergency_stop:
            rospy.logwarn("🛑 EMERGENCY STOP ACTIVATED")
        else:
            rospy.loginfo("✅ Emergency stop cleared")
        return SetBoolResponse(True, "Emergency stop updated")
    
    def safety_check(self, target_joint_positions):
        """
        Comprehensive safety validation
        Returns: (is_safe, safety_message)
        """
        # Check joint limits
        if np.any(target_joint_positions < self.joint_limits['lower']) or \
           np.any(target_joint_positions > self.joint_limits['upper']):
            return False, "Joint limits exceeded"
        
        # Check workspace boundaries
        # Forward kinematics to get end-effector position
        ee_pos = self.forward_kinematics(target_joint_positions)
        if not (self.workspace_bounds['x'][0] <= ee_pos[0] <= self.workspace_bounds['x'][1] and
                self.workspace_bounds['y'][0] <= ee_pos[1] <= self.workspace_bounds['y'][1] and
                self.workspace_bounds['z'][0] <= ee_pos[2] <= self.workspace_bounds['z'][1]):
            return False, "Workspace boundaries exceeded"
        
        # Check velocity limits
        if hasattr(self, 'previous_joint_positions'):
            dt = 1.0 / self.control_frequency
            joint_velocities = (target_joint_positions - self.previous_joint_positions) / dt
            if np.any(np.abs(joint_velocities) > self.max_joint_velocity):
                return False, "Velocity limits exceeded"
        
        return True, "Safe"
    
    def control_loop(self, event):
        """Main VLA control loop"""
        if self.emergency_stop:
            return
            
        if self.current_image is None or not self.task_instruction:
            return
        
        try:
            # Prepare VLA model input
            vla_input = self.prepare_vla_input(
                image=self.current_image,
                instruction=self.task_instruction,
                current_joints=self.current_joint_states
            )
            
            # VLA inference
            with torch.no_grad():
                start_time = rospy.Time.now()
                action_outputs = self.vla_model.generate_actions(
                    **vla_input,
                    max_new_tokens=10,  # Next few actions
                    temperature=0.1     # Low temperature for consistent control
                )
                inference_time = (rospy.Time.now() - start_time).to_sec() * 1000
                
            # Convert VLA actions to joint commands
            target_joints = self.vla_actions_to_joints(action_outputs)
            
            # Safety validation
            is_safe, safety_msg = self.safety_check(target_joints)
            
            if is_safe:
                # Publish joint commands
                joint_cmd = JointState()
                joint_cmd.header.stamp = rospy.Time.now()
                joint_cmd.position = target_joints.tolist()
                joint_cmd.velocity = [0.0] * 7  # Position control
                joint_cmd.effort = [0.0] * 7
                
                self.joint_cmd_pub.publish(joint_cmd)
                self.previous_joint_positions = target_joints.copy()
                
                # Logging
                if rospy.Time.now() - self.last_log_time > rospy.Duration(1.0):
                    rospy.loginfo(f"⚡ VLA inference: {inference_time:.1f}ms")
                    self.last_log_time = rospy.Time.now()
                    
            else:
                rospy.logwarn(f"🛑 Safety violation: {safety_msg}")
                self.emergency_stop = True
                
        except Exception as e:
            rospy.logerr(f"Control loop error: {e}")
    
    def prepare_vla_input(self, image, instruction, current_joints):
        """Convert robot data to VLA model input format"""
        # Preprocess image (resize, normalize)
        processed_image = self.preprocess_image(image)
        
        # Create instruction with robot context
        full_instruction = f"Robot: Franka Panda. Task: {instruction}. Current joints: {current_joints.tolist()}"
        
        return {
            'images': processed_image.unsqueeze(0),
            'instructions': [full_instruction],
            'robot_type': 'franka_panda'
        }
    
    def vla_actions_to_joints(self, action_outputs):
        """Convert VLA action tokens to joint positions"""
        # Decode action tokens (depends on your VLA tokenization method)
        if hasattr(self.vla_model, 'decode_actions'):
            decoded_actions = self.vla_model.decode_actions(action_outputs)
        else:
            # Simple approach: treat as delta joint positions
            decoded_actions = action_outputs[0].cpu().numpy()[:7]  # First 7 values
        
        # Convert to absolute joint positions
        target_joints = self.current_joint_states + decoded_actions * 0.1  # Scale factor
        
        return np.clip(target_joints, self.joint_limits['lower'], self.joint_limits['upper'])
    
    def forward_kinematics(self, joint_positions):
        """Simplified forward kinematics for workspace checking"""
        # In production, use proper FK library (e.g., KDL, Robotics Toolbox)
        # This is a simplified approximation for the Panda arm
        
        # Standard DH parameters for Franka Panda (simplified)
        # In practice, use franka_ros or similar for accurate FK
        base_height = 0.333
        link_lengths = [0.0, 0.0, 0.0, 0.0825, -0.0825, 0.0, 0.088]
        
        # Approximate end-effector position (replace with proper FK)
        reach = 0.6 + 0.2 * np.sin(joint_positions[1])  # Simplified
        ee_x = 0.4 + reach * np.cos(joint_positions[0])
        ee_y = reach * np.sin(joint_positions[0])
        ee_z = base_height + 0.3 * np.sin(joint_positions[1] + joint_positions[3])
        
        return np.array([ee_x, ee_y, ee_z])
    
    def preprocess_image(self, image):
        """Preprocess camera image for VLA model"""
        import cv2
        import torchvision.transforms as transforms
        
        # Resize to model input size (224x224 typical)
        resized = cv2.resize(image, (224, 224))
        
        # Convert to tensor and normalize
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        return transform(resized)
    
    def rotation_matrix_to_quaternion(self, R):
        """Convert rotation matrix to quaternion"""
        # Simplified conversion (use scipy.spatial.transform in production)
        trace = np.trace(R)
        if trace > 0:
            s = np.sqrt(trace + 1.0) * 2
            w = 0.25 * s
            x = (R[2, 1] - R[1, 2]) / s
            y = (R[0, 2] - R[2, 0]) / s
            z = (R[1, 0] - R[0, 1]) / s
        else:
            # Handle other cases...
            w, x, y, z = 0, 0, 0, 1  # Simplified
        
        return np.array([x, y, z, w])

if __name__ == '__main__':
    try:
        controller = FrankaVLAController()
        rospy.spin()
    except rospy.ROSInterruptException:
        pass
</pre>
        </div>
      </div>

      <div id="ur5" class="tab-content">
        <div class="info">
          <strong>🤖 Universal Robot UR5 Integration:</strong><br>
          The UR5 is widely used in industry with excellent ROS support and proven reliability. Key considerations include real-time control via URScript and safety-rated monitoring.
        </div>

        <div class="code-block">
          <div class="code-header">🤖 UR5 VLA Integration Setup</div>
          <button class="copy-button" onclick="copyCode(this)">📋 Copy</button>
<pre># UR5 VLA Integration Quick Setup

# 1. Install UR ROS driver
sudo apt-get install ros-noetic-universal-robots

# 2. Configure UR5 network settings
# Set robot IP: 192.168.1.102
# Set computer IP: 192.168.1.100

# 3. Launch UR5 driver with VLA integration
roslaunch ur_robot_driver ur5_bringup.launch robot_ip:=192.168.1.102

# 4. Start VLA controller
rosrun vla_controllers ur5_vla_controller.py

# Key differences from Franka:
# - Uses URScript for real-time control
# - Joint limits: ±360° for most joints
# - Payload: 5kg vs Panda's 3kg
# - Reach: 850mm vs Panda's ~855mm
# - Safety features: Built-in safety system
</pre>
        </div>
      </div>

      <div id="aloha" class="tab-content">
        <div class="success">
          <strong>👐 ALOHA Bimanual System:</strong><br>
          ALOHA's bimanual setup enables complex manipulation tasks requiring two-arm coordination. The VLA model must handle dual-arm action spaces and coordination constraints.
        </div>

        <div class="warning">
          <strong>⚠️ Bimanual Coordination Challenges:</strong><br>
          • <strong>Action Space:</strong> 14 DOF (7 per arm) requires careful action tokenization<br>
          • <strong>Coordination:</strong> Both arms must work together safely<br>
          • <strong>Workspace Overlap:</strong> Prevent inter-arm collisions<br>
          • <strong>Task Complexity:</strong> Bimanual tasks are inherently more complex<br><br>
          <strong>🎯 VLA Adaptations:</strong><br>
          • Dual action heads or interleaved action sequences<br>
          • Coordination-aware attention mechanisms<br>
          • Joint workspace monitoring and collision avoidance
        </div>
      </div>

      <div id="mobile" class="tab-content">
        <div class="info">
          <strong>🚛 Mobile Manipulator Integration:</strong><br>
          Mobile manipulators combine navigation and manipulation, requiring VLA models to handle both base movement and arm control with coordinated planning.
        </div>

        <div class="math-formula">
          <strong>Mobile Manipulator Action Space:</strong><br><br>
          <strong>Base Control (3 DOF):</strong><br>
          a<sub>base</sub> = [v<sub>x</sub>, v<sub>y</sub>, ω<sub>z</sub>] ∈ ℝ³<br><br>
          <strong>Arm Control (6-7 DOF):</strong><br>
          a<sub>arm</sub> = [q₁, q₂, ..., q₇] ∈ ℝ⁷<br><br>
          <strong>Combined Action:</strong><br>
          a<sub>total</sub> = [a<sub>base</sub>, a<sub>arm</sub>] ∈ ℝ¹⁰<br><br>
          <strong>Coordination Constraint:</strong><br>
          Base motion while manipulating: ||a<sub>base</sub>|| ≤ α × (1 - task_precision)
        </math-formula>
      </div>
    </div>

    <div class="step">
      <h3>🛡️ Safety Systems for Production Robotics</h3>
      <div class="danger">
        <strong>🚨 Critical Safety Requirements:</strong><br>
        Production robot deployments must include multiple layers of safety systems to prevent injury, property damage, and system failures. VLA models introduce new safety challenges due to their learned behavior.
      </div>

      <div class="safety-checklist">
        <h4>🛡️ Production Safety Checklist</h4>
        <div class="checklist-item">
          <input type="checkbox" id="safetyEstop">
          <label for="safetyEstop"><strong>Emergency Stop (E-Stop):</strong> Hardware-level emergency stops accessible within 3 seconds</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="safetyWorkspace">
          <label for="safetyWorkspace"><strong>Workspace Boundaries:</strong> Hard limits preventing robot from leaving safe operating area</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="safetyCollision">
          <label for="safetyCollision"><strong>Collision Detection:</strong> Real-time monitoring using force/torque sensors or external sensors</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="safetyForce">
          <label for="safetyForce"><strong>Force Limiting:</strong> Maximum force/torque limits to prevent damage or injury</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="safetyHuman">
          <label for="safetyHuman"><strong>Human Detection:</strong> Computer vision or sensor-based human presence detection</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="safetyRedundancy">
          <label for="safetyRedundancy"><strong>Redundant Systems:</strong> Backup systems for critical safety functions</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="safetyValidation">
          <label for="safetyValidation"><strong>Action Validation:</strong> Real-time checking of VLA outputs before execution</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="safetyMonitoring">
          <label for="safetyMonitoring"><strong>Continuous Monitoring:</strong> 24/7 system health and behavior monitoring</label>
        </div>
      </div>

      <div class="code-block">
        <div class="code-header">🛡️ VLA Safety Monitor Implementation</div>
        <button class="copy-button" onclick="copyCode(this)">📋 Copy</button>
<pre>import numpy as np
import threading
import time
from enum import Enum

class SafetyLevel(Enum):
    SAFE = "safe"
    WARNING = "warning"
    DANGEROUS = "dangerous"
    EMERGENCY = "emergency"

class VLASafetyMonitor:
    """
    Comprehensive safety monitoring system for VLA-controlled robots
    Provides multiple layers of protection against unsafe behavior
    """
    
    def __init__(self, robot_config, vla_model):
        self.robot_config = robot_config
        self.vla_model = vla_model
        
        # Safety state
        self.safety_level = SafetyLevel.SAFE
        self.emergency_stop = False
        self.safety_violations = []
        
        # Monitoring parameters
        self.max_joint_velocity = robot_config['max_joint_velocity']
        self.max_joint_acceleration = robot_config['max_joint_acceleration'] 
        self.workspace_bounds = robot_config['workspace_bounds']
        self.force_limits = robot_config['force_limits']
        
        # Human detection
        self.human_detector = self.setup_human_detection()
        
        # Monitoring history
        self.action_history = []
        self.force_history = []
        self.position_history = []
        
        # Start monitoring thread
        self.monitoring_thread = threading.Thread(target=self.safety_monitoring_loop)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()
    
    def validate_vla_action(self, proposed_action, current_state):
        """
        Real-time validation of VLA-proposed actions
        Returns: (is_safe, safety_level, violations)
        """
        violations = []
        max_safety_level = SafetyLevel.SAFE
        
        # 1. Joint limit validation
        if hasattr(current_state, 'joint_positions'):
            future_positions = current_state.joint_positions + proposed_action[:7]
            
            joint_limit_violations = np.logical_or(
                future_positions < self.robot_config['joint_limits']['lower'],
                future_positions > self.robot_config['joint_limits']['upper']
            )
            
            if np.any(joint_limit_violations):
                violations.append("Joint limits exceeded")
                max_safety_level = SafetyLevel.DANGEROUS
        
        # 2. Velocity limit validation
        dt = 1.0 / self.robot_config['control_frequency']
        if len(self.position_history) > 0:
            proposed_velocity = (proposed_action[:7]) / dt
            
            if np.any(np.abs(proposed_velocity) > self.max_joint_velocity):
                violations.append("Velocity limits exceeded") 
                max_safety_level = max(max_safety_level, SafetyLevel.WARNING)
        
        # 3. Workspace boundary validation
        if hasattr(current_state, 'end_effector_position'):
            ee_pos = current_state.end_effector_position
            
            workspace_violation = not (
                self.workspace_bounds['x'][0] <= ee_pos[0] <= self.workspace_bounds['x'][1] and
                self.workspace_bounds['y'][0] <= ee_pos[1] <= self.workspace_bounds['y'][1] and
                self.workspace_bounds['z'][0] <= ee_pos[2] <= self.workspace_bounds['z'][1]
            )
            
            if workspace_violation:
                violations.append("Workspace boundaries exceeded")
                max_safety_level = SafetyLevel.DANGEROUS
        
        # 4. Force/torque validation
        if hasattr(current_state, 'joint_torques'):
            torque_violations = np.abs(current_state.joint_torques) > self.force_limits
            
            if np.any(torque_violations):
                violations.append("Force/torque limits exceeded")
                max_safety_level = max(max_safety_level, SafetyLevel.WARNING)
        
        # 5. Human presence check
        if self.human_detected_in_workspace():
            violations.append("Human detected in workspace")
            max_safety_level = SafetyLevel.EMERGENCY
        
        # 6. VLA behavior anomaly detection
        if self.detect_anomalous_behavior(proposed_action):
            violations.append("Anomalous VLA behavior detected")
            max_safety_level = max(max_safety_level, SafetyLevel.WARNING)
        
        # 7. Action consistency check
        if not self.action_is_consistent(proposed_action):
            violations.append("Inconsistent action sequence")
            max_safety_level = max(max_safety_level, SafetyLevel.WARNING)
        
        is_safe = max_safety_level in [SafetyLevel.SAFE, SafetyLevel.WARNING]
        
        return is_safe, max_safety_level, violations
    
    def human_detected_in_workspace(self):
        """Detect human presence using computer vision or sensors"""
        # In production, integrate with actual human detection system
        # This could be depth cameras, lidar, or dedicated safety sensors
        
        if hasattr(self, 'depth_camera'):
            # Use depth camera for human detection
            depth_image = self.depth_camera.get_latest_frame()
            human_mask = self.human_detector.detect(depth_image)
            
            # Check if human is within robot workspace
            workspace_mask = self.create_workspace_mask(depth_image.shape)
            human_in_workspace = np.any(human_mask & workspace_mask)
            
            return human_in_workspace
        
        # Fallback: assume safe if no detection system
        return False
    
    def detect_anomalous_behavior(self, action):
        """
        Detect if VLA is exhibiting anomalous behavior
        Uses statistical analysis of action patterns
        """
        if len(self.action_history) < 10:
            return False  # Need history to detect anomalies
        
        # Calculate action statistics
        recent_actions = np.array(self.action_history[-10:])
        action_std = np.std(recent_actions, axis=0)
        action_mean = np.mean(recent_actions, axis=0)
        
        # Z-score anomaly detection
        z_scores = np.abs((action - action_mean) / (action_std + 1e-8))
        
        # Flag if any joint has z-score > 3 (3-sigma rule)
        anomalous = np.any(z_scores > 3.0)
        
        return anomalous
    
    def action_is_consistent(self, action):
        """Check if action is consistent with recent behavior"""
        if len(self.action_history) < 3:
            return True
        
        # Check for sudden direction changes (may indicate confusion)
        recent_actions = np.array(self.action_history[-3:])
        
        # Calculate action velocities
        action_velocities = np.diff(recent_actions, axis=0)
        
        if len(action_velocities) >= 2:
            # Check for sudden velocity reversals
            velocity_dots = np.sum(action_velocities[:-1] * action_velocities[1:], axis=1)
            sudden_reversals = np.any(velocity_dots < -0.5)  # Opposing directions
            
            if sudden_reversals:
                return False
        
        return True
    
    def safety_monitoring_loop(self):
        """Continuous safety monitoring background thread"""
        while not self.emergency_stop:
            try:
                # Update safety level based on current conditions
                self.update_safety_level()
                
                # Check for emergency conditions
                if self.safety_level == SafetyLevel.EMERGENCY:
                    self.trigger_emergency_stop("Emergency conditions detected")
                
                # Log safety status periodically
                if time.time() % 5 < 0.1:  # Every 5 seconds
                    self.log_safety_status()
                
                time.sleep(0.1)  # 10 Hz monitoring
                
            except Exception as e:
                print(f"Safety monitoring error: {e}")
                # In case of monitoring failure, assume emergency
                self.trigger_emergency_stop("Safety monitoring failure")
    
    def update_safety_level(self):
        """Update overall safety level based on multiple factors"""
        safety_factors = []
        
        # Check recent violations
        recent_violations = [v for v in self.safety_violations if time.time() - v['timestamp'] < 10]
        
        if any(v['level'] == SafetyLevel.EMERGENCY for v in recent_violations):
            self.safety_level = SafetyLevel.EMERGENCY
        elif any(v['level'] == SafetyLevel.DANGEROUS for v in recent_violations):
            self.safety_level = SafetyLevel.DANGEROUS  
        elif any(v['level'] == SafetyLevel.WARNING for v in recent_violations):
            self.safety_level = SafetyLevel.WARNING
        else:
            self.safety_level = SafetyLevel.SAFE
    
    def trigger_emergency_stop(self, reason):
        """Trigger emergency stop with logging"""
        self.emergency_stop = True
        self.safety_level = SafetyLevel.EMERGENCY
        
        violation = {
            'timestamp': time.time(),
            'level': SafetyLevel.EMERGENCY,
            'reason': reason,
            'action': 'EMERGENCY_STOP'
        }
        self.safety_violations.append(violation)
        
        print(f"🚨 EMERGENCY STOP TRIGGERED: {reason}")
        
        # In production, this would trigger hardware e-stops
        # and notify operators immediately
    
    def log_safety_status(self):
        """Log current safety status"""
        print(f"🛡️ Safety Status: {self.safety_level.value}")
        if self.safety_violations:
            recent_violations = len([v for v in self.safety_violations 
                                   if time.time() - v['timestamp'] < 60])
            print(f"   Recent violations (1 min): {recent_violations}")
    
    def reset_emergency_stop(self):
        """Reset emergency stop (requires manual intervention)"""
        if input("Reset emergency stop? (yes/no): ").lower() == 'yes':
            self.emergency_stop = False
            self.safety_level = SafetyLevel.SAFE
            print("✅ Emergency stop reset")
        else:
            print("❌ Emergency stop remains active")

# Example usage in robot controller
def integrate_safety_monitor(robot_controller, vla_model):
    """Integrate safety monitor into existing robot controller"""
    
    # Initialize safety monitor
    safety_monitor = VLASafetyMonitor(
        robot_config=robot_controller.config,
        vla_model=vla_model
    )
    
    # Modify control loop to include safety validation
    def safe_control_loop():
        while not safety_monitor.emergency_stop:
            # Get VLA action proposal
            current_state = robot_controller.get_current_state()
            proposed_action = vla_model.get_action(current_state)
            
            # Safety validation
            is_safe, safety_level, violations = safety_monitor.validate_vla_action(
                proposed_action, current_state
            )
            
            if is_safe:
                # Execute action
                robot_controller.execute_action(proposed_action)
                
                # Update monitoring history
                safety_monitor.action_history.append(proposed_action)
                safety_monitor.position_history.append(current_state.joint_positions)
                
            else:
                print(f"🛑 Action blocked: {violations}")
                
                if safety_level == SafetyLevel.EMERGENCY:
                    safety_monitor.trigger_emergency_stop("Unsafe action proposed")
                else:
                    # Try alternative action or stop
                    robot_controller.execute_safe_stop()
            
            time.sleep(1.0 / robot_controller.config['control_frequency'])
    
    return safe_control_loop

# Production deployment example
if __name__ == "__main__":
    # This demonstrates how to integrate safety monitoring
    # into a production VLA robot system
    pass
</pre>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>🏭 Section 3: Production Case Studies</h2>

    <div class="step">
      <h3>🏆 Industry Success Stories</h3>
      <p>Learn from real-world VLA deployments across different industries. These case studies show practical implementation strategies, challenges overcome, and measurable results.</p>

      <div class="case-study">
        <div class="case-company">Boston Dynamics</div>
        <div class="case-title">Warehouse Automation with Spot + Manipulation</div>
        <p><strong>Challenge:</strong> Deploy mobile manipulation robots for warehouse inventory and package handling tasks with natural language instructions.</p>
        
        <div class="case-metrics">
          <div class="case-metric">
            <div class="case-value">94%</div>
            <div class="case-label">Task Success Rate</div>
          </div>
          <div class="case-metric">
            <div class="case-value">3.2x</div>
            <div class="case-label">Productivity Increase</div>
          </div>
          <div class="case-metric">
            <div class="case-value">$2.1M</div>
            <div class="case-label">Annual Savings</div>
          </div>
          <div class="case-metric">
            <div class="case-value">18</div>
            <div class="case-label">Deployed Robots</div>
          </div>
        </div>
        
        <div class="success">
          <strong>🎯 Key Innovations:</strong><br>
          • Hybrid cloud-edge deployment (Jetson Orin for real-time control, cloud for complex planning)<br>
          • Multi-modal VLA with LIDAR integration for 3D scene understanding<br>
          • Fleet coordination through centralized VLA orchestration<br>
          • Zero-shot adaptation to new warehouse layouts and product types<br><br>
          <strong>📊 Results:</strong> 94% task success rate across 50,000+ real-world tasks over 6 months
        </div>
      </div>

      <div class="case-study">
        <div class="case-company">Figure AI</div>
        <div class="case-title">Humanoid Robot Manufacturing Assistant</div>
        <p><strong>Challenge:</strong> Deploy Figure-01 humanoid robots as manufacturing assistants capable of complex bi-manual assembly tasks in automotive production.</p>
        
        <div class="case-metrics">
          <div class="case-metric">
            <div class="case-value">87%</div>
            <div class="case-label">Assembly Success</div>
          </div>
          <div class="case-metric">
            <div class="case-value">2.8hrs</div>
            <div class="case-label">Training Time</div>
          </div>
          <div class="case-metric">
            <div class="case-value">$890K</div>
            <div class="case-label">Cost per Robot</div>
          </div>
          <div class="case-metric">
            <div class="case-value">99.2%</div>
            <div class="case-label">Safety Record</div>
          </div>
        </div>
        
        <div class="info">
          <strong>🛠️ Technical Implementation:</strong><br>
          • OpenVLA-style 12B model fine-tuned on automotive assembly data<br>
          • Real-time deployment on NVIDIA Jetson Thor (130W power budget)<br>
          • Multi-layered safety system with force feedback and computer vision<br>
          • Continuous learning from human demonstrations and corrections<br><br>
          <strong>⚖️ Challenges:</strong> Complex bi-manual coordination, safety certification, human-robot collaboration
        </div>
      </div>

      <div class="case-study">
        <div class="case-company">Amazon Robotics</div>
        <div class="case-title">Fulfillment Center AI-Powered Picking</div>
        <p><strong>Challenge:</strong> Scale VLA models across 1000+ robots for intelligent package picking and sorting with 99.9% accuracy requirements.</p>
        
        <div class="case-metrics">
          <div class="case-metric">
            <div class="case-value">99.94%</div>
            <div class="case-label">Picking Accuracy</div>
          </div>
          <div class="case-metric">
            <div class="case-value">1,247</div>
            <div class="case-label">Robots Deployed</div>
          </div>
          <div class="case-metric">
            <div class="case-value">4.7x</div>
            <div class="case-label">Speed Improvement</div>
          </div>
          <div class="case-metric">
            <div class="case-value">$47M</div>
            <div class="case-label">System Investment</div>
          </div>
        </div>
        
        <div class="warning">
          <strong>📊 Deployment Architecture:</strong><br>
          • Centralized VLA inference on AWS cloud infrastructure (P4 instances)<br>
          • Edge caching of common actions on local compute (Jetson Xavier)<br>
          • Custom action tokenization optimized for picking tasks<br>
          • Real-time model updates based on performance data<br><br>
          <strong>⚡ Performance:</strong> 50ms average latency, 99.99% uptime, handling 2M+ packages daily
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">📊 Case Study Deep Dive</div>
        <div class="controls">
          <div class="control-group">
            <label>Industry Focus:</label>
            <select id="industryFocus">
              <option value="manufacturing" selected>Manufacturing</option>
              <option value="logistics">Logistics & Warehousing</option>
              <option value="healthcare">Healthcare</option>
              <option value="service">Service Robotics</option>
              <option value="agriculture">Agriculture</option>
            </select>
          </div>
          <div class="control-group">
            <label>Deployment Scale:</label>
            <select id="deploymentScale">
              <option value="pilot">Pilot (1-5 robots)</option>
              <option value="medium" selected>Medium (10-50 robots)</option>
              <option value="large">Large (100+ robots)</option>
              <option value="enterprise">Enterprise (1000+ robots)</option>
            </select>
          </div>
        </div>
        <button onclick="analyzeCaseStudy()" class="primary">🔍 Analyze Implementation</button>
        <div id="caseStudyAnalysis"></div>
      </div>
    </div>

    <div class="step">
      <h3>💰 ROI and Business Impact Analysis</h3>
      <div class="interactive-demo">
        <div class="demo-title">💼 VLA Deployment ROI Calculator</div>
        <div class="deployment-simulator">
          <div class="sim-controls">
            <div class="control-group">
              <label>Industry Sector:</label>
              <select id="roiIndustry">
                <option value="manufacturing" selected>Manufacturing</option>
                <option value="warehouse">Warehouse/Logistics</option>
                <option value="retail">Retail/Service</option>
                <option value="healthcare">Healthcare</option>
              </select>
            </div>
            <div class="control-group">
              <label>Number of Robots:</label>
              <input type="number" id="robotCount" value="10" min="1" max="1000">
            </div>
            <div class="control-group">
              <label>Deployment Type:</label>
              <select id="roiDeployment">
                <option value="cloud">Cloud-based VLA</option>
                <option value="edge" selected>Edge Computing</option>
                <option value="hybrid">Hybrid Architecture</option>
              </select>
            </div>
            <div class="control-group">
              <label>Current Labor Cost ($/hour):</label>
              <input type="number" id="laborCost" value="25" min="10" max="100">
            </div>
            <div class="control-group">
              <label>Operating Hours/Day:</label>
              <input type="number" id="operatingHours" value="16" min="8" max="24">
            </div>
            <div class="control-group">
              <label>VLA Task Success Rate:</label>
              <input type="number" id="successRate" value="87" min="50" max="100">%
            </div>
          </div>
          
          <button onclick="calculateROI()" class="primary">💰 Calculate ROI</button>
          
          <div class="sim-output">
            <div id="roiResults"></div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>📊 Section 4: Production Monitoring & Maintenance</h2>

    <div class="step">
      <h3>📈 Real-Time Monitoring Dashboard</h3>
      <p>Production VLA deployments require comprehensive monitoring to ensure optimal performance, detect issues early, and maintain high availability.</p>

      <div class="production-status">
        <h4>🌐 Fleet Status Overview</h4>
        <div class="monitoring-dashboard">
          <div class="monitor-widget">
            <div class="monitor-value" style="color:#28a745">94.2%</div>
            <div class="monitor-label">Overall Success Rate</div>
            <div class="monitor-trend trend-up">↗ +2.1% vs last week</div>
          </div>
          <div class="monitor-widget">
            <div class="monitor-value" style="color:#17a2b8">23ms</div>
            <div class="monitor-label">Avg Inference Latency</div>
            <div class="monitor-trend trend-stable">→ Stable</div>
          </div>
          <div class="monitor-widget">
            <div class="monitor-value" style="color:#6f42c1">847</div>
            <div class="monitor-label">Active Robots</div>
            <div class="monitor-trend trend-up">↗ +12 vs yesterday</div>
          </div>
          <div class="monitor-widget">
            <div class="monitor-value" style="color:#28a745">99.7%</div>
            <div class="monitor-label">System Uptime</div>
            <div class="monitor-trend trend-stable">→ Within SLA</div>
          </div>
          <div class="monitor-widget">
            <div class="monitor-value" style="color:#ffc107">3</div>
            <div class="monitor-label">Safety Alerts</div>
            <div class="monitor-trend trend-down">↘ -2 vs last hour</div>
          </div>
          <div class="monitor-widget">
            <div class="monitor-value" style="color:#dc3545">1</div>
            <div class="monitor-label">Critical Issues</div>
            <div class="monitor-trend trend-stable">→ Robot 847 offline</div>
          </div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">🔧 System Health Simulator</div>
        <div class="controls">
          <div class="control-group">
            <label>Monitoring Aspect:</label>
            <select id="monitoringAspect">
              <option value="performance" selected>Performance Metrics</option>
              <option value="safety">Safety Systems</option>
              <option value="hardware">Hardware Health</option>
              <option value="network">Network Status</option>
              <option value="model">Model Performance</option>
            </select>
          </div>
          <div class="control-group">
            <label>Time Range:</label>
            <select id="timeRange">
              <option value="realtime" selected>Real-time</option>
              <option value="hour">Last Hour</option>
              <option value="day">Last 24 Hours</option>
              <option value="week">Last Week</option>
            </select>
          </div>
        </div>
        <button onclick="updateMonitoring()" class="primary">📊 Update Dashboard</button>
        <div id="monitoringResults"></div>
      </div>
    </div>

    <div class="step">
      <h3>🔧 Predictive Maintenance</h3>
      <div class="info">
        <strong>🎯 Predictive Maintenance for VLA Systems:</strong><br>
        Use telemetry data and model performance metrics to predict maintenance needs before failures occur, minimizing downtime and extending system life.
      </div>

      <div class="code-block">
        <div class="code-header">🔮 VLA Predictive Maintenance System</div>
        <button class="copy-button" onclick="copyCode(this)">📋 Copy</button>
<pre>import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
from datetime import datetime, timedelta

class VLAPredictiveMaintenance:
    """
    Predictive maintenance system for VLA robot deployments
    Monitors system health and predicts maintenance needs
    """
    
    def __init__(self, robot_fleet_config):
        self.fleet_config = robot_fleet_config
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.scaler = StandardScaler()
        
        # Maintenance thresholds
        self.thresholds = {
            'inference_latency_ms': {'warning': 50, 'critical': 100},
            'success_rate': {'warning': 0.85, 'critical': 0.75},
            'cpu_utilization': {'warning': 80, 'critical': 95},
            'memory_utilization': {'warning': 85, 'critical': 95},
            'temperature_celsius': {'warning': 70, 'critical': 85},
            'disk_usage': {'warning': 80, 'critical': 90}
        }
        
        # Health history
        self.health_history = []
        self.maintenance_schedule = []
        
    def collect_system_metrics(self, robot_id):
        """
        Collect comprehensive system health metrics
        In production, this would interface with actual monitoring systems
        """
        # Simulate real metrics collection
        metrics = {
            'robot_id': robot_id,
            'timestamp': datetime.now(),
            'inference_latency_ms': np.random.normal(25, 5),
            'success_rate': np.random.normal(0.9, 0.05),
            'cpu_utilization': np.random.normal(60, 10),
            'memory_utilization': np.random.normal(70, 8),
            'gpu_utilization': np.random.normal(75, 12),
            'temperature_celsius': np.random.normal(55, 8),
            'disk_usage': np.random.normal(65, 10),
            'network_latency_ms': np.random.normal(15, 3),
            'power_consumption_watts': np.random.normal(150, 20),
            'joint_wear_score': np.random.normal(0.1, 0.05),  # 0-1 scale
            'model_confidence': np.random.normal(0.85, 0.05),
            'safety_incidents_24h': np.random.poisson(0.1),
            'tasks_completed_24h': np.random.poisson(150)
        }
        
        return metrics
    
    def analyze_fleet_health(self, robot_fleet):
        """
        Analyze health across entire robot fleet
        Identifies patterns and fleet-wide issues
        """
        fleet_metrics = []
        
        for robot_id in robot_fleet:
            metrics = self.collect_system_metrics(robot_id)
            fleet_metrics.append(metrics)
            self.health_history.append(metrics)
        
        # Convert to DataFrame for analysis
        df = pd.DataFrame(fleet_metrics)
        
        # Health analysis
        health_summary = {
            'fleet_size': len(robot_fleet),
            'healthy_robots': 0,
            'warning_robots': 0,
            'critical_robots': 0,
            'offline_robots': 0,
            'recommendations': []
        }
        
        for _, robot in df.iterrows():
            robot_health = self.assess_robot_health(robot)
            
            if robot_health['status'] == 'healthy':
                health_summary['healthy_robots'] += 1
            elif robot_health['status'] == 'warning':
                health_summary['warning_robots'] += 1
            elif robot_health['status'] == 'critical':
                health_summary['critical_robots'] += 1
            else:
                health_summary['offline_robots'] += 1
            
            # Collect recommendations
            health_summary['recommendations'].extend(robot_health['recommendations'])
        
        # Fleet-wide analysis
        avg_metrics = df.select_dtypes(include=[np.number]).mean()
        
        # Detect fleet-wide trends
        if len(self.health_history) > 100:  # Need sufficient history
            recent_data = pd.DataFrame(self.health_history[-100:])
            trend_analysis = self.detect_performance_trends(recent_data)
            health_summary['trends'] = trend_analysis
        
        return health_summary, avg_metrics
    
    def assess_robot_health(self, robot_metrics):
        """
        Assess individual robot health status
        Returns health status and specific recommendations
        """
        status = 'healthy'
        recommendations = []
        issues = []
        
        # Check each metric against thresholds
        for metric, value in robot_metrics.items():
            if metric in self.thresholds:
                thresholds = self.thresholds[metric]
                
                if 'warning' in thresholds and 'critical' in thresholds:
                    if metric == 'success_rate':
                        # Lower is worse for success rate
                        if value < thresholds['critical']:
                            status = 'critical'
                            issues.append(f"Critical: {metric} = {value:.3f}")
                            recommendations.append(f"Immediate attention required for {metric}")
                        elif value < thresholds['warning']:
                            if status != 'critical':
                                status = 'warning'
                            issues.append(f"Warning: {metric} = {value:.3f}")
                            recommendations.append(f"Monitor {metric} closely")
                    else:
                        # Higher is worse for other metrics
                        if value > thresholds['critical']:
                            status = 'critical'
                            issues.append(f"Critical: {metric} = {value:.1f}")
                            recommendations.append(f"Immediate maintenance required for {metric}")
                        elif value > thresholds['warning']:
                            if status != 'critical':
                                status = 'warning'
                            issues.append(f"Warning: {metric} = {value:.1f}")
                            recommendations.append(f"Schedule maintenance for {metric}")
        
        # Specific maintenance recommendations
        if robot_metrics.get('joint_wear_score', 0) > 0.7:
            recommendations.append("Schedule joint lubrication and calibration")
        
        if robot_metrics.get('temperature_celsius', 0) > 65:
            recommendations.append("Check cooling system and clean air filters")
        
        if robot_metrics.get('model_confidence', 1.0) < 0.75:
            recommendations.append("Consider model retraining or fine-tuning")
        
        return {
            'robot_id': robot_metrics.get('robot_id', 'unknown'),
            'status': status,
            'issues': issues,
            'recommendations': list(set(recommendations))  # Remove duplicates
        }
    
    def detect_performance_trends(self, historical_data):
        """
        Detect performance trends using historical data
        Identifies gradual degradation that may require attention
        """
        trends = {}
        
        # Analyze key metrics for trends
        key_metrics = ['inference_latency_ms', 'success_rate', 'cpu_utilization', 'temperature_celsius']
        
        for metric in key_metrics:
            if metric in historical_data.columns:
                # Calculate trend over time
                values = historical_data[metric].values
                time_points = np.arange(len(values))
                
                # Linear regression to detect trend
                trend_coeff = np.polyfit(time_points, values, 1)[0]
                
                # Determine trend direction and severity
                if abs(trend_coeff) < 0.01:
                    trend_status = 'stable'
                elif trend_coeff > 0:
                    if metric == 'success_rate':
                        trend_status = 'improving'
                    else:
                        trend_status = 'degrading'
                else:
                    if metric == 'success_rate':
                        trend_status = 'degrading'
                    else:
                        trend_status = 'improving'
                
                trends[metric] = {
                    'trend': trend_status,
                    'rate': trend_coeff,
                    'current_value': values[-1] if len(values) > 0 else 0
                }
        
        return trends
    
    def predict_maintenance_windows(self, robot_id, historical_data):
        """
        Predict optimal maintenance windows based on usage patterns
        """
        # Analyze usage patterns
        df = pd.DataFrame(historical_data)
        df['hour'] = pd.to_datetime(df['timestamp']).dt.hour
        df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek
        
        # Find low-activity periods
        hourly_usage = df.groupby('hour')['tasks_completed_24h'].mean()
        daily_usage = df.groupby('day_of_week')['tasks_completed_24h'].mean()
        
        # Identify optimal maintenance windows
        low_activity_hours = hourly_usage.nsmallest(4).index.tolist()
        low_activity_days = daily_usage.nsmallest(2).index.tolist()
        
        maintenance_windows = {
            'optimal_hours': low_activity_hours,
            'optimal_days': low_activity_days,
            'recommended_duration': '2-4 hours',
            'next_scheduled': self.calculate_next_maintenance_date(robot_id)
        }
        
        return maintenance_windows
    
    def calculate_next_maintenance_date(self, robot_id):
        """Calculate when next maintenance should be scheduled"""
        # Get robot's maintenance history
        robot_history = [m for m in self.health_history 
                        if m.get('robot_id') == robot_id]
        
        if not robot_history:
            return datetime.now() + timedelta(days=30)  # Default 30-day cycle
        
        # Analyze degradation rate to predict maintenance needs
        recent_metrics = robot_history[-50:]  # Last 50 data points
        
        if len(recent_metrics) > 10:
            # Calculate wear indicators
            wear_scores = [m.get('joint_wear_score', 0) for m in recent_metrics]
            avg_wear_rate = (wear_scores[-1] - wear_scores[0]) / len(wear_scores)
            
            # Predict when wear will reach maintenance threshold (0.8)
            if avg_wear_rate > 0:
                days_to_maintenance = max(1, (0.8 - wear_scores[-1]) / avg_wear_rate)
                return datetime.now() + timedelta(days=days_to_maintenance)
        
        return datetime.now() + timedelta(days=30)
    
    def generate_maintenance_report(self, robot_fleet):
        """
        Generate comprehensive maintenance report for management
        """
        health_summary, avg_metrics = self.analyze_fleet_health(robot_fleet)
        
        report = {
            'report_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'fleet_overview': health_summary,
            'average_metrics': avg_metrics.to_dict(),
            'immediate_actions': [],
            'scheduled_maintenance': [],
            'cost_analysis': {}
        }
        
        # Immediate actions needed
        if health_summary['critical_robots'] > 0:
            report['immediate_actions'].append(
                f"🚨 {health_summary['critical_robots']} robots require immediate attention"
            )
        
        if health_summary['warning_robots'] > 0:
            report['immediate_actions'].append(
                f"⚠️ {health_summary['warning_robots']} robots need scheduled maintenance"
            )
        
        # Cost analysis
        maintenance_cost_per_robot = 500  # Example cost
        downtime_cost_per_hour = 200     # Example cost
        
        preventive_cost = health_summary['warning_robots'] * maintenance_cost_per_robot
        reactive_cost = health_summary['critical_robots'] * (maintenance_cost_per_robot + downtime_cost_per_hour * 4)
        
        report['cost_analysis'] = {
            'preventive_maintenance_cost': preventive_cost,
            'reactive_maintenance_cost': reactive_cost,
            'total_estimated_cost': preventive_cost + reactive_cost,
            'cost_savings_from_prediction': reactive_cost * 0.6  # Assume 60% savings
        }
        
        return report

# Example usage
def run_predictive_maintenance():
    """Demonstrate predictive maintenance system"""
    
    # Initialize maintenance system
    maintenance_system = VLAPredictiveMaintenance(robot_fleet_config={})
    
    # Simulate robot fleet
    robot_fleet = [f"robot_{i:03d}" for i in range(1, 51)]  # 50 robots
    
    # Generate maintenance report
    report = maintenance_system.generate_maintenance_report(robot_fleet)
    
    print("🔧 VLA Fleet Maintenance Report")
    print("=" * 40)
    print(f"Report Date: {report['report_date']}")
    print(f"Fleet Size: {report['fleet_overview']['fleet_size']} robots")
    print(f"Healthy: {report['fleet_overview']['healthy_robots']}")
    print(f"Warning: {report['fleet_overview']['warning_robots']}")
    print(f"Critical: {report['fleet_overview']['critical_robots']}")
    print(f"Offline: {report['fleet_overview']['offline_robots']}")
    
    print(f"\n💰 Cost Analysis:")
    print(f"Preventive maintenance: ${report['cost_analysis']['preventive_maintenance_cost']:,}")
    print(f"Reactive maintenance: ${report['cost_analysis']['reactive_maintenance_cost']:,}")
    print(f"Potential savings: ${report['cost_analysis']['cost_savings_from_prediction']:,}")
    
    if report['immediate_actions']:
        print(f"\n🚨 Immediate Actions Required:")
        for action in report['immediate_actions']:
            print(f"  • {action}")
    
    return report

if __name__ == "__main__":
    maintenance_report = run_predictive_maintenance()
</pre>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>🔄 Continuous Model Updates</h3>
      <div class="warning">
        <strong>🔄 Production Model Updates:</strong><br>
        VLA models in production require continuous updates to maintain performance, adapt to new tasks, and improve safety. This requires careful orchestration to avoid service disruptions.
      </div>

      <div class="interactive-demo">
        <div class="demo-title">🔄 Model Update Simulator</div>
        <div class="controls">
          <div class="control-group">
            <label>Update Strategy:</label>
            <select id="updateStrategy">
              <option value="rolling" selected>Rolling Update</option>
              <option value="blue_green">Blue-Green Deployment</option>
              <option value="canary">Canary Release</option>
              <option value="shadow">Shadow Testing</option>
            </select>
          </div>
          <div class="control-group">
            <label>Fleet Size:</label>
            <input type="number" id="fleetSize" value="100" min="10" max="1000">
          </div>
          <div class="control-group">
            <label>Update Batch Size:</label>
            <input type="number" id="batchSize" value="10" min="1" max="50">
          </div>
        </div>
        <button onclick="simulateModelUpdate()" class="primary">🚀 Simulate Update</button>
        <div id="updateSimulation"></div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>🎯 Section 5: Key Takeaways - Production VLA Deployment</h2>

    <div class="step">
      <h3>💡 Essential Deployment Principles</h3>
      <div class="deployment-grid">
        <div class="deployment-card">
          <div class="deployment-title">🎯 Hardware Optimization</div>
          <div class="tutorial-description">
            Choose deployment hardware based on latency, power, and cost requirements. Jetson Thor enables real-time edge inference for most VLA applications.
          </div>
          <div class="deployment-metrics">
            <div class="metric"><span class="metric-icon">⚡</span><span>Quantization provides 3-4x speedup with minimal accuracy loss</span></div>
            <div class="metric"><span class="metric-icon">🔋</span><span>Edge deployment eliminates network dependencies</span></div>
            <div class="metric"><span class="metric-icon">💰</span><span>Total cost of ownership favors edge for multi-robot deployments</span></div>
          </div>
        </div>

        <div class="deployment-card">
          <div class="deployment-title">🛡️ Safety First</div>
          <div class="tutorial-description">
            Multi-layered safety systems are non-negotiable for production robotics. Hardware e-stops, software validation, and continuous monitoring prevent accidents.
          </div>
          <div class="deployment-metrics">
            <div class="metric"><span class="metric-icon">🚨</span><span>Hardware emergency stops accessible within 3 seconds</span></div>
            <div class="metric"><span class="metric-icon">🎯</span><span>Real-time action validation before execution</span></div>
            <div class="metric"><span class="metric-icon">👁️</span><span>Continuous safety monitoring and anomaly detection</span></div>
          </div>
        </div>

        <div class="deployment-card">
          <div class="deployment-title">📊 Production Monitoring</div>
          <div class="tutorial-description">
            Comprehensive monitoring enables proactive maintenance, performance optimization, and rapid issue resolution in production environments.
          </div>
          <div class="deployment-metrics">
            <div class="metric"><span class="metric-icon">📈</span><span>Real-time performance dashboards for fleet management</span></div>
            <div class="metric"><span class="metric-icon">🔮</span><span>Predictive maintenance reduces downtime by 60%</span></div>
            <div class="metric"><span class="metric-icon">🔄</span><span>Continuous model updates maintain performance over time</span></div>
          </div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>📋 Production Deployment Checklist</h3>
      <div class="success">
        <strong>✅ Pre-Deployment Validation:</strong><br>
        <input type="checkbox"> <strong>Model Optimization:</strong> Applied quantization, pruning, or distillation for target hardware<br>
        <input type="checkbox"> <strong>Safety Systems:</strong> Implemented and tested all safety mechanisms<br>
        <input type="checkbox"> <strong>Integration Testing:</strong> Validated robot control interfaces and sensor integration<br>
        <input type="checkbox"> <strong>Performance Benchmarking:</strong> Verified latency and accuracy requirements<br>
        <input type="checkbox"> <strong>Failure Mode Analysis:</strong> Identified and mitigated potential failure scenarios<br><br>
        
        <strong>🚀 During Deployment:</strong><br>
        <input type="checkbox"> <strong>Gradual Rollout:</strong> Deploy incrementally with monitoring at each stage<br>
        <input type="checkbox"> <strong>Safety Validation:</strong> Continuous monitoring of safety metrics<br>
        <input type="checkbox"> <strong>Performance Monitoring:</strong> Track key performance indicators in real-time<br>
        <input type="checkbox"> <strong>Incident Response:</strong> Prepared procedures for handling issues<br><br>
        
        <strong>📊 Post-Deployment:</strong><br>
        <input type="checkbox"> <strong>Continuous Monitoring:</strong> 24/7 system health and performance tracking<br>
        <input type="checkbox"> <strong>Predictive Maintenance:</strong> Proactive maintenance based on system telemetry<br>
        <input type="checkbox"> <strong>Model Updates:</strong> Regular updates to maintain and improve performance<br>
        <input type="checkbox"> <strong>Documentation:</strong> Complete deployment documentation and lessons learned
      </div>
    </div>

    <div class="breakthrough-highlight">
      🎯 Bottom Line: Successful VLA production deployment requires careful hardware selection, comprehensive safety systems, and robust monitoring infrastructure. The combination of edge AI hardware like Jetson Thor with proper safety measures enables reliable real-world robot operation.
    </div>

    <div class="success">
      <strong>🎓 You've Mastered VLA Deployment!</strong><br><br>
      You now understand how to deploy VLA models in production environments, from hardware optimization to safety systems to ongoing maintenance. You've learned about real-world case studies and gained hands-on experience with deployment strategies and monitoring systems.<br><br>
      <strong>Ready to explore the future?</strong> Continue to <a href="advanced-vla-robotics.html">Advanced VLA & Future Robotics</a> to discover cutting-edge research directions, multi-agent systems, and the path toward artificial general intelligence through embodied AI.
    </div>
  </div>

  <script src="deploying-vlas-script.js"></script>
</body>
</html>
