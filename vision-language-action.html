<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Vision-Language-Action Fundamentals: The Robotics Revolution</title>
  <style>
    body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:1200px;margin:0 auto;padding:20px;background:linear-gradient(135deg,#1a1a1a 0%,#2d2d2d 100%);color:#e0e0e0;line-height:1.6}
    .container{background:#fff;color:#2d2d2d;border-radius:20px;padding:30px;margin:20px 0;border:1px solid #e0e0e0;box-shadow:0 4px 20px rgba(0,0,0,.1)}
    .nav-bar{background:#2d2d2d;color:#fff;padding:15px 30px;border-radius:15px;margin:20px 0;display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:15px}
    .nav-home,.nav-prev,.nav-next{background:#fff;color:#2d2d2d;padding:8px 16px;border-radius:6px;text-decoration:none;font-weight:bold;transition:all .3s}
    .nav-next{background:#28a745;color:#fff}
    .nav-home:hover,.nav-prev:hover,.nav-next:hover{transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    .nav-title{font-size:1.2em;font-weight:bold;flex:1;min-width:300px}
    .step{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;margin:15px 0;border-radius:15px;border-left:4px solid #2d2d2d}
    .interactive-demo{background:#f8f9fa;border:2px solid #e9ecef;border-radius:15px;padding:25px;margin:20px 0}
    .demo-title{font-size:1.3em;font-weight:bold;margin-bottom:20px;text-align:center;color:#2d2d2d}
    .controls{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .control-group{display:flex;flex-direction:column;gap:5px}
    .control-group label{font-weight:bold;font-size:14px;color:#2d2d2d}
    button{background:#2d2d2d;border:none;color:#fff;padding:12px 24px;border-radius:8px;cursor:pointer;font-weight:bold;transition:all .3s;margin:5px}
    button:hover{background:#1a1a1a;transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    button.primary{background:#28a745}
    button.primary:hover{background:#218838}
    input,select,textarea{padding:8px 12px;border:1px solid #dadce0;border-radius:6px;background:#fff;color:#2d2d2d}
    textarea{min-height:70px;resize:vertical}
    .math-formula{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;text-align:center;font-size:16px;box-shadow:0 2px 8px rgba(0,0,0,.1)}
    .code-block{background:#2d2d2d;color:#e0e0e0;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;font-size:14px;overflow:auto;position:relative}
    .code-header{background:#1a1a1a;color:#28a745;padding:8px 15px;margin:-20px -20px 15px -20px;border-radius:8px 8px 0 0;font-weight:bold;font-size:12px}
    .copy-button{position:absolute;top:10px;right:10px;background:#28a745;color:#fff;border:none;padding:4px 8px;border-radius:4px;cursor:pointer;font-size:10px}
    .architecture-flow{display:flex;justify-content:space-between;align-items:center;margin:20px 0;padding:20px;background:#fff;border-radius:12px;border:2px solid #e9ecef;flex-wrap:wrap;gap:15px}
    .arch-component{background:#f8f9fa;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center;min-width:120px;flex:1}
    .arch-component.vision{border-color:#dc3545}
    .arch-component.language{border-color:#007bff}
    .arch-component.action{border-color:#fd7e14}
    .arch-component.fusion{border-color:#28a745}
    .arch-component.output{border-color:#ffc107}
    .arch-arrow{font-size:24px;color:#28a745;font-weight:bold}
    .model-comparison{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:20px;margin:20px 0}
    .model-card{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;transition:all .3s;cursor:pointer}
    .model-card:hover{border-color:#28a745;transform:translateY(-3px);box-shadow:0 6px 20px rgba(0,0,0,.1)}
    .model-card.selected{border-color:#28a745;background:#d4edda}
    .model-name{font-size:1.3em;font-weight:bold;margin-bottom:10px;color:#2d2d2d}
    .model-specs{background:#f8f9fa;padding:12px;border-radius:6px;margin:10px 0;font-size:12px;font-family:'Courier New',monospace}
    .model-capabilities{margin:15px 0}
    .capability{display:flex;align-items:center;margin:5px 0;font-size:14px}
    .capability-icon{margin-right:8px;font-size:16px}
    .token-flow{display:flex;flex-wrap:wrap;gap:8px;margin:15px 0;padding:15px;background:#f8f9fa;border-radius:8px}
    .token{padding:8px 12px;background:#e9ecef;border-radius:6px;font-family:'Courier New',monospace;font-size:12px;border:2px solid transparent}
    .token.text{border-color:#007bff;background:#cce7ff}
    .token.image{border-color:#dc3545;background:#ffcccc}
    .token.action{border-color:#fd7e14;background:#fed7aa}
    .token.special{border-color:#28a745;background:#ccffcc}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(150px,1fr));gap:15px;margin:20px 0}
    .metric-card{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center}
    .metric-value{font-size:1.5em;font-weight:bold;color:#2d2d2d;margin-bottom:5px}
    .metric-label{font-size:12px;color:#666}
    .benchmark-table{width:100%;border-collapse:collapse;margin:15px 0;background:#fff;border:1px solid #e9ecef;border-radius:8px;overflow:hidden}
    .benchmark-table th{background:#2d2d2d;color:#fff;padding:12px;text-align:center;font-weight:bold}
    .benchmark-table td{padding:12px;text-align:center;border-bottom:1px solid #e9ecef}
    .score-excellent{background:#d4edda;color:#155724;font-weight:bold}
    .score-good{background:#d1ecf1;color:#0c5460;font-weight:bold}
    .score-average{background:#fff3cd;color:#856404;font-weight:bold}
    .score-poor{background:#f8d7da;color:#721c24;font-weight:bold}
    .robot-simulation{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0;text-align:center}
    .robot-icon{font-size:4em;margin:20px 0;animation:robotMove 2s ease-in-out infinite}
    @keyframes robotMove{0%{transform:translateY(0px)}50%{transform:translateY(-10px)}100%{transform:translateY(0px)}}
    .action-sequence{display:flex;flex-wrap:wrap;gap:10px;margin:15px 0;padding:15px;background:#fff3cd;border-radius:8px}
    .action-step{background:#fff;border:2px solid #fd7e14;border-radius:6px;padding:10px;flex:1;min-width:150px;text-align:center}
    .comparison-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:15px;margin:20px 0}
    .comparison-card{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;cursor:pointer;transition:all .3s}
    .comparison-card:hover{border-color:#28a745;transform:translateY(-2px)}
    .comparison-card.selected{border-color:#28a745;background:#d4edda}
    .comparison-title{font-weight:bold;margin-bottom:8px;color:#2d2d2d}
    .comparison-description{font-size:13px;color:#666;margin-bottom:8px}
    .comparison-math{background:#f8f9fa;padding:8px;border-radius:4px;font-size:11px;font-family:'Courier New',monospace}
    .info{background:#d1ecf1;border-left:4px solid #17a2b8;color:#0c5460;padding:15px;border-radius:8px;margin:15px 0}
    .success{background:#d4edda;border-left:4px solid #28a745;color:#155724;padding:15px;border-radius:8px;margin:15px 0}
    .warning{background:#fff3cd;border-left:4px solid #ffc107;color:#856404;padding:15px;border-radius:8px;margin:15px 0}
    .breakthrough-highlight{background:linear-gradient(135deg,#28a745,#20c997);color:#fff;padding:20px;border-radius:12px;text-align:center;margin:20px 0;font-size:18px;font-weight:bold;box-shadow:0 4px 15px rgba(40,167,69,.3)}
    .tabs{display:flex;margin-bottom:20px;border-bottom:2px solid #e9ecef}
    .tab{padding:12px 24px;cursor:pointer;border-bottom:3px solid transparent;font-weight:bold;transition:all .3s}
    .tab:hover{background:#f8f9fa}
    .tab.active{border-bottom-color:#28a745;background:#d4edda;color:#155724}
    .tab-content{display:none}
    .tab-content.active{display:block}
    @keyframes pulse{0%{transform:scale(1);opacity:.8}50%{transform:scale(1.05);opacity:1}100%{transform:scale(1);opacity:.8}}
    .processing-animation{animation:pulse 2s ease-in-out infinite}
    .timeline{position:relative;margin:20px 0}
    .timeline-item{display:flex;align-items:center;margin:15px 0;padding:15px;background:#fff;border-radius:8px;border-left:4px solid #28a745}
    .timeline-date{font-weight:bold;min-width:100px;color:#28a745}
    .timeline-content{flex:1;margin-left:15px}
    .performance-indicator{display:inline-block;width:12px;height:12px;border-radius:50%;margin:0 2px}
    .perf-excellent{background:#28a745}
    .perf-good{background:#17a2b8}
    .perf-average{background:#ffc107}
    .perf-poor{background:#dc3545}
    .vla-playground{background:linear-gradient(135deg,#e3f2fd,#f3e5f5);border:3px solid #28a745;border-radius:15px;padding:25px;margin:20px 0}
    .robot-workspace{background:#fff;border:2px dashed #28a745;border-radius:10px;padding:20px;margin:15px 0;min-height:200px;position:relative}
    .robot-avatar{position:absolute;top:50%;left:20px;transform:translateY(-50%);font-size:3em;transition:all 1s ease}
    .object{position:absolute;font-size:2em;transition:all 1s ease;cursor:pointer}
    .action-panel{background:#f8f9fa;border-radius:8px;padding:15px;margin:15px 0}
  </style>
</head>
<body>
  <div class="nav-bar">
    <div class="nav-title">ü§ñ Vision-Language-Action Fundamentals: The Robotics Revolution</div>
    <a href="index.html" class="nav-home">üè† Home</a>
    <a href="vision-language-models.html" class="nav-prev">‚Üê VLM Fundamentals</a>
    <a href="training-deploying-vlas.html" class="nav-next">Next: Training & Deploying VLAs ‚Üí</a>
  </div>

  <div class="container">
    <h1>ü§ñ Vision-Language-Action: From Understanding to Embodied Intelligence</h1>
    <p>Building on Vision-Language Models, <strong>Vision-Language-Action (VLA) models</strong> represent the next evolutionary leap: AI systems that can not only see and understand the world, but can <strong>interact with it physically</strong>. These models bridge the gap between digital reasoning and real-world action, enabling robots to understand instructions, perceive their environment, and execute complex tasks.</p>
    
    <div class="breakthrough-highlight">
      üåü The Final Frontier: From GPT understanding text to robots understanding and acting in the physical world ‚Äî VLAs make AI embodied!
    </div>
  </div>

  <div class="container">
    <h2>üöÄ Section 1: The VLA Revolution - Why Robots Need Transformers</h2>
    
    <div class="step">
      <h3>üèóÔ∏è From VLMs to VLAs: The Final Frontier</h3>
      <p>While Vision-Language Models revolutionized how AI understands multimodal content, they remained confined to digital responses. <strong>VLA models break this barrier</strong> by adding a crucial third modality: <em>actions</em>. This enables AI to move from passive understanding to active interaction with the physical world.</p>

      <div class="architecture-flow">
        <div class="arch-component vision">
          <h4>üëÅÔ∏è Vision</h4>
          <div>Camera Input</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ RGB images/video<br>‚Ä¢ Depth information<br>‚Ä¢ Scene understanding
          </div>
        </div>
        <div class="arch-arrow">+</div>
        <div class="arch-component language">
          <h4>üó£Ô∏è Language</h4>
          <div>Text Instructions</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Natural language commands<br>‚Ä¢ Task descriptions<br>‚Ä¢ Conversational context
          </div>
        </div>
        <div class="arch-arrow">+</div>
        <div class="arch-component action">
          <h4>ü§ñ Action</h4>
          <div>Robot Control</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Joint positions<br>‚Ä¢ End-effector poses<br>‚Ä¢ Movement trajectories
          </div>
        </div>
        <div class="arch-arrow">=</div>
        <div class="arch-component fusion">
          <h4>üß† Embodied AI</h4>
          <div>Integrated Intelligence</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Scene-aware planning<br>‚Ä¢ Instruction following<br>‚Ä¢ Real-world interaction
          </div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üé≠ VLA Evolution Timeline</div>
        <p><strong>Explore the journey from text-only AI to embodied intelligence:</strong></p>

        <div class="controls">
          <div class="control-group">
            <label>AI Evolution Stage:</label>
            <select id="evolutionStage">
              <option value="language" selected>Language Models (GPT-3)</option>
              <option value="vision-language">Vision-Language (GPT-4V)</option>
              <option value="action">Vision-Language-Action (VLA)</option>
              <option value="embodied">Fully Embodied AI (Future)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Capability Focus:</label>
            <select id="capabilityFocus">
              <option value="understanding" selected>Understanding</option>
              <option value="reasoning">Reasoning</option>
              <option value="planning">Planning</option>
              <option value="execution">Execution</option>
            </select>
          </div>
        </div>

        <button onclick="exploreEvolution()" class="primary">üöÄ Explore AI Evolution</button>
        <div id="evolutionVisualization"></div>
      </div>
    </div>

    <div class="step">
      <h3>üîß The Robotics Bottleneck: Why Traditional Approaches Failed</h3>
      <div class="comparison-grid">
        <div class="comparison-card" onclick="selectApproach('traditional', this)">
          <div class="comparison-title">üè≠ Traditional Robotics</div>
          <div class="comparison-description">Rule-based, task-specific programming</div>
          <div class="comparison-math">
            if sensor_reading &gt; threshold:<br>&nbsp;&nbsp;execute_hardcoded_action()
          </div>
        </div>
        <div class="comparison-card" onclick="selectApproach('learning', this)">
          <div class="comparison-title">üß† Learning-Based Methods</div>
          <div class="comparison-description">RL and imitation learning approaches</div>
          <div class="comparison-math">
            œÄ(a|s) = policy_network(state)<br>Limited to specific tasks
          </div>
        </div>
        <div class="comparison-card" onclick="selectApproach('vla', this)">
          <div class="comparison-title">üåü VLA Foundation Models</div>
          <div class="comparison-description">Transformer-based generalist robots</div>
          <div class="comparison-math">
            action_tokens = VLA(image, text)<br>Universal across tasks & robots
          </div>
        </div>
      </div>
      <div id="approachAnalysis"></div>
    </div>

    <div class="step">
      <h3>üéØ The Paradigm Shift: Foundation Models for Physical Intelligence</h3>
      <div class="warning">
        <strong>üè≠ Traditional Robotics Problems:</strong><br>
        ‚Ä¢ <strong>Task-specific:</strong> Each robot application requires months of custom programming<br>
        ‚Ä¢ <strong>Brittle:</strong> Fails when encountering slightly different scenarios<br>
        ‚Ä¢ <strong>Expensive:</strong> $50K‚Äì$500K per robot deployment including engineering<br>
        ‚Ä¢ <strong>Limited:</strong> Cannot adapt to new tasks without complete reprogramming<br><br>
        <strong>üìä Industry Statistics:</strong><br>
        ‚Ä¢ 80% of industrial robots perform single, repetitive tasks<br>
        ‚Ä¢ Average deployment time: 6‚Äì18 months per application<br>
        ‚Ä¢ 90% of robots cannot adapt to unexpected situations
      </div>

      <div class="success">
        <strong>üöÄ VLA Foundation Model Benefits:</strong><br>
        ‚Ä¢ <strong>Generalist:</strong> One model handles diverse tasks across robot types<br>
        ‚Ä¢ <strong>Sample efficient:</strong> Learn new tasks from few demonstrations<br>
        ‚Ä¢ <strong>Cost effective:</strong> Reduce deployment costs by 10‚Äì100√ó<br>
        ‚Ä¢ <strong>Adaptable:</strong> Handle novel situations through language instructions<br><br>
        <strong>üìà VLA Performance Metrics:</strong><br>
        ‚Ä¢ Cross-task transfer: 5‚Äì20√ó faster learning on new tasks<br>
        ‚Ä¢ Zero-shot capabilities: Handle 40‚Äì60% of novel scenarios<br>
        ‚Ä¢ Deployment time: Days to weeks instead of months<br>
        ‚Ä¢ Cost reduction: 90%+ reduction in custom engineering
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üîÑ Traditional vs VLA Comparison</div>
        <div class="controls">
          <div class="control-group">
            <label>Task Complexity:</label>
            <select id="taskComplexity">
              <option value="simple" selected>Simple (Pick & Place)</option>
              <option value="medium">Medium (Assembly Task)</option>
              <option value="complex">Complex (Cooking Recipe)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Environment Variability:</label>
            <select id="environmentVar">
              <option value="controlled" selected>Controlled Lab</option>
              <option value="structured">Structured Workspace</option>
              <option value="unstructured">Real-world Chaos</option>
            </select>
          </div>
        </div>
        <button onclick="compareApproaches()" class="primary">‚öñÔ∏è Compare Approaches</button>
        <div id="comparisonResults"></div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üß© Section 2: Action Tokenization - Making Movements Into Language</h2>

    <div class="step">
      <h3>üîÑ The Core Innovation: Actions as Tokens</h3>
      <p>The breakthrough insight of VLA models is treating robot actions as <strong>discrete tokens</strong>, just like words in language. This allows the same transformer architecture that powers ChatGPT to control robot movements with unprecedented generality and efficiency.</p>

      <div class="math-formula">
        <strong>Action Tokenization Pipeline:</strong><br><br>
        <strong>1. Continuous Action Space:</strong><br>
        a<sub>continuous</sub> ‚àà ‚Ñù<sup>d</sup> (e.g., 7-DOF arm: joint angles, gripper)<br><br>
        <strong>2. Discretization Strategies:</strong><br>
        ‚Ä¢ <strong>Binning:</strong> a<sub>discrete</sub> = quantize(a<sub>continuous</sub>, bins=256)<br>
        ‚Ä¢ <strong>Vector Quantization:</strong> a<sub>tokens</sub> = VQ-VAE(a<sub>continuous</sub>)<br>
        ‚Ä¢ <strong>DCT Compression:</strong> a<sub>compressed</sub> = DCT(action_sequence)<br><br>
        <strong>3. Token Sequence:</strong><br>
        [IMG] patch‚ÇÅ patch‚ÇÇ ‚Ä¶ [TXT] "pick" "up" "cup" [ACT] a‚ÇÅ a‚ÇÇ a‚ÇÉ [/ACT]<br><br>
        <strong>4. Unified Processing:</strong><br>
        Next_Action_Token = Transformer(vision_tokens + text_tokens + action_history)
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üé≠ Interactive Action Tokenizer</div>
        <p><strong>See how robot movements become discrete tokens:</strong></p>
        <div class="controls">
          <div class="control-group">
            <label>Robot Action:</label>
            <select id="robotAction">
              <option value="reach" selected>Reach to Object</option>
              <option value="grasp">Grasp Object</option>
              <option value="lift">Lift & Move</option>
              <option value="place">Place Object</option>
              <option value="pour">Pour Liquid</option>
            </select>
          </div>
          <div class="control-group">
            <label>Tokenization Method:</label>
            <select id="tokenizationMethod">
              <option value="binning" selected>Uniform Binning</option>
              <option value="vq">Vector Quantization</option>
              <option value="dct">DCT Compression (FAST)</option>
              <option value="flow">Flow Matching</option>
            </select>
          </div>
          <div class="control-group">
            <label>Token Resolution:</label>
            <select id="tokenResolution">
              <option value="low">Low (64 tokens)</option>
              <option value="medium" selected>Medium (256 tokens)</option>
              <option value="high">High (1024 tokens)</option>
            </select>
          </div>
        </div>
        <button onclick="generateActionTokens()" class="primary">üé≠ Tokenize Action</button>
        <div id="actionTokenVisualization"></div>
      </div>
    </div>

    <div class="step">
      <h3>üìä Action Space Mathematics: From Continuous to Discrete</h3>

      <div class="math-formula">
        <strong>Action Space Transformation Mathematics:</strong><br><br>
        <strong>Robot State Space:</strong><br>
        s = [x, y, z, roll, pitch, yaw, gripper] ‚àà ‚Ñù‚Å∑<br>
        Continuous range: x,y,z ‚àà [-1, 1], angles ‚àà [-œÄ, œÄ]<br><br>
        <strong>Binning Tokenization:</strong><br>
        token_x = ‚åä(x + 1) √ó (bins/2)‚åã ‚àà [0, bins-1]<br>
        Total vocabulary: bins^7 = 256‚Å∑ ‚âà 72 trillion tokens!<br><br>
        <strong>Vector Quantization (Better):</strong><br>
        VQ-VAE: a ‚Üí encode(a) ‚Üí quantize(z) ‚Üí decode(z‚Ä≤) ‚Üí a‚Ä≤<br>
        Vocabulary size: ~8K tokens (manageable for transformers)<br><br>
        <strong>FAST (DCT) Tokenization:</strong><br>
        Trajectory œÑ = [a‚ÇÅ, a‚ÇÇ, ‚Ä¶, a‚Çú] ‚Üí DCT(œÑ) ‚Üí top-k frequencies<br>
        Enables high-frequency control (200Hz) with low token count
      </div>

      <div class="tabs">
        <div class="tab active" onclick="switchActionTab('binning', this)">üìä Uniform Binning</div>
        <div class="tab" onclick="switchActionTab('vq', this)">üß© Vector Quantization</div>
        <div class="tab" onclick="switchActionTab('dct', this)">‚ö° DCT (FAST)</div>
        <div class="tab" onclick="switchActionTab('flow', this)">üåä Flow Matching</div>
      </div>

      <div id="binning" class="tab-content active">
        <div class="code-block">
          <div class="code-header">üìä Uniform Binning Implementation</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import numpy as np
import torch

class UniformActionTokenizer:
    """
    Simple uniform binning for action tokenization
    """
    def __init__(self, action_dims=7, bins_per_dim=256, action_range=(-1, 1)):
        self.action_dims = action_dims
        self.bins_per_dim = bins_per_dim
        self.action_min, self.action_max = action_range
        
        # Calculate bin edges
        self.bin_edges = np.linspace(self.action_min, self.action_max, bins_per_dim + 1)
        
        # Vocabulary size grows exponentially with dimensions!
        self.vocab_size = bins_per_dim ** action_dims
        print(f"Warning: Vocab size = {self.vocab_size:,} tokens!")
    
    def tokenize(self, actions):
        """Convert continuous actions to discrete tokens"""
        # Clip actions to valid range
        actions = np.clip(actions, self.action_min, self.action_max)
        
        # Bin each dimension
        tokens = []
        for dim in range(self.action_dims):
            # Find which bin each action falls into
            dim_tokens = np.digitize(actions[:, dim], self.bin_edges) - 1
            # Ensure tokens are in valid range
            dim_tokens = np.clip(dim_tokens, 0, self.bins_per_dim - 1)
            tokens.append(dim_tokens)
        
        return np.array(tokens).T  # Shape: [sequence_length, action_dims]
    
    def detokenize(self, tokens):
        """Convert discrete tokens back to continuous actions"""
        actions = []
        for dim in range(self.action_dims):
            # Get bin centers as continuous values
            dim_actions = self.bin_edges[tokens[:, dim]] + \
                         (self.bin_edges[1] - self.bin_edges[0]) / 2
            actions.append(dim_actions)
        
        return np.array(actions).T
    
    def get_vocab_size(self):
        return self.vocab_size

# Example usage
tokenizer = UniformActionTokenizer(action_dims=7, bins_per_dim=256)

# Sample robot trajectory
trajectory = np.random.uniform(-1, 1, (10, 7))  # 10 timesteps, 7-DOF robot
print("Original trajectory shape:", trajectory.shape)

# Tokenize
tokens = tokenizer.tokenize(trajectory)
print("Tokenized shape:", tokens.shape)
print("Sample tokens:", tokens[0])  # First timestep tokens

# Detokenize
reconstructed = tokenizer.detokenize(tokens)
print("Reconstruction error:", np.mean(np.abs(trajectory - reconstructed)))

# Problem: Massive vocabulary size makes this impractical!
print(f"Vocabulary size: {tokenizer.get_vocab_size():,} tokens")
print("This would require a {:.1f}B parameter embedding layer!".format(
    tokenizer.get_vocab_size() * 512 / 1e9  # Assuming 512D embeddings
))</pre>
        </div>
        <div class="warning">
          <strong>‚ö†Ô∏è Uniform Binning Challenges:</strong><br>
          ‚Ä¢ <strong>Exponential vocabulary growth:</strong> 256‚Å∑ = 72+ trillion possible tokens<br>
          ‚Ä¢ <strong>Sparse data:</strong> Most token combinations never seen in training<br>
          ‚Ä¢ <strong>Quantization error:</strong> Loses precision in continuous space<br>
          ‚Ä¢ <strong>Memory requirements:</strong> Embedding layer needs massive parameters<br><br>
          <strong>Why it fails:</strong> Works for 1‚Äì2 dimensions, completely impractical for 7+ DOF robots
        </div>
      </div>

      <div id="vq" class="tab-content">
        <div class="code-block">
          <div class="code-header">üß© Vector Quantization (VQ-VAE) Approach</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import torch
import torch.nn as nn
import torch.nn.functional as F

class VectorQuantizer(nn.Module):
    """
    Vector Quantization layer for action tokenization
    Learns a codebook of common action patterns
    """
    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):
        super().__init__()
        
        self.embedding_dim = embedding_dim
        self.num_embeddings = num_embeddings
        self.commitment_cost = commitment_cost
        
        # Initialize codebook with uniform distribution
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)
        
    def forward(self, inputs):
        # Convert inputs from [B, C, H, W] to [B, H, W, C]
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape
        
        # Flatten input
        flat_input = inputs.view(-1, self.embedding_dim)
        
        # Calculate distances to codebook entries
        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) 
                    + torch.sum(self.embedding.weight**2, dim=1)
                    - 2 * torch.matmul(flat_input, self.embedding.weight.t()))
        
        # Find closest codebook entry
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)
        
        # Quantize and unflatten
        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)
        
        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        q_latent_loss = F.mse_loss(quantized, inputs.detach())
        loss = q_latent_loss + self.commitment_cost * e_latent_loss
        
        # Straight-through estimator
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))
        
        return quantized.permute(0, 3, 1, 2).contiguous(), loss, perplexity, encoding_indices

class ActionVQVAE(nn.Module):
    """
    VQ-VAE specifically designed for robot action sequences
    """
    def __init__(self, action_dim=7, hidden_dim=128, num_embeddings=8192):
        super().__init__()
        
        self.action_dim = action_dim
        
        # Encoder: action sequence -> latent space
        self.encoder = nn.Sequential(
            nn.Linear(action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Vector quantizer
        self.vq_layer = VectorQuantizer(num_embeddings, hidden_dim)
        
        # Decoder: quantized latent -> action sequence
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
    def forward(self, actions):
        # Encode actions
        encoded = self.encoder(actions)
        
        # Add spatial dimensions for VQ layer (hack for 1D actions)
        encoded = encoded.unsqueeze(-1).unsqueeze(-1)  # [B, T, C, 1, 1]
        encoded = encoded.permute(0, 2, 3, 4, 1)       # [B, C, 1, 1, T]
        
        # Vector quantize
        quantized, vq_loss, perplexity, encoding_indices = self.vq_layer(encoded.squeeze(-1))
        
        # Remove spatial dimensions
        quantized = quantized.squeeze(-1).squeeze(-1)
        quantized = quantized.permute(0, 2, 1)  # [B, T, C]
        
        # Decode
        reconstructed = self.decoder(quantized)
        
        return {
            'reconstructed': reconstructed,
            'vq_loss': vq_loss,
            'perplexity': perplexity,
            'encoding_indices': encoding_indices,
            'quantized': quantized
        }

# Training the VQ-VAE
def train_action_vqvae(model, dataloader, num_epochs=100):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_actions in dataloader:
            optimizer.zero_grad()
            
            outputs = model(batch_actions)
            
            # Reconstruction loss
            recon_loss = F.mse_loss(outputs['reconstructed'], batch_actions)
            
            # Total loss
            loss = recon_loss + outputs['vq_loss']
            loss.backward()
            
            optimizer.step()
            total_loss += loss.item()
        
        if epoch % 20 == 0:
            print(f"Epoch {epoch}, Loss: {total_loss/len(dataloader):.4f}, "
                  f"Perplexity: {outputs['perplexity']:.2f}")

# Example usage for robot actions
action_vqvae = ActionVQVAE(action_dim=7, num_embeddings=8192)

# Create sample robot trajectory data
robot_trajectories = torch.randn(1000, 50, 7)  # 1000 trajectories, 50 timesteps, 7-DOF

# The VQ-VAE learns to represent actions with only 8192 tokens instead of 256^7!
print(f"Vocabulary size: {8192} tokens (vs {256**7:,} for binning)")
print("This makes transformer training feasible!")</pre>
        </div>
        <div class="success">
          <strong>‚úÖ Vector Quantization Advantages:</strong><br>
          ‚Ä¢ <strong>Manageable vocabulary:</strong> 8K tokens instead of trillions<br>
          ‚Ä¢ <strong>Learned representations:</strong> Captures common action patterns<br>
          ‚Ä¢ <strong>Continuous reconstruction:</strong> Can recover smooth movements<br>
          ‚Ä¢ <strong>Transfer learning:</strong> Codebook transfers across robot types<br><br>
          <strong>üéØ Real-world Success:</strong><br>
          ‚Ä¢ OpenVLA-like designs use VQ approaches for 7B-scale models<br>
          ‚Ä¢ Enables training on ~1M robot demonstrations<br>
          ‚Ä¢ Competes with much larger proprietary systems
        </div>
      </div>

      <div id="dct" class="tab-content">
        <div class="code-block">
          <div class="code-header">‚ö° FAST (DCT) Tokenization</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import numpy as np
import torch
import torch.nn as nn
from scipy.fft import dct, idct

class FASTActionTokenizer:
    """
    FAST (Frequency-domain Action Sequence Tokenization)
    Uses DCT compression for high-frequency robot control
    """
    def __init__(self, sequence_length=64, action_dim=7, num_frequencies=16):
        self.sequence_length = sequence_length
        self.action_dim = action_dim
        self.num_frequencies = num_frequencies
        
        # Total tokens per action sequence
        self.tokens_per_sequence = num_frequencies * action_dim
        
        # Create frequency bins for quantization
        self.freq_bins = np.linspace(-1, 1, 256)  # 8-bit quantization
        
    def encode_sequence(self, action_sequence):
        """
        Convert action sequence to frequency domain tokens
        
        Args:
            action_sequence: [sequence_length, action_dim] numpy array
        
        Returns:
            tokens: [num_frequencies * action_dim] discrete tokens
        """
        # Apply DCT to each action dimension
        freq_coefficients = dct(action_sequence, axis=0, norm='ortho')
        
        # Keep only the most important frequencies
        truncated_coeffs = freq_coefficients[:self.num_frequencies, :]
        
        # Quantize frequency coefficients
        quantized_coeffs = np.digitize(truncated_coeffs.flatten(), self.freq_bins) - 1
        quantized_coeffs = np.clip(quantized_coeffs, 0, len(self.freq_bins) - 1)
        
        return quantized_coeffs
    
    def decode_sequence(self, tokens):
        """
        Convert frequency domain tokens back to action sequence
        
        Args:
            tokens: [num_frequencies * action_dim] discrete tokens
        
        Returns:
            action_sequence: [sequence_length, action_dim] numpy array
        """
        # Dequantize tokens back to frequency coefficients
        dequantized = self.freq_bins[tokens]
        freq_coefficients = dequantized.reshape(self.num_frequencies, self.action_dim)
        
        # Pad with zeros for remaining frequencies
        full_coefficients = np.zeros((self.sequence_length, self.action_dim))
        full_coefficients[:self.num_frequencies, :] = freq_coefficients
        
        # Apply inverse DCT
        reconstructed_sequence = idct(full_coefficients, axis=0, norm='ortho')
        
        return reconstructed_sequence
    
    def get_vocab_size(self):
        return len(self.freq_bins)  # Only 256 tokens needed!

class FASTActionModel(nn.Module):
    """
    Transformer model using FAST tokenization for robot control
    Enables high-frequency control (200Hz) with efficient tokenization
    """
    def __init__(self, vocab_size=256, hidden_dim=512, num_layers=8, num_heads=8):
        super().__init__()
        
        # Token embeddings for frequency coefficients
        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)
        
        # Transformer layers
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=num_heads,
                dim_feedforward=hidden_dim * 4,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=num_layers
        )
        
        # Output projection to frequency tokens
        self.output_proj = nn.Linear(hidden_dim, vocab_size)
        
    def forward(self, input_tokens):
        # Embed tokens
        embedded = self.token_embedding(input_tokens)
        
        # Transformer processing
        transformed = self.transformer(embedded)
        
        # Project to output vocabulary
        logits = self.output_proj(transformed)
        
        return logits

# Example: High-frequency robot control with FAST
def demonstrate_fast_control():
    # Initialize FAST tokenizer
    tokenizer = FASTActionTokenizer(
        sequence_length=64,    # 64 timesteps at 200Hz = 0.32 seconds
        action_dim=7,          # 7-DOF robot arm
        num_frequencies=16     # Keep top 16 frequency components
    )
    
    print(f"Vocabulary size: {tokenizer.get_vocab_size()} tokens")
    print(f"Tokens per sequence: {tokenizer.tokens_per_sequence}")
    
    # Generate sample smooth robot trajectory
    t = np.linspace(0, 2*np.pi, 64)
    action_sequence = np.zeros((64, 7))
    
    # Create smooth sinusoidal movements for each joint
    for joint in range(7):
        frequency = 0.5 + joint * 0.1  # Different frequency per joint
        action_sequence[:, joint] = 0.5 * np.sin(frequency * t)
    
    print("Original trajectory shape:", action_sequence.shape)
    
    # Tokenize using FAST
    tokens = tokenizer.encode_sequence(action_sequence)
    print("Encoded tokens shape:", tokens.shape)
    print("Sample tokens:", tokens[:10])
    
    # Decode back to actions
    reconstructed = tokenizer.decode_sequence(tokens)
    print("Reconstructed shape:", reconstructed.shape)
    
    # Measure reconstruction quality
    mse_error = np.mean((action_sequence - reconstructed) ** 2)
    print(f"Reconstruction MSE: {mse_error:.6f}")
    
    return tokenizer, action_sequence, reconstructed

# Demonstrate FAST advantages
tokenizer, original, reconstructed = demonstrate_fast_control()

print("\nüöÄ FAST Tokenization Advantages:")
print(f"‚úÖ Compact: Only {tokenizer.tokens_per_sequence} tokens per trajectory")
print(f"‚úÖ High-frequency: Supports 200Hz control (vs 10-20Hz for other methods)")
print(f"‚úÖ Smooth: Preserves trajectory smoothness through frequency domain")
print(f"‚úÖ Efficient: {tokenizer.get_vocab_size()} vocab size vs millions for binning")

# Calculate compression ratio
original_size = original.size * 32  # 32-bit floats
compressed_size = tokenizer.tokens_per_sequence * 8  # 8-bit tokens
compression_ratio = original_size / compressed_size

print(f"‚úÖ Compression: {compression_ratio:.1f}x smaller than full trajectory")</pre>
        </div>
        <div class="info">
          <strong>‚ö° FAST Tokenization Benefits:</strong><br>
          ‚Ä¢ <strong>High-frequency control:</strong> Enables 200Hz robot control vs 10‚Äì20Hz for other methods<br>
          ‚Ä¢ <strong>Frequency efficiency:</strong> Most robot motions are low-frequency, DCT captures this<br>
          ‚Ä¢ <strong>Smooth trajectories:</strong> Frequency domain naturally preserves motion smoothness<br>
          ‚Ä¢ <strong>Compact representation:</strong> 16 frequency coefficients per action dimension<br><br>
          <strong>üéØ Real-world Applications:</strong><br>
          ‚Ä¢ Used in recent VLA-style research for precise manipulation tasks<br>
          ‚Ä¢ Enables real-time robot control with transformer inference<br>
          ‚Ä¢ Particularly effective for continuous, smooth robot motions
        </div>
      </div>

      <div id="flow" class="tab-content">
        <div class="warning">
          <strong>üåä Flow Matching for Action Generation:</strong><br>
          ‚Ä¢ <strong>Continuous generation:</strong> Directly generates smooth action trajectories<br>
          ‚Ä¢ <strong>No discretization:</strong> Avoids quantization errors entirely<br>
          ‚Ä¢ <strong>Diffusion-based:</strong> Uses flow-matching or diffusion models for actions<br>
          ‚Ä¢ <strong>Emerging approach:</strong> Still in research phase, very promising results<br><br>
          <strong>üî¨ Technical Details:</strong><br>
          ‚Ä¢ Model learns to transform noise into valid robot trajectories<br>
          ‚Ä¢ Conditioning on vision and language for task-specific generation<br>
          ‚Ä¢ Enables precise, safe motion planning through learned dynamics<br>
          ‚Ä¢ Can generate multiple candidate trajectories for robust execution
        </div>
      </div>
    </div>

    <div class="step">
      <h3>üéØ Token Efficiency: The Key to Real-Time Control</h3>
      <div class="metric-grid">
        <div class="metric-card"><div class="metric-value">256‚Å∑</div><div class="metric-label">Binning Vocabulary</div></div>
        <div class="metric-card"><div class="metric-value">8K</div><div class="metric-label">VQ-VAE Vocabulary</div></div>
        <div class="metric-card"><div class="metric-value">256</div><div class="metric-label">FAST Vocabulary</div></div>
        <div class="metric-card"><div class="metric-value">200Hz</div><div class="metric-label">Control Frequency</div></div>
      </div>
      <div class="success">
        <strong>üéØ Why Token Efficiency Matters:</strong><br>
        ‚Ä¢ <strong>Inference Speed:</strong> Smaller vocabularies = faster transformer inference<br>
        ‚Ä¢ <strong>Training Stability:</strong> Fewer tokens = better sample efficiency<br>
        ‚Ä¢ <strong>Memory Requirements:</strong> Token embeddings scale with vocabulary size<br>
        ‚Ä¢ <strong>Real-time Control:</strong> Must generate actions faster than robot needs them<br><br>
        <strong>üìä Performance Impact:</strong><br>
        ‚Ä¢ FAST: 200Hz achievable (short-horizon control windows)<br>
        ‚Ä¢ VQ-VAE: 50‚Äì100Hz typical for manipulation<br>
        ‚Ä¢ Binning: 10‚Äì20Hz max, usually too slow<br>
        ‚Ä¢ Flow matching: Variable; depends on sampler steps
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üèÜ Section 3: Open Source vs Closed Source - The VLA David vs Goliath Story</h2>

    <div class="step">
      <h3>üìä OpenVLA vs RT-2-X: When 7B Beats 55B</h3>
      <p>In a striking comparison, an open 7B-parameter VLA can compete with (and sometimes surpass) much larger closed models‚Äîhighlighting how **architecture, data curation, and tokenization** matter as much as raw parameter count.</p>

      <div class="breakthrough-highlight">
        üèÜ Takeaway: Smart representation + training beats brute-force parameter count.
      </div>

      <div class="model-comparison">
        <div class="model-card" onclick="selectVLAModel('openvla', this)" style="border-color:#28a745">
          <div class="model-name">ü¶ô OpenVLA-style (7B, open)</div>
          <div class="model-specs">
            <strong>Backbone:</strong> LLM + vision encoders (DINOv2/SigLIP)<br>
            <strong>Training Data:</strong> ~1M robot demonstrations<br>
            <strong>Training Cost:</strong> O(10^5‚Äì10^6 USD)<br>
            <strong>Performance:</strong> Competitive on manipulation<br>
            <strong>Robots:</strong> Many embodiments via adapters<br>
            <strong>License:</strong> Open
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">üéØ</span><span>Cross-embodiment generalization</span></div>
            <div class="capability"><span class="capability-icon">üí∞</span><span>Lower training cost</span></div>
            <div class="capability"><span class="capability-icon">üîß</span><span>Fine-tuning friendly</span></div>
            <div class="capability"><span class="capability-icon">üìö</span><span>Transparent research</span></div>
          </div>
        </div>

        <div class="model-card" onclick="selectVLAModel('rt2x', this)" style="border-color:#dc3545">
          <div class="model-name">üîí RT-2-X-style (‚âà55B, closed)</div>
          <div class="model-specs">
            <strong>Backbone:</strong> Large multimodal transformer<br>
            <strong>Training Data:</strong> Multi-million demos + web<br>
            <strong>Training Cost:</strong> O(10^7 USD)+<br>
            <strong>Performance:</strong> Strong; access limited<br>
            <strong>Robots:</strong> Internal + select partners<br>
            <strong>License:</strong> Proprietary
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">üè¢</span><span>Big-infra engineering</span></div>
            <div class="capability"><span class="capability-icon">üî¨</span><span>Extensive internal evals</span></div>
            <div class="capability"><span class="capability-icon">‚ö†Ô∏è</span><span>Limited external access</span></div>
            <div class="capability"><span class="capability-icon">üí∏</span><span>High training cost</span></div>
          </div>
        </div>
      </div>

      <div id="vlaModelAnalysis"></div>
    </div>

    <div class="step">
      <h3>üöÄ The Open Source VLA Ecosystem</h3>
      <div class="interactive-demo">
        <div class="demo-title">üåü Open Source VLA Model Explorer</div>
        <div class="controls">
          <div class="control-group">
            <label>Model Category:</label>
            <select id="vlaCategory">
              <option value="general" selected>General Purpose VLAs</option>
              <option value="efficient">Efficient/Mobile VLAs</option>
              <option value="specialized">Specialized Applications</option>
              <option value="research">Research Prototypes</option>
            </select>
          </div>
          <div class="control-group">
            <label>Deployment Target:</label>
            <select id="deploymentTarget">
              <option value="cloud" selected>Cloud/Server Deployment</option>
              <option value="edge">Edge Computing (Jetson)</option>
              <option value="mobile">Mobile/Consumer Robotics</option>
              <option value="industrial">Industrial Applications</option>
            </select>
          </div>
        </div>
        <button onclick="exploreVLAEcosystem()" class="primary">üîç Explore VLA Models</button>
        <div id="vlaEcosystemResults"></div>
      </div>
    </div>

    <div class="step">
      <h3>ü¶ô OpenVLA: The Open Source Pattern</h3>
      <div class="math-formula">
        <strong>OpenVLA-Style Architecture Pipeline:</strong><br><br>
        <strong>1. Multimodal Fusion:</strong><br>
        Vision: DINOv2 + SigLIP ‚Üí visual_tokens ‚àà ‚Ñù^(N√óD)<br>
        Language: LLM tokenizer ‚Üí text_tokens ‚àà ‚Ñù^(M√óD)<br><br>
        <strong>2. Unified Processing:</strong><br>
        Mixed_Tokens = [&lt;image&gt;, visual_tokens, &lt;/image&gt;, text_tokens, &lt;action&gt;, action_history]&nbsp;‚â§ context_len<br><br>
        <strong>3. Action Generation:</strong><br>
        action_logits = LM_Head(Transformer_Output)<br>
        action_tokens = Sample(Softmax(action_logits))<br><br>
        <strong>4. Detokenization:</strong><br>
        robot_actions = VQ_Decoder(action_tokens) ‚àà ‚Ñù^(T√ó7)
      </div>

      <div class="code-block">
        <div class="code-header">ü¶ô OpenVLA-Style Implementation (Display-only)</div>
        <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import torch
import torch.nn as nn
from transformers import LlamaForCausalLM, LlamaTokenizer
import numpy as np

class OpenVLA(nn.Module):
    """
    Simplified, display-only sketch of an OpenVLA-style model.
    NOTE: This snippet illustrates structure. It will not run as-is without
    proper processors, image patching, and checkpoint access.
    """
    def __init__(self, 
                 llama_model="meta-llama/Llama-2-7b-hf",
                 action_vocab_size=8192,
                 max_action_tokens=100):
        super().__init__()
        
        # Core language model (LLM backbone)
        self.llama = LlamaForCausalLM.from_pretrained(llama_model)
        self.tokenizer = LlamaTokenizer.from_pretrained(llama_model)
        
        # Vision encoders (DINOv2 + SigLIP fusion, placeholders)
        self.vision_encoder = self._build_vision_encoder()
        
        # Action tokenization
        self.action_vocab_size = action_vocab_size
        self.max_action_tokens = max_action_tokens
        
        # Extend tokenizer with special multimodal tokens
        special_tokens = {
            'additional_special_tokens': [
                '<image>', '</image>', '<action>', '</action>',
                '<robot>', '</robot>', '<task>', '</task>'
            ]
        }
        self.tokenizer.add_special_tokens(special_tokens)
        self.llama.resize_token_embeddings(len(self.tokenizer))
        
        # Action head for generating robot control tokens
        hidden_size = self.llama.config.hidden_size
        self.action_head = nn.Linear(hidden_size, action_vocab_size)
        
    def _build_vision_encoder(self):
        """Build fusion of DINOv2 and SigLIP vision encoders (placeholders)."""
        from transformers import AutoModel
        
        dinov2 = AutoModel.from_pretrained("facebook/dinov2-base")  # requires image processor in practice
        siglip = AutoModel.from_pretrained("google/siglip-base-patch16-224")
        
        # Match LLM hidden size with a projection
        fusion_proj = nn.Linear(768 + 768, self.llama.config.hidden_size)
        return nn.ModuleDict({'dinov2': dinov2, 'siglip': siglip, 'fusion': fusion_proj})
    
    def encode_vision(self, images):
        """
        Encode images using DINOv2 + SigLIP fusion.
        images: preprocessed image tensor batches.
        """
        dinov2_features = self.vision_encoder.dinov2(images).last_hidden_state
        siglip_features = self.vision_encoder.siglip(images).last_hidden_state
        fused = torch.cat([dinov2_features, siglip_features], dim=-1)
        vision_tokens = self.vision_encoder.fusion(fused)  # [B, N_img, D]
        return vision_tokens
    
    def _embed_special(self, token_str, device):
        tid = self.tokenizer.convert_tokens_to_ids(token_str)
        tid = torch.tensor([tid], device=device)
        return self.llama.get_input_embeddings()(tid)[None, :, :]  # [1, 1, D]
    
    def _build_input_sequence(self, vision_tokens, text_input_ids, action_history_embeds=None):
        """
        Concatenate: <image> + vision_tokens + </image> + text_embeds + optional <action> + history.
        Returns embeddings ready for the LLM forward.
        """
        device = vision_tokens.device
        text_embeds = self.llama.get_input_embeddings()(text_input_ids)  # [B, T_txt, D]
        
        img_start = self._embed_special('<image>', device)       # [1,1,D]
        img_end   = self._embed_special('</image>', device)
        
        if action_history_embeds is not None:
            act_start = self._embed_special('<action>', device)
            act_end   = self._embed_special('</action>', device)
        else:
            act_start = act_end = None
        
        B = text_embeds.size(0)
        # Tile special tokens to batch
        img_start = img_start.expand(B, -1, -1)  # [B,1,D]
        img_end   = img_end.expand(B, -1, -1)    # [B,1,D]
        if act_start is not None:
            act_start = act_start.expand(B, -1, -1)
            act_end   = act_end.expand(B, -1, -1)
        
        seq_parts = [img_start, vision_tokens, img_end, text_embeds]
        if action_history_embeds is not None:
            seq_parts.extend([act_start, action_history_embeds, act_end])
        
        return torch.cat(seq_parts, dim=1)  # [B, T_total, D]
    
    def forward(self, images, text_input, action_history=None):
        """
        Forward pass for VLA inference (display-only).
        """
        device = images.device
        B = images.size(0)
        
        # Encode vision
        vision_tokens = self.encode_vision(images)  # [B, N_vis, D]
        
        # Tokenize text
        tok = self.tokenizer(list(text_input), return_tensors='pt', padding=True, truncation=True, max_length=512)
        text_input_ids = tok.input_ids.to(device)   # [B, T_txt]
        
        # Optional: embed previous action tokens (if provided)
        action_history_embeds = None
        if action_history is not None:
            # Assume action_history already token ids shaped [B, T_act]
            action_history_embeds = self.llama.get_input_embeddings()(action_history.to(device))
        
        # Build multimodal sequence
        inputs_embeds = self._build_input_sequence(vision_tokens, text_input_ids, action_history_embeds)
        
        # Run LLM
        out = self.llama(inputs_embeds=inputs_embeds, use_cache=False)
        
        # Project to action vocab
        action_logits = self.action_head(out.last_hidden_state)  # [B, T_total, V_act]
        return action_logits</pre>
      </div>
    </div>
  </div>
  <div class="container">
    <h2>üî¨ Section 4: RT-2 vs OpenVLA Architecture Deep Dive</h2>

    <div class="step">
      <h3>üèóÔ∏è RT-2: The Closed Source Pioneer</h3>
      <p>Google's RT-2 (Robotics Transformer 2) established the VLA paradigm by treating robot actions as text tokens. Built on PaLM-E foundations, RT-2 demonstrated that large multimodal models could generalize across robot embodiments and tasks.</p>

      <div class="architecture-flow">
        <div class="arch-component vision">
          <h4>üì∑ Vision Processing</h4>
          <div>ViT Backbone</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ 224√ó224 RGB images<br>‚Ä¢ Patch tokenization<br>‚Ä¢ Spatial attention
          </div>
        </div>
        <div class="arch-arrow">‚Üí</div>
        <div class="arch-component language">
          <h4>üß† PaLM-E Core</h4>
          <div>Multimodal LLM</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ 55B+ parameters<br>‚Ä¢ Joint vision-text training<br>‚Ä¢ Instruction following
          </div>
        </div>
        <div class="arch-arrow">‚Üí</div>
        <div class="arch-component action">
          <h4>üéØ Action Head</h4>
          <div>String Tokenization</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Actions as text<br>‚Ä¢ "move(0.1,0.2,0.3)"<br>‚Ä¢ Language modeling loss
          </div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üîç RT-2 Action Tokenization</div>
        <p><strong>RT-2 treats robot actions as natural language strings:</strong></p>

        <div class="controls">
          <div class="control-group">
            <label>Robot Task:</label>
            <select id="rt2Task">
              <option value="pickup" selected>Pick up the apple</option>
              <option value="place">Place cup on table</option>
              <option value="pour">Pour water into glass</option>
              <option value="open">Open the drawer</option>
            </select>
          </div>
          <div class="control-group">
            <label>Action Precision:</label>
            <select id="rt2Precision">
              <option value="low" selected>Low (2 decimals)</option>
              <option value="medium">Medium (3 decimals)</option>
              <option value="high">High (4 decimals)</option>
            </select>
          </div>
        </div>

        <button onclick="generateRT2Actions()" class="primary">ü§ñ Generate RT-2 Actions</button>
        <div id="rt2ActionOutput"></div>
      </div>
    </div>

    <div class="step">
      <h3>ü¶ô OpenVLA: The Open Source Revolution</h3>
      <p>OpenVLA demonstrates that open source approaches can match or exceed proprietary systems through careful architecture design, efficient tokenization, and community-driven data curation.</p>

      <div class="tabs">
        <div class="tab active" onclick="switchArchTab('overview', this)">üèóÔ∏è Architecture</div>
        <div class="tab" onclick="switchArchTab('training', this)">üéì Training</div>
        <div class="tab" onclick="switchArchTab('performance', this)">üìä Performance</div>
        <div class="tab" onclick="switchArchTab('deployment', this)">üöÄ Deployment</div>
      </div>

      <div id="overview" class="tab-content active">
        <div class="math-formula">
          <strong>OpenVLA Multi-Stage Architecture:</strong><br><br>
          <strong>Stage 1 - Vision Encoding:</strong><br>
          I<sub>RGB</sub> ‚àà ‚Ñù<sup>H√óW√ó3</sup> ‚Üí DINOv2(I) + SigLIP(I) ‚Üí V<sub>tokens</sub> ‚àà ‚Ñù<sup>N√ó768</sup><br><br>
          <strong>Stage 2 - Multimodal Fusion:</strong><br>
          [&lt;img&gt;, V<sub>tokens</sub>, &lt;/img&gt;, T<sub>instruction</sub>, &lt;act&gt;, A<sub>history</sub>] ‚Üí LLama-7B<br><br>
          <strong>Stage 3 - Action Prediction:</strong><br>
          Hidden<sub>states</sub> ‚Üí ActionHead(h) ‚Üí Softmax ‚Üí Sample(logits) ‚Üí VQ_Decode ‚Üí robot_actions<br><br>
          <strong>Key Innovation:</strong> Efficient cross-embodiment via adapter layers
        </div>

        <div class="code-block">
          <div class="code-header">üîß OpenVLA Cross-Embodiment Adapters</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>class CrossEmbodimentAdapter(nn.Module):
    """
    Lightweight adapter for different robot morphologies
    Enables one model to control many robot types
    """
    def __init__(self, hidden_dim=4096, robot_dof=7, adapter_rank=64):
        super().__init__()
        self.robot_dof = robot_dof
        
        # Low-rank adaptation for robot-specific control
        self.adapter_down = nn.Linear(hidden_dim, adapter_rank, bias=False)
        self.adapter_up = nn.Linear(adapter_rank, robot_dof * 2, bias=False)  # mean + std
        
        # Robot-specific normalization parameters
        self.action_mean = nn.Parameter(torch.zeros(robot_dof))
        self.action_std = nn.Parameter(torch.ones(robot_dof))
        
        # Initialize with small weights
        nn.init.normal_(self.adapter_down.weight, std=0.02)
        nn.init.zeros_(self.adapter_up.weight)
    
    def forward(self, hidden_states, robot_id=None):
        """
        Apply robot-specific adaptation to action predictions
        
        Args:
            hidden_states: [batch, seq, hidden_dim] transformer outputs
            robot_id: Optional robot identifier for multi-robot batches
        """
        # Low-rank adaptation
        adapter_output = self.adapter_up(self.adapter_down(hidden_states))
        action_params = adapter_output.view(-1, self.robot_dof, 2)  # [B*T, DOF, 2]
        
        raw_actions = action_params[..., 0]  # Mean predictions
        action_uncertainty = torch.softplus(action_params[..., 1])  # Std predictions
        
        # Robot-specific normalization
        normalized_actions = raw_actions * self.action_std + self.action_mean
        
        return normalized_actions, action_uncertainty

class MultiRobotVLA(nn.Module):
    """OpenVLA with support for multiple robot embodiments"""
    def __init__(self, base_model_path, robot_configs):
        super().__init__()
        
        # Base VLA model (shared across robots)
        self.base_vla = OpenVLA.from_pretrained(base_model_path)
        
        # Robot-specific adapters
        self.robot_adapters = nn.ModuleDict()
        for robot_name, config in robot_configs.items():
            self.robot_adapters[robot_name] = CrossEmbodimentAdapter(
                hidden_dim=self.base_vla.config.hidden_size,
                robot_dof=config['dof'],
                adapter_rank=config.get('adapter_rank', 64)
            )
        
        # Freeze base model, only train adapters
        for param in self.base_vla.parameters():
            param.requires_grad = False
    
    def forward(self, images, instructions, robot_type, action_history=None):
        # Get base model representations
        hidden_states = self.base_vla.forward_hidden(images, instructions, action_history)
        
        # Apply robot-specific adaptation
        if robot_type in self.robot_adapters:
            actions, uncertainty = self.robot_adapters[robot_type](hidden_states)
        else:
            raise ValueError(f"Unknown robot type: {robot_type}")
        
        return actions, uncertainty

# Example: Training adapters for multiple robots
def train_cross_embodiment_adapters():
    """
    Train lightweight adapters for different robot types
    Enables rapid deployment to new robot platforms
    """
    robot_configs = {
        'franka_panda': {'dof': 7, 'workspace': [0.3, 0.7, -0.3, 0.3, 0.0, 0.8]},
        'ur5': {'dof': 6, 'workspace': [0.2, 0.8, -0.4, 0.4, 0.0, 1.0]},
        'xarm7': {'dof': 7, 'workspace': [0.2, 0.7, -0.4, 0.4, 0.0, 0.9]},
        'mobile_manipulator': {'dof': 9, 'workspace': [0.0, 2.0, -1.0, 1.0, 0.0, 1.5]}  # 3 base + 6 arm
    }
    
    # Initialize multi-robot model
    model = MultiRobotVLA("openvla-7b", robot_configs)
    
    # Only adapter parameters are trainable (efficient!)
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")
    print("Adapter training is 100-1000x more efficient than full fine-tuning!")
    
    return model

# Usage example
multi_robot_model = train_cross_embodiment_adapters()</pre>
        </div>
      </div>

      <div id="training" class="tab-content">
        <div class="metric-grid">
          <div class="metric-card"><div class="metric-value">~1M</div><div class="metric-label">Robot Demonstrations</div></div>
          <div class="metric-card"><div class="metric-value">$100K</div><div class="metric-label">Training Cost Estimate</div></div>
          <div class="metric-card"><div class="metric-value">15</div><div class="metric-label">Robot Embodiments</div></div>
          <div class="metric-card"><div class="metric-value">7B</div><div class="metric-label">Parameters</div></div>
        </div>

        <div class="breakthrough-highlight">
          üéØ Training Innovation: Co-training on internet text + robot data creates emergent instruction-following capabilities
        </div>

        <div class="timeline">
          <div class="timeline-item">
            <div class="timeline-date">Phase 1</div>
            <div class="timeline-content">
              <strong>Base Model Pre-training</strong><br>
              LLama-7B foundation + vision encoder alignment on image-text pairs
            </div>
          </div>
          <div class="timeline-item">
            <div class="timeline-date">Phase 2</div>
            <div class="timeline-content">
              <strong>Robot Data Integration</strong><br>
              Joint training on Open X-Embodiment dataset with action tokenization
            </div>
          </div>
          <div class="timeline-item">
            <div class="timeline-date">Phase 3</div>
            <div class="timeline-content">
              <strong>Instruction Fine-tuning</strong><br>
              Task-conditioned training with natural language commands
            </div>
          </div>
          <div class="timeline-item">
            <div class="timeline-date">Phase 4</div>
            <div class="timeline-content">
              <strong>Cross-Embodiment Adapters</strong><br>
              Lightweight fine-tuning for specific robot platforms
            </div>
          </div>
        </div>
      </div>

      <div id="performance" class="tab-content">
        <div class="interactive-demo">
          <div class="demo-title">üìä Performance Comparison: OpenVLA vs RT-2-X</div>
          <div class="controls">
            <div class="control-group">
              <label>Evaluation Domain:</label>
              <select id="evalDomain">
                <option value="manipulation" selected>Object Manipulation</option>
                <option value="navigation">Mobile Navigation</option>
                <option value="assembly">Assembly Tasks</option>
                <option value="kitchen">Kitchen Tasks</option>
              </select>
            </div>
            <div class="control-group">
              <label>Metric Type:</label>
              <select id="metricType">
                <option value="success" selected>Success Rate</option>
                <option value="efficiency">Sample Efficiency</option>
                <option value="generalization">Generalization</option>
                <option value="cost">Training Cost</option>
              </select>
            </div>
          </div>
          <button onclick="compareVLAPerformance()" class="primary">üìà Compare Performance</button>
          <div id="performanceComparison"></div>
        </div>
      </div>

      <div id="deployment" class="tab-content">
        <div class="success">
          <strong>üöÄ Deployment Advantages of OpenVLA:</strong><br><br>
          <strong>‚Ä¢ Hardware Flexibility:</strong> Runs on single GPU (A100/H100) vs RT-2-X requiring multi-GPU clusters<br>
          <strong>‚Ä¢ Cost Efficiency:</strong> $50-100/month cloud costs vs $500-1000/month for equivalent RT-2-X inference<br>
          <strong>‚Ä¢ Customization:</strong> Full model weights enable domain-specific fine-tuning<br>
          <strong>‚Ä¢ Edge Deployment:</strong> Quantized versions run on Jetson Orin (15-30W power draw)<br>
          <strong>‚Ä¢ Community Support:</strong> Active development, bug fixes, and model improvements
        </div>

        <div class="robot-simulation">
          <div class="demo-title">ü§ñ Deployment Simulator</div>
          <div class="robot-icon">ü§ñ</div>
          <div class="controls">
            <div class="control-group">
              <label>Deployment Target:</label>
              <select id="deploymentType">
                <option value="cloud" selected>Cloud (A100)</option>
                <option value="edge">Edge (Jetson Orin)</option>
                <option value="mobile">Mobile (Jetson Nano)</option>
              </select>
            </div>
          </div>
          <button onclick="simulateDeployment()" class="primary">üöÄ Simulate Deployment</button>
          <div id="deploymentResults"></div>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üåê Section 5: Cross-Embodiment Learning - One Model, Many Robots</h2>

    <div class="step">
      <h3>üîÑ The Cross-Embodiment Challenge</h3>
      <p>Traditional robotics requires separate controllers for each robot type. VLA models enable <strong>cross-embodiment learning</strong>: training one foundation model that can control diverse robot morphologies through shared representations and adaptive interfaces.</p>

      <div class="vla-playground">
        <div class="demo-title">üéÆ Interactive Cross-Embodiment Playground</div>
        <p><strong>See how one VLA model controls different robot types:</strong></p>
        
        <div class="controls">
          <div class="control-group">
            <label>Robot Type:</label>
            <select id="robotType" onchange="updateRobotDisplay()">
              <option value="franka" selected>Franka Panda (7-DOF Arm)</option>
              <option value="ur5">Universal Robot UR5 (6-DOF)</option>
              <option value="mobile">Mobile Manipulator (9-DOF)</option>
              <option value="humanoid">Humanoid Robot (20+ DOF)</option>
              <option value="quadruped">Quadruped (12-DOF Legs)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Task Command:</label>
            <input type="text" id="taskCommand" placeholder="Pick up the red cube" style="width:100%">
          </div>
        </div>

        <div class="robot-workspace">
          <div class="robot-avatar" id="robotAvatar">ü§ñ</div>
          <div class="object" id="object1" style="top:60%; right:30%">üü•</div>
          <div class="object" id="object2" style="top:40%; right:60%">üü¢</div>
          <div class="object" id="object3" style="top:70%; right:70%">üì¶</div>
        </div>

        <div class="action-panel">
          <button onclick="executeVLATask()" class="primary">üéØ Execute VLA Task</button>
          <button onclick="resetPlayground()">üîÑ Reset Scene</button>
          <div id="executionLog"></div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>üß© Shared Representations Across Embodiments</h3>
      <div class="math-formula">
        <strong>Cross-Embodiment Learning Mathematics:</strong><br><br>
        <strong>1. Shared Visual-Language Embedding:</strong><br>
        œÜ<sub>shared</sub>(I, T) ‚Üí h<sub>common</sub> ‚àà ‚Ñù<sup>D</sup> &nbsp; (same for all robots)<br><br>
        <strong>2. Embodiment-Specific Action Mapping:</strong><br>
        h<sub>common</sub> ‚Üí A<sub>robot_i</sub>(h) ‚Üí a<sub>i</sub> ‚àà ‚Ñù<sup>d_i</sup> &nbsp; (robot-specific DOF)<br><br>
        <strong>3. Adapter Efficiency:</strong><br>
        |Œ∏<sub>adapter</sub>| ‚â™ |Œ∏<sub>shared</sub>| &nbsp; (1% of total parameters)<br><br>
        <strong>4. Transfer Learning:</strong><br>
        New robot: freeze Œ∏<sub>shared</sub>, train only Œ∏<sub>adapter</sub> &nbsp; (hours vs months)
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üîó Embodiment Transfer Visualizer</div>
        <div class="controls">
          <div class="control-group">
            <label>Source Robot (Trained):</label>
            <select id="sourceRobot">
              <option value="franka" selected>Franka Panda</option>
              <option value="ur5">UR5</option>
              <option value="mobile">Mobile Manipulator</option>
            </select>
          </div>
          <div class="control-group">
            <label>Target Robot (New):</label>
            <select id="targetRobot">
              <option value="xarm7" selected>xArm7</option>
              <option value="kuka">KUKA iiwa</option>
              <option value="humanoid">Tesla Optimus</option>
            </select>
          </div>
          <div class="control-group">
            <label>Transfer Method:</label>
            <select id="transferMethod">
              <option value="adapter" selected>Lightweight Adapter</option>
              <option value="finetune">Full Fine-tuning</option>
              <option value="zero_shot">Zero-shot Transfer</option>
            </select>
          </div>
        </div>
        <button onclick="visualizeTransfer()" class="primary">üîÑ Visualize Transfer</button>
        <div id="transferVisualization"></div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üéØ Section 6: Key Takeaways - The Future of Embodied AI</h2>

    <div class="step">
      <h3>üí° Core Insights from VLA Revolution</h3>
      <div class="model-comparison">
        <div class="model-card">
          <div class="model-name">üß© Action Tokenization</div>
          <div class="comparison-description">
            The breakthrough that enables language models to control robots by treating actions as discrete tokens
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">‚úÖ</span><span>Enables transformer architectures for robotics</span></div>
            <div class="capability"><span class="capability-icon">‚ö°</span><span>Vector Quantization and FAST methods work best</span></div>
            <div class="capability"><span class="capability-icon">üéØ</span><span>Balance between precision and token efficiency</span></div>
          </div>
        </div>

        <div class="model-card">
          <div class="model-name">üåê Cross-Embodiment Learning</div>
          <div class="comparison-description">
            One foundation model controlling diverse robot types through shared representations
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">üîÑ</span><span>Dramatically reduces per-robot development time</span></div>
            <div class="capability"><span class="capability-icon">üí∞</span><span>Makes robotics accessible to smaller teams</span></div>
            <div class="capability"><span class="capability-icon">üìà</span><span>Improves with scale and diversity of training data</span></div>
          </div>
        </div>

        <div class="model-card">
          <div class="model-name">üèÜ Open Source Success</div>
          <div class="comparison-description">
            OpenVLA-style models proving competitive with much larger proprietary systems
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">‚öñÔ∏è</span><span>7B parameters competing with 55B+ models</span></div>
            <div class="capability"><span class="capability-icon">üîß</span><span>Full customization and fine-tuning capabilities</span></div>
            <div class="capability"><span class="capability-icon">üåü</span><span>Community-driven improvements and datasets</span></div>
          </div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>üöÄ What's Next: The Path Forward</h3>
      <div class="timeline">
        <div class="timeline-item">
          <div class="timeline-date">2024-2025</div>
          <div class="timeline-content">
            <strong>VLA Foundation Era</strong><br>
            Open source models reach production quality. Cross-embodiment learning becomes standard.
          </div>
        </div>
        <div class="timeline-item">
          <div class="timeline-date">2025-2026</div>
          <div class="timeline-content">
            <strong>Multi-Modal Integration</strong><br>
            Audio, haptic, and proprioceptive sensing integrated into VLA models for richer interaction.
          </div>
        </div>
        <div class="timeline-item">
          <div class="timeline-date">2026-2027</div>
          <div class="timeline-content">
            <strong>Constitutional AI for Robots</strong><br>
            Safe, reliable robot behavior through principled self-correction and value alignment.
          </div>
        </div>
        <div class="timeline-item">
          <div class="timeline-date">2027+</div>
          <div class="timeline-content">
            <strong>General Robot Intelligence</strong><br>
            VLA models approaching human-level competence in real-world manipulation and interaction.
          </div>
        </div>
      </div>
    </div>

    <div class="breakthrough-highlight">
      üéØ Bottom Line: VLA models represent the ChatGPT moment for robotics ‚Äî transforming robots from task-specific machines into general-purpose, instruction-following agents.
    </div>

    <div class="success">
      <strong>üéì You've Mastered VLA Fundamentals!</strong><br><br>
      You now understand how Vision-Language-Action models work, from action tokenization mathematics to cross-embodiment learning. You've seen how open source approaches like OpenVLA can compete with much larger proprietary systems through smart architecture choices.<br><br>
      <strong>Ready for the next level?</strong> Explore <a href="training-deploying-vlas.html">Training & Deploying VLAs</a> to learn how to build and deploy these systems in production, or dive into <a href="advanced-vla-robotics.html">Advanced VLA & Future Robotics</a> for cutting-edge research directions.
    </div>
  </div>

  <script>
    // ---------------------------
    // Utility: copy code blocks
    // ---------------------------
    function copyCode(btn){
      try{
        const pre = btn.parentElement.querySelector('pre');
        const text = pre.innerText;
        navigator.clipboard.writeText(text);
        btn.textContent = '‚úî Copied';
        setTimeout(()=>btn.textContent='üìã Copy', 1500);
      }catch(e){
        console.error(e);
      }
    }

    // ---------------------------
    // Tabs for tokenization
    // ---------------------------
    function switchActionTab(id, el){
      document.querySelectorAll('.tab').forEach(t=>t.classList.remove('active'));
      document.querySelectorAll('.tab-content').forEach(p=>p.classList.remove('active'));
      el.classList.add('active');
      document.getElementById(id).classList.add('active');
    }

    // ---------------------------
    // Section 1: Evolution demo
    // ---------------------------
    function exploreEvolution(){
      const stage = document.getElementById('evolutionStage').value;
      const focus = document.getElementById('capabilityFocus').value;
      const box = document.getElementById('evolutionVisualization');
      const summaries = {
        language: 'Text-only models map tokens-to-tokens. Great at '+focus+'. No grounding.',
        'vision-language': 'Adds pixels. Grounded '+focus+' across images. Still digital-only.',
        action: 'Adds actions. '+focus+' spans pixels‚Üípolicy. Embodied control emerges.',
        embodied: 'Closed-loop perception‚Äìplanning‚Äìcontrol with real-world feedback.'
      };
      box.innerHTML = `
        <div class="success">
          <strong>Stage:</strong> ${stage.toUpperCase()} &nbsp; ‚Ä¢ &nbsp; <strong>Focus:</strong> ${focus}<br><br>
          ${summaries[stage]}
        </div>`;
    }

    // ---------------------------
    // Section 1: Approach compare
    // ---------------------------
    let selectedApproach = null;
    function selectApproach(type, el){
      document.querySelectorAll('.comparison-card').forEach(c=>c.classList.remove('selected'));
      el.classList.add('selected');
      selectedApproach = type;
      const box = document.getElementById('approachAnalysis');
      const explain = {
        traditional: {
          pros: ['Deterministic', 'Fast control loops'],
          cons: ['Brittle in OOD', 'High engineering cost', 'Poor generalization']
        },
        learning: {
          pros: ['Learns from data', 'Better adaptation'],
          cons: ['Task-specific', 'Sample hungry', 'Safety constraints']
        },
        vla: {
          pros: ['Generalist across tasks', 'Language-conditioned', 'Zero/low-shot'],
          cons: ['Token rate limits', 'Data curation needed']
        }
      };
      const e = explain[type];
      box.innerHTML = `
        <div class="info">
          <strong>Selected:</strong> ${type.toUpperCase()}<br><br>
          <b>Pros:</b> ${e.pros.join(', ')}<br>
          <b>Cons:</b> ${e.cons.join(', ')}
        </div>`;
    }

    // ---------------------------
    // Section 1: Traditional vs VLA comparison
    // ---------------------------
    function compareApproaches(){
      const task = document.getElementById('taskComplexity').value;
      const env = document.getElementById('environmentVar').value;
      const box = document.getElementById('comparisonResults');

      const baseline = {
        simple:   { traditional: 0.9, learning: 0.85, vla: 0.92 },
        medium:   { traditional: 0.6, learning: 0.75, vla: 0.85 },
        complex:  { traditional: 0.2, learning: 0.5,  vla: 0.75 }
      }[task];

      const envFactor = {
        controlled:   1.0,
        structured:   0.9,
        unstructured: 0.75
      }[env];

      const scores = Object.fromEntries(
        Object.entries(baseline).map(([k,v]) => [k, Math.max(0, Math.min(1, v * envFactor))])
      );

      const color = (s)=> s>=0.85 ? 'score-excellent' : s>=0.7 ? 'score-good' : s>=0.5 ? 'score-average' : 'score-poor';
      box.innerHTML = `
        <table class="benchmark-table">
          <thead><tr><th>Approach</th><th>Estimated Success</th></tr></thead>
          <tbody>
            <tr><td>Traditional</td><td class="${color(scores.traditional)}">${(scores.traditional*100).toFixed(0)}%</td></tr>
            <tr><td>Learning</td><td class="${color(scores.learning)}">${(scores.learning*100).toFixed(0)}%</td></tr>
            <tr><td>VLA</td><td class="${color(scores.vla)}">${(scores.vla*100).toFixed(0)}%</td></tr>
          </tbody>
        </table>
      `;
    }

    // ---------------------------
    // Section 2: Tokenizer demo
    // ---------------------------
    function generateActionTokens(){
      const action = document.getElementById('robotAction').value;
      const method = document.getElementById('tokenizationMethod').value;
      const res = document.getElementById('tokenResolution').value;
      const box = document.getElementById('actionTokenVisualization');

      const vocab = { low:64, medium:256, high:1024 }[res];
      // Simulate a 7-D action vector in [-1,1]
      const vec = Array.from({length:7}, (_,i)=>+(Math.sin((i+1)*1.1)*0.5 + 0.5* (action==='grasp'?0.2:0)).toFixed(3));
      let tokens = [];

      if(method==='binning'){
        tokens = vec.map(v => Math.min(vocab-1, Math.max(0, Math.floor(v * vocab))));
      }else if(method==='vq'){
        // Fake codebook assignment: round to nearest centroid index
        tokens = vec.map(v => Math.min(vocab-1, Math.max(0, Math.round(v * (vocab-1)))));
      }else if(method==='dct'){
        // Pretend DCT keeps K coeffs per dim ‚Üí flatten
        const K = Math.max(4, Math.floor(Math.log2(vocab)));
        tokens = Array.from({length:7*K}, (_,i)=> (i*13)%vocab);
      }else{ // flow
        tokens = ['(continuous trajectory ‚Äî no discrete tokens)'];
      }

      const pretty = Array.isArray(tokens) ? tokens.slice(0,20).join(' ') + (tokens.length>20?' ‚Ä¶':'') : tokens;
      box.innerHTML = `
        <div class="token-flow">
          <span class="token text special">[&lt;task&gt;] ${action}</span>
          <span class="token image">[IMG]</span>
          <span class="token action">[ACT]</span>
          <span class="token">${pretty}</span>
          <span class="token action">[/ACT]</span>
        </div>
        <div class="info"><b>Vocab:</b> ${vocab} ‚Ä¢ <b>Method:</b> ${method.toUpperCase()}</div>
      `;
    }

    // ---------------------------
    // Section 3: Model selection cards
    // ---------------------------
    function selectVLAModel(id, el){
      document.querySelectorAll('.model-card').forEach(c=>c.classList.remove('selected'));
      el.classList.add('selected');
      const info = {
        openvla: {
          summary: 'Open 7B-style model with vision fusion + action head. Strong manipulation at moderate cost.',
          params: '‚âà7B', pros:['Open weights','Adaptable','Lower cost'], cons:['Token rate bound','Needs curated demos']
        },
        rt2x: {
          summary: 'Large closed multimodal model. Strong results; limited external access.',
          params: '‚âà55B', pros:['Scale','Engineering'], cons:['Costly','Closed weights']
        }
      }[id];
      const box = document.getElementById('vlaModelAnalysis');
      box.innerHTML = `
        <div class="success">
          <b>Selected:</b> ${id.toUpperCase()} ‚Ä¢ <b>Params:</b> ${info.params}<br><br>
          ${info.summary}<br>
          <b>Pros:</b> ${info.pros.join(', ')}<br>
          <b>Cons:</b> ${info.cons.join(', ')}
        </div>
      `;
    }

    // ---------------------------
    // Section 3: Ecosystem explorer
    // ---------------------------
    function exploreVLAEcosystem(){
      const cat = document.getElementById('vlaCategory').value;
      const target = document.getElementById('deploymentTarget').value;
      const box = document.getElementById('vlaEcosystemResults');

      const db = [
        {name:'OpenVLA-style 7B', cat:'general', target:'cloud', notes:'Multimodal fusion; VQ/FAST action head'},
        {name:'SmolVLA-style 0.4‚Äì0.6B', cat:'efficient', target:'mobile', notes:'Async inference; laptop-friendly'},
        {name:'œÄ0-FAST (3B)', cat:'specialized', target:'edge', notes:'Frequency tokenization for high-rate control'},
        {name:'NVIDIA GR00T N1.5', cat:'general', target:'edge', notes:'Diffusion action head; Jetson optimized'},
        {name:'Research Proto: Flow-matching', cat:'research', target:'cloud', notes:'Continuous trajectory generation'}
      ];

      const filtered = db.filter(x => (x.cat===cat) && (x.target===target));
      if(!filtered.length){
        box.innerHTML = `<div class="warning"><b>No exact matches.</b> Try different filters.</div>`;
        return;
      }
      box.innerHTML = filtered.map(x=>`
        <div class="step">
          <b>${x.name}</b><br>
          <span class="comparison-description">${x.notes}</span>
        </div>
      `).join('');
    }

    // ---------------------------
    // Section 4: RT-2 Action Generation
    // ---------------------------
    function generateRT2Actions(){
      const task = document.getElementById('rt2Task').value;
      const precision = document.getElementById('rt2Precision').value;
      const box = document.getElementById('rt2ActionOutput');
      
      const decimals = {low:2, medium:3, high:4}[precision];
      
      // Simulate RT-2 style action sequences
      const taskActions = {
        pickup: [
          "move_to(0.15, 0.23, 0.45)",
          "rotate_gripper(-0.12, 0.34, 1.57)", 
          "open_gripper(0.08)",
          "move_to(0.15, 0.23, 0.35)",
          "close_gripper(0.02)",
          "lift_to(0.15, 0.23, 0.50)"
        ],
        place: [
          "move_to(0.45, -0.12, 0.40)",
          "rotate_gripper(0.00, 0.00, 0.78)",
          "move_to(0.45, -0.12, 0.32)",
          "open_gripper(0.08)",
          "move_to(0.45, -0.12, 0.45)",
          "close_gripper(0.00)"
        ],
        pour: [
          "grasp_container(0.25, 0.15, 0.38)",
          "lift_container(0.25, 0.15, 0.45)",
          "move_to(0.35, -0.20, 0.42)",
          "tilt_container(0.52, 0.15, 0.26)",
          "pour_action(2.5)",
          "untilt_container(0.00, 0.00, 0.00)"
        ],
        open: [
          "approach_handle(0.55, 0.08, 0.35)",
          "grasp_handle(0.03)",
          "pull_motion(-0.15, 0.00, 0.00)",
          "release_handle(0.08)",
          "retract_arm(0.45, 0.08, 0.40)"
        ]
      };

      const actions = taskActions[task] || taskActions.pickup;
      const actionTokens = actions.map(action => {
        // Add some noise to make it realistic
        return action.replace(/(-?\d+\.\d+)/g, (match) => {
          const val = parseFloat(match) + (Math.random()-0.5)*0.05;
          return val.toFixed(decimals);
        });
      });

      box.innerHTML = `
        <div class="token-flow">
          ${actionTokens.map(action => `<span class="token action">${action}</span>`).join('')}
        </div>
        <div class="info">
          <strong>RT-2 Approach:</strong> Actions as natural language strings<br>
          <strong>Vocabulary:</strong> ~32K text tokens (shared with language)<br>
          <strong>Precision:</strong> ${decimals} decimal places
        </div>
      `;
    }

    // ---------------------------
    // Section 4: Architecture tabs
    // ---------------------------
    function switchArchTab(id, el){
      document.querySelectorAll('.tab').forEach(t=>t.classList.remove('active'));
      document.querySelectorAll('.tab-content').forEach(p=>p.classList.remove('active'));
      el.classList.add('active');
      document.getElementById(id).classList.add('active');
    }

    // ---------------------------
    // Section 4: Performance comparison
    // ---------------------------
    function compareVLAPerformance(){
      const domain = document.getElementById('evalDomain').value;
      const metric = document.getElementById('metricType').value;
      const box = document.getElementById('performanceComparison');

      const data = {
        manipulation: { 
          success: {openvla: 78, rt2x: 82}, 
          efficiency: {openvla: 92, rt2x: 85}, 
          generalization: {openvla: 85, rt2x: 88},
          cost: {openvla: 95, rt2x: 30}
        },
        navigation: { 
          success: {openvla: 72, rt2x: 79}, 
          efficiency: {openvla: 88, rt2x: 81},
          generalization: {openvla: 83, rt2x: 86},
          cost: {openvla: 95, rt2x: 25}
        },
        assembly: { 
          success: {openvla: 69, rt2x: 75}, 
          efficiency: {openvla: 85, rt2x: 78},
          generalization: {openvla: 80, rt2x: 82},
          cost: {openvla: 95, rt2x: 20}
        },
        kitchen: { 
          success: {openvla: 74, rt2x: 80}, 
          efficiency: {openvla: 90, rt2x: 83},
          generalization: {openvla: 87, rt2x: 89},
          cost: {openvla: 95, rt2x: 35}
        }
      };

      const scores = data[domain][metric];
      const winner = scores.openvla > scores.rt2x ? 'OpenVLA' : 'RT-2-X';
      const gap = Math.abs(scores.openvla - scores.rt2x);

      box.innerHTML = `
        <table class="benchmark-table">
          <thead>
            <tr><th>Model</th><th>${metric.charAt(0).toUpperCase() + metric.slice(1)} Score</th><th>Notes</th></tr>
          </thead>
          <tbody>
            <tr>
              <td>OpenVLA (7B)</td>
              <td class="${scores.openvla >= 85 ? 'score-excellent' : scores.openvla >= 70 ? 'score-good' : 'score-average'}">${scores.openvla}%</td>
              <td>Open source, efficient training</td>
            </tr>
            <tr>
              <td>RT-2-X (55B)</td>
              <td class="${scores.rt2x >= 85 ? 'score-excellent' : scores.rt2x >= 70 ? 'score-good' : 'score-average'}">${scores.rt2x}%</td>
              <td>Proprietary, high resource cost</td>
            </tr>
          </tbody>
        </table>
        <div class="info">
          <strong>Domain:</strong> ${domain} ‚Ä¢ <strong>Winner:</strong> ${winner} by ${gap}%<br>
          ${metric === 'cost' ? 'Cost score = 100 - (relative training cost)' : 'Higher scores indicate better performance'}
        </div>
      `;
    }

    // ---------------------------
    // Section 4: Deployment simulation
    // ---------------------------
    function simulateDeployment(){
      const deployment = document.getElementById('deploymentType').value;
      const box = document.getElementById('deploymentResults');

      const specs = {
        cloud: {
          hardware: 'NVIDIA A100 (40GB)',
          power: '400W',
          memory: '40GB VRAM',
          inference: '10-15ms latency',
          cost: '$3.50/hour',
          throughput: '50-100 actions/sec'
        },
        edge: {
          hardware: 'Jetson AGX Orin (32GB)', 
          power: '30W',
          memory: '32GB unified',
          inference: '50-100ms latency',
          cost: '$2000 one-time',
          throughput: '10-20 actions/sec'
        },
        mobile: {
          hardware: 'Jetson Nano (4GB)',
          power: '10W', 
          memory: '4GB shared',
          inference: '200-500ms latency',
          cost: '$100 one-time',
          throughput: '2-5 actions/sec'
        }
      };

      const spec = specs[deployment];
      box.innerHTML = `
        <div class="metric-grid">
          <div class="metric-card"><div class="metric-value">${spec.power}</div><div class="metric-label">Power Draw</div></div>
          <div class="metric-card"><div class="metric-value">${spec.inference}</div><div class="metric-label">Inference Time</div></div>
          <div class="metric-card"><div class="metric-value">${spec.cost}</div><div class="metric-label">Cost</div></div>
          <div class="metric-card"><div class="metric-value">${spec.throughput}</div><div class="metric-label">Action Rate</div></div>
        </div>
        <div class="success">
          <strong>Hardware:</strong> ${spec.hardware}<br>
          <strong>Memory:</strong> ${spec.memory}<br>
          <strong>Best for:</strong> ${deployment === 'cloud' ? 'High-performance research, multi-robot orchestration' : 
                                   deployment === 'edge' ? 'Production robots, real-time control' : 
                                   'Consumer robots, battery-powered applications'}
        </div>
      `;
    }

    // ---------------------------
    // Section 5: Robot display updates
    // ---------------------------
    function updateRobotDisplay(){
      const robotType = document.getElementById('robotType').value;
      const avatar = document.getElementById('robotAvatar');
      
      const robotEmoji = {
        franka: 'ü¶æ', ur5: 'ü§ñ', mobile: 'üöõ', humanoid: 'ü§ñ‚Äç‚ôÇÔ∏è', quadruped: 'üêï‚Äçü¶Æ'
      };
      
      avatar.textContent = robotEmoji[robotType] || 'ü§ñ';
      avatar.style.fontSize = robotType === 'humanoid' ? '4em' : robotType === 'quadruped' ? '3em' : '3.5em';
    }

    function executeVLATask(){
      const robotType = document.getElementById('robotType').value;
      const command = document.getElementById('taskCommand').value || 'Pick up the red cube';
      const log = document.getElementById('executionLog');
      
      const robot = document.getElementById('robotAvatar');
      robot.style.transform = 'translateY(-50%) translateX(200px)';
      
      log.innerHTML = `
        <div class="action-sequence">
          <div class="action-step">üß† Processing:<br>"${command}"</div>
          <div class="action-step">üëÅÔ∏è Vision:<br>Detected objects</div>
          <div class="action-step">üéØ Planning:<br>Generated trajectory</div>
          <div class="action-step">ü§ñ Executing:<br>${robotType} motion</div>
        </div>
        <div class="success">
          <strong>‚úÖ Task completed!</strong><br>
          Cross-embodiment adapter enabled ${robotType} to execute instruction using shared VLA representations.
        </div>
      `;
      
      setTimeout(() => {
        robot.style.transform = 'translateY(-50%) translateX(0px)';
      }, 2000);
    }

    function resetPlayground(){
      document.getElementById('robotAvatar').style.transform = 'translateY(-50%) translateX(0px)';
      document.getElementById('executionLog').innerHTML = '';
      document.getElementById('taskCommand').value = '';
    }

    // ---------------------------
    // Section 5: Transfer learning visualization
    // ---------------------------
    function visualizeTransfer(){
      const source = document.getElementById('sourceRobot').value;
      const target = document.getElementById('targetRobot').value;
      const method = document.getElementById('transferMethod').value;
      const box = document.getElementById('transferVisualization');

      const transferData = {
        adapter: { time: '2-6 hours', data: '50-200 demos', params: '0.1-1% of model', success: '85-95%' },
        finetune: { time: '1-3 days', data: '1K-5K demos', params: '100% of model', success: '90-98%' },
        zero_shot: { time: '0 hours', data: '0 demos', params: '0% additional', success: '60-80%' }
      };

      const data = transferData[method];
      box.innerHTML = `
        <div class="architecture-flow">
          <div class="arch-component vision">
            <h4>üìö Source: ${source.toUpperCase()}</h4>
            <div>Trained Model</div>
          </div>
          <div class="arch-arrow">‚Üí</div>
          <div class="arch-component fusion">
            <h4>üîÑ ${method.toUpperCase()}</h4>
            <div>${data.time}</div>
          </div>
          <div class="arch-arrow">‚Üí</div>
          <div class="arch-component action">
            <h4>üéØ Target: ${target.toUpperCase()}</h4>
            <div>Deployed Model</div>
          </div>
        </div>
        <div class="metric-grid">
          <div class="metric-card"><div class="metric-value">${data.time}</div><div class="metric-label">Training Time</div></div>
          <div class="metric-card"><div class="metric-value">${data.data}</div><div class="metric-label">Data Required</div></div>
          <div class="metric-card"><div class="metric-value">${data.params}</div><div class="metric-label">Parameters Trained</div></div>
          <div class="metric-card"><div class="metric-value">${data.success}</div><div class="metric-label">Expected Performance</div></div>
        </div>
      `;
    }

    // Initialize new demos
    generateRT2Actions();
    compareVLAPerformance();
    simulateDeployment();
    updateRobotDisplay();
    visualizeTransfer();
    // Initialize some defaults so page isn‚Äôt empty
    exploreEvolution();
    compareApproaches();
    generateActionTokens();
  </script>
</body>
</html>
