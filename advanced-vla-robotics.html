<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Advanced VLA & Multi-Agent Robotics</title>
  <style>
    body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:1200px;margin:0 auto;padding:20px;background:linear-gradient(135deg,#1a1a1a 0%,#2d2d2d 100%);color:#e0e0e0;line-height:1.6}
    .container{background:#fff;color:#2d2d2d;border-radius:20px;padding:30px;margin:20px 0;border:1px solid #e0e0e0;box-shadow:0 4px 20px rgba(0,0,0,.1)}
    .nav-bar{background:#2d2d2d;color:#fff;padding:15px 30px;border-radius:15px;margin:20px 0;display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:15px}
    .nav-home,.nav-prev,.nav-next{background:#fff;color:#2d2d2d;padding:8px 16px;border-radius:6px;text-decoration:none;font-weight:bold;transition:all .3s}
    .nav-next{background:#28a745;color:#fff}
    .nav-home:hover,.nav-prev:hover,.nav-next:hover{transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    .nav-title{font-size:1.2em;font-weight:bold;flex:1;min-width:300px}
    .step{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;margin:15px 0;border-radius:15px;border-left:4px solid #2d2d2d}
    .interactive-demo{background:#f8f9fa;border:2px solid #e9ecef;border-radius:15px;padding:25px;margin:20px 0}
    .demo-title{font-size:1.3em;font-weight:bold;margin-bottom:20px;text-align:center;color:#2d2d2d}
    .controls{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .control-group{display:flex;flex-direction:column;gap:5px}
    .control-group label{font-weight:bold;font-size:14px;color:#2d2d2d}
    button{background:#2d2d2d;border:none;color:#fff;padding:12px 24px;border-radius:8px;cursor:pointer;font-weight:bold;transition:all .3s;margin:5px}
    button:hover{background:#1a1a1a;transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    button.primary{background:#28a745}
    button.primary:hover{background:#218838}
    button.danger{background:#dc3545}
    button.danger:hover{background:#c82333}
    button.secondary{background:#6c757d}
    button.secondary:hover{background:#5a6268}
    input,select,textarea{padding:8px 12px;border:1px solid #dadce0;border-radius:6px;background:#fff;color:#2d2d2d}
    textarea{min-height:70px;resize:vertical}
    .math-formula{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;text-align:center;font-size:16px;box-shadow:0 2px 8px rgba(0,0,0,.1)}
    .code-block{background:#2d2d2d;color:#e0e0e0;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;font-size:14px;overflow:auto;position:relative}
    .code-header{background:#1a1a1a;color:#28a745;padding:8px 15px;margin:-20px -20px 15px -20px;border-radius:8px 8px 0 0;font-weight:bold;font-size:12px}
    .copy-button{position:absolute;top:10px;right:10px;background:#28a745;color:#fff;border:none;padding:4px 8px;border-radius:4px;cursor:pointer;font-size:10px}
    .sensor-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .sensor-card{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;text-align:center;transition:all .3s;cursor:pointer}
    .sensor-card:hover{border-color:#28a745;transform:translateY(-3px);box-shadow:0 6px 20px rgba(0,0,0,.1)}
    .sensor-card.active{border-color:#28a745;background:#d4edda}
    .sensor-icon{font-size:2.5em;margin-bottom:10px}
    .sensor-title{font-size:1.1em;font-weight:bold;margin-bottom:8px;color:#2d2d2d}
    .sensor-description{font-size:12px;color:#666;margin-bottom:10px}
    .sensor-specs{background:#f8f9fa;padding:8px;border-radius:6px;font-size:11px;font-family:'Courier New',monospace}
    .fusion-architecture{background:#fff;border:2px solid #28a745;border-radius:15px;padding:25px;margin:20px 0}
    .fusion-layer{background:#f8f9fa;border:1px solid #e9ecef;border-radius:8px;padding:15px;margin:10px 0;position:relative}
    .layer-title{font-size:1.2em;font-weight:bold;color:#28a745;margin-bottom:10px}
    .layer-flow{display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:10px}
    .flow-component{background:#2d2d2d;color:#fff;padding:8px 12px;border-radius:6px;font-size:12px;text-align:center;min-width:80px}
    .flow-arrow{font-size:20px;color:#28a745;font-weight:bold}
    .constitutional-framework{background:linear-gradient(135deg,#e3f2fd,#f3e5f5);border:3px solid #2196f3;border-radius:15px;padding:25px;margin:20px 0}
    .constitutional-principle{background:#fff;border:1px solid #2196f3;border-radius:8px;padding:15px;margin:10px 0}
    .principle-title{font-size:1.1em;font-weight:bold;color:#1976d2;margin-bottom:8px}
    .principle-rule{background:#e3f2fd;padding:8px;border-radius:4px;font-size:12px;margin:5px 0;font-family:'Courier New',monospace}
    .multi-agent-arena{background:#fff;border:3px dashed #28a745;border-radius:15px;padding:20px;margin:20px 0;min-height:400px;position:relative;overflow:hidden}
    .robot-agent{position:absolute;width:40px;height:40px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:20px;font-weight:bold;color:#fff;cursor:pointer;transition:all .3s;box-shadow:0 2px 8px rgba(0,0,0,.2)}
    .robot-agent:hover{transform:scale(1.1);z-index:10}
    .robot-leader{background:#dc3545}
    .robot-worker{background:#28a745}
    .robot-scout{background:#007bff}
    .robot-specialist{background:#6f42c1}
    .robot-coordinator{background:#fd7e14}
    .communication-line{position:absolute;height:2px;background:#28a745;opacity:0.6;transform-origin:left center;transition:all .3s}
    .communication-line.active{opacity:1;animation:pulse 1s infinite}
    .agent-status{position:absolute;bottom:10px;left:10px;right:10px;background:rgba(255,255,255,0.9);padding:10px;border-radius:6px;font-size:12px;max-height:80px;overflow-y:auto}
    .coordination-panel{background:#2d2d2d;color:#fff;border-radius:10px;padding:20px;margin:15px 0}
    .coord-title{font-size:1.2em;font-weight:bold;margin-bottom:15px;color:#28a745}
    .coord-controls{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px}
    .coord-button{background:#28a745;color:#fff;border:none;padding:10px 15px;border-radius:6px;cursor:pointer;font-weight:bold;transition:all .3s}
    .coord-button:hover{background:#218838;transform:translateY(-2px)}
    .world-model-viewer{background:linear-gradient(135deg,#1a1a1a,#2d2d2d);border:2px solid #28a745;border-radius:15px;padding:20px;margin:20px 0;color:#fff;min-height:300px}
    .world-canvas{background:#000;border:1px solid #28a745;border-radius:8px;width:100%;height:250px;display:flex;align-items:center;justify-content:center;margin:15px 0;position:relative;overflow:hidden}
    .physics-object{position:absolute;border-radius:4px;transition:all .1s ease;cursor:pointer}
    .physics-cube{background:#28a745;width:20px;height:20px}
    .physics-ball{background:#007bff;width:20px;height:20px;border-radius:50%}
    .physics-platform{background:#6c757d;height:4px}
    .world-controls{display:flex;justify-content:center;gap:10px;flex-wrap:wrap;margin:10px 0}
    .prediction-display{background:rgba(40,167,69,0.1);border:1px solid #28a745;border-radius:6px;padding:10px;margin:10px 0;font-size:12px;font-family:'Courier New',monospace}
    .tabs{display:flex;margin-bottom:20px;border-bottom:2px solid #e9ecef;flex-wrap:wrap}
    .tab{padding:12px 20px;cursor:pointer;border-bottom:3px solid transparent;font-weight:bold;transition:all .3s;margin-bottom:-2px}
    .tab:hover{background:#f8f9fa}
    .tab.active{border-bottom-color:#28a745;background:#d4edda;color:#155724}
    .tab-content{display:none}
    .tab-content.active{display:block}
    .info{background:#d1ecf1;border-left:4px solid #17a2b8;color:#0c5460;padding:15px;border-radius:8px;margin:15px 0}
    .success{background:#d4edda;border-left:4px solid #28a745;color:#155724;padding:15px;border-radius:8px;margin:15px 0}
    .warning{background:#fff3cd;border-left:4px solid #ffc107;color:#856404;padding:15px;border-radius:8px;margin:15px 0}
    .danger{background:#f8d7da;border-left:4px solid #dc3545;color:#721c24;padding:15px;border-radius:8px;margin:15px 0}
    .breakthrough-highlight{background:linear-gradient(135deg,#28a745,#20c997);color:#fff;padding:20px;border-radius:12px;text-align:center;margin:20px 0;font-size:18px;font-weight:bold;box-shadow:0 4px 15px rgba(40,167,69,.3)}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(150px,1fr));gap:15px;margin:20px 0}
    .metric-card{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center}
    .metric-value{font-size:1.5em;font-weight:bold;color:#2d2d2d;margin-bottom:5px}
    .metric-label{font-size:12px;color:#666}
    .progress-bar{background:#e9ecef;border-radius:10px;height:8px;margin:10px 0;overflow:hidden}
    .progress-fill{background:#28a745;height:100%;transition:width 1s ease}
    .simulation-output{background:#000;color:#00ff00;padding:15px;border-radius:8px;margin:10px 0;font-family:'Courier New',monospace;font-size:12px;min-height:100px;overflow-y:auto;max-height:300px}
    .sim-line{margin:2px 0}
    .sim-timestamp{color:#ffff00}
    .sim-robot{color:#00ffff}
    .sim-action{color:#ff6600}
    .sim-success{color:#00ff00}
    .sim-error{color:#ff0000}
    .emergency-panel{background:#dc3545;color:#fff;border-radius:10px;padding:15px;margin:15px 0;text-align:center;display:none}
    .emergency-panel.active{display:block;animation:blink 1s infinite}
    @keyframes blink{0%,50%{opacity:1}51%,100%{opacity:.7}}
    @keyframes pulse{0%{opacity:.6}50%{opacity:1}100%{opacity:.6}}
    .role-assignment{background:#f8f9fa;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0}
    .role-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:15px;margin:15px 0}
    .role-card{background:#fff;border:2px solid #dee2e6;border-radius:8px;padding:15px;transition:all .3s;cursor:pointer}
    .role-card:hover{border-color:#28a745;transform:translateY(-2px)}
    .role-card.selected{border-color:#28a745;background:#d4edda}
    .role-title{font-size:1.1em;font-weight:bold;margin-bottom:8px}
    .role-description{font-size:12px;color:#666;margin-bottom:10px}
    .role-capabilities{background:#f8f9fa;padding:8px;border-radius:4px;font-size:11px;font-family:'Courier New',monospace}
    .consensus-mechanism{background:linear-gradient(135deg,#17a2b8,#20c997);color:#fff;border-radius:12px;padding:20px;margin:15px 0}
    .consensus-title{font-size:1.3em;font-weight:bold;margin-bottom:15px;text-align:center}
    .consensus-steps{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px}
    .consensus-step{background:rgba(255,255,255,0.1);padding:12px;border-radius:8px;text-align:center}
    .step-number{background:rgba(255,255,255,0.2);width:25px;height:25px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;margin:0 auto 8px auto}
    .step-title{font-weight:bold;margin-bottom:5px}
    .step-desc{font-size:11px;opacity:.9}
    .safety-monitor{background:#fff3cd;border:2px solid #ffc107;border-radius:10px;padding:15px;margin:15px 0}
    .safety-title{font-size:1.2em;font-weight:bold;color:#856404;margin-bottom:10px}
    .safety-metrics{display:grid;grid-template-columns:repeat(auto-fit,minmax(120px,1fr));gap:10px}
    .safety-metric{text-align:center;padding:8px;background:#fff;border-radius:4px;border:1px solid #ffeaa7}
    .safety-value{font-size:1.1em;font-weight:bold;color:#856404}
    .safety-label{font-size:10px;color:#6c541e}
    .agent-communication{background:#e3f2fd;border:2px solid #2196f3;border-radius:10px;padding:15px;margin:15px 0;max-height:200px;overflow-y:auto}
    .comm-message{background:#fff;border:1px solid #bbdefb;border-radius:6px;padding:8px;margin:5px 0;font-size:12px}
    .comm-sender{font-weight:bold;color:#1976d2}
    .comm-content{margin:3px 0;color:#333}
    .comm-timestamp{font-size:10px;color:#666;text-align:right}
  </style>
</head>
<body>
  <div class="nav-bar">
    <div class="nav-title">🔬 Advanced VLA & Multi-Agent Robotics</div>
    <a href="index.html" class="nav-home">🏠 Home</a>
    <a href="deploying-vlas.html" class="nav-prev">← Deploying VLAs</a>
    <a href="path-to-agi.html" class="nav-next">Next: Path to AGI →</a>
  </div>

  <div class="container">
    <h1>🔬 Advanced VLA & Multi-Agent Robotics: Next-Generation Embodied Intelligence</h1>
    <p>Explore cutting-edge techniques in Vision-Language-Action models and multi-agent robotics systems. This tutorial covers advanced multi-modal sensor fusion, constitutional AI for robot safety, coordinated multi-agent systems, and world model integration - all designed for practical near-term deployment.</p>
    
    <div class="breakthrough-highlight">
      🎯 The Mission: Build intelligent multi-agent robotic systems that safely coordinate complex tasks through natural language and advanced AI reasoning
    </div>
  </div>

  <div class="container">
    <h2>🌐 Section 1: Multi-Modal Extensions Beyond Vision</h2>
    
    <div class="step">
      <h3>🎵 The Multi-Sensory Robot Revolution</h3>
      <p>While vision-language-action models have revolutionized robotics, the next breakthrough comes from integrating multiple sensory modalities. Modern robots need to hear, feel, and sense their environment just like humans do.</p>

      <div class="sensor-grid">
        <div class="sensor-card active" onclick="selectSensor('vision', this)">
          <div class="sensor-icon">👁️</div>
          <div class="sensor-title">Vision (RGB-D)</div>
          <div class="sensor-description">Primary visual perception with depth information</div>
          <div class="sensor-specs">Resolution: 1920x1080<br>FPS: 30<br>Depth: 0.1-10m</div>
        </div>

        <div class="sensor-card" onclick="selectSensor('audio', this)">
          <div class="sensor-icon">🎵</div>
          <div class="sensor-title">Audio</div>
          <div class="sensor-description">Sound localization and speech recognition</div>
          <div class="sensor-specs">Sample Rate: 48kHz<br>Channels: 4 (array)<br>Range: 20Hz-20kHz</div>
        </div>

        <div class="sensor-card" onclick="selectSensor('haptic', this)">
          <div class="sensor-icon">✋</div>
          <div class="sensor-title">Haptic/Force</div>
          <div class="sensor-description">Touch, pressure, and force feedback</div>
          <div class="sensor-specs">Force: ±300N<br>Torque: ±50Nm<br>Frequency: 1kHz</div>
        </div>

        <div class="sensor-card" onclick="selectSensor('proprioception', this)">
          <div class="sensor-icon">🤖</div>
          <div class="sensor-title">Proprioception</div>
          <div class="sensor-description">Joint angles, velocities, and internal state</div>
          <div class="sensor-specs">Joints: 7 DOF<br>Resolution: 0.1°<br>Rate: 1kHz</div>
        </div>

        <div class="sensor-card" onclick="selectSensor('lidar', this)">
          <div class="sensor-icon">📡</div>
          <div class="sensor-title">LiDAR</div>
          <div class="sensor-description">Precise 3D spatial mapping and obstacle detection</div>
          <div class="sensor-specs">Range: 100m<br>Points: 1.3M/sec<br>Accuracy: ±2cm</div>
        </div>

        <div class="sensor-card" onclick="selectSensor('imu', this)">
          <div class="sensor-icon">🧭</div>
          <div class="sensor-title">IMU</div>
          <div class="sensor-description">Inertial measurement for motion and orientation</div>
          <div class="sensor-specs">Accel: ±16g<br>Gyro: ±2000°/s<br>Rate: 1kHz</div>
        </div>
      </div>

      <div id="sensorDetails"></div>
    </div>

    <div class="step">
      <h3>🔀 Multi-Modal Fusion Architecture</h3>
      <div class="fusion-architecture">
        <div class="fusion-layer">
          <div class="layer-title">🎯 Attention-Based Sensor Fusion</div>
          <div class="layer-flow">
            <div class="flow-component">Vision<br>Tokens</div>
            <div class="flow-arrow">→</div>
            <div class="flow-component">Audio<br>Tokens</div>
            <div class="flow-arrow">→</div>
            <div class="flow-component">Haptic<br>Tokens</div>
            <div class="flow-arrow">→</div>
            <div class="flow-component">Cross-Modal<br>Attention</div>
            <div class="flow-arrow">→</div>
            <div class="flow-component">Fused<br>Representation</div>
          </div>
        </div>

        <div class="fusion-layer">
          <div class="layer-title">⚡ Temporal Fusion Pipeline</div>
          <div class="layer-flow">
            <div class="flow-component">T-3</div>
            <div class="flow-arrow">→</div>
            <div class="flow-component">T-2</div>
            <div class="flow-arrow">→</div>
            <div class="flow-component">T-1</div>
            <div class="flow-arrow">→</div>
            <div class="flow-component">T-0</div>
            <div class="flow-arrow">→</div>
            <div class="flow-component">Action<br>Prediction</div>
          </div>
        </div>
      </div>

      <div class="math-formula">
        <strong>Multi-Modal Cross-Attention Mechanism:</strong><br><br>
        <strong>Sensor Token Embedding:</strong><br>
        h<sub>vision</sub> = Linear(V) ∈ ℝ<sup>T×d</sup>, h<sub>audio</sub> = Linear(A) ∈ ℝ<sup>T×d</sup>, h<sub>haptic</sub> = Linear(H) ∈ ℝ<sup>T×d</sup><br><br>
        <strong>Cross-Modal Attention:</strong><br>
        Attention(Q, K, V) = softmax(QK<sup>T</sup>/√d)V<br>
        CrossAttn<sub>v→a</sub> = Attention(h<sub>vision</sub>, h<sub>audio</sub>, h<sub>audio</sub>)<br><br>
        <strong>Multi-Head Fusion:</strong><br>
        MultiHead(Q,K,V) = Concat(head<sub>1</sub>,...,head<sub>h</sub>)W<sup>O</sup><br>
        where head<sub>i</sub> = Attention(QW<sub>i</sub><sup>Q</sup>, KW<sub>i</sub><sup>K</sup>, VW<sub>i</sub><sup>V</sup>)<br><br>
        <strong>Final Representation:</strong><br>
        h<sub>fused</sub> = LayerNorm(h<sub>vision</sub> + h<sub>audio</sub> + h<sub>haptic</sub> + h<sub>proprio</sub>)
      </div>

      <div class="interactive-demo">
        <div class="demo-title">🎛️ Multi-Modal VLA Designer</div>
        <p><strong>Design your custom multi-modal VLA architecture:</strong></p>

        <div class="controls">
          <div class="control-group">
            <label>Vision Encoder:</label>
            <select id="visionEncoder">
              <option value="vit" selected>Vision Transformer (ViT)</option>
              <option value="resnet">ResNet-50 CNN</option>
              <option value="clip">CLIP Visual Encoder</option>
              <option value="dinov2">DINOv2 Self-Supervised</option>
            </select>
          </div>

          <div class="control-group">
            <label>Audio Processing:</label>
            <select id="audioProcessor">
              <option value="whisper" selected>Whisper ASR + Features</option>
              <option value="wav2vec">Wav2Vec2 Representations</option>
              <option value="spectrogram">Mel-Spectrogram CNN</option>
              <option value="transformer">Audio Transformer</option>
            </select>
          </div>

          <div class="control-group">
            <label>Haptic Integration:</label>
            <select id="hapticIntegration">
              <option value="mlp" selected>Multi-Layer Perceptron</option>
              <option value="lstm">LSTM Temporal Modeling</option>
              <option value="transformer">Transformer Encoder</option>
              <option value="graph">Graph Neural Network</option>
            </select>
          </div>

          <div class="control-group">
            <label>Fusion Strategy:</label>
            <select id="fusionStrategy">
              <option value="cross_attention" selected>Cross-Attention</option>
              <option value="early_fusion">Early Fusion (Concat)</option>
              <option value="late_fusion">Late Fusion (Weighted)</option>
              <option value="hierarchical">Hierarchical Fusion</option>
            </select>
          </div>

          <div class="control-group">
            <label>Model Size:</label>
            <select id="modelSize">
              <option value="small">Small (1.5B params)</option>
              <option value="medium" selected>Medium (7B params)</option>
              <option value="large">Large (13B params)</option>
              <option value="xlarge">Extra Large (30B params)</option>
            </select>
          </div>

          <div class="control-group">
            <label>Target Robot:</label>
            <select id="targetRobot">
              <option value="franka" selected>Franka Panda (7-DOF arm)</option>
              <option value="ur5">Universal Robot UR5</option>
              <option value="aloha">ALOHA (Bimanual)</option>
              <option value="mobile">Mobile Manipulator</option>
              <option value="humanoid">Humanoid Robot</option>
            </select>
          </div>
        </div>

        <button onclick="designMultiModalVLA()" class="primary">🔬 Design Architecture</button>
        <div id="architectureDesign"></div>
      </div>
    </div>

    <div class="step">
      <h3>💻 Complete Multi-Modal VLA Implementation</h3>
      <div class="code-block">
        <div class="code-header">🔬 Multi-Modal VLA Architecture Implementation</div>
        <button class="copy-button" onclick="copyCode(this)">📋 Copy</button>
<pre>import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchaudio
import numpy as np
from typing import Dict, List, Optional, Tuple

class MultiModalVLAModel(nn.Module):
    """
    Advanced Multi-Modal Vision-Language-Action Model
    Integrates vision, audio, haptic, and proprioceptive inputs
    for comprehensive robot control
    """
    
    def __init__(
        self, 
        vision_encoder_type='vit',
        audio_encoder_type='whisper',
        haptic_encoder_type='mlp',
        fusion_strategy='cross_attention',
        hidden_dim=768,
        num_layers=12,
        num_heads=12,
        action_dim=7,
        max_seq_length=512
    ):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.max_seq_length = max_seq_length
        self.action_dim = action_dim
        
        # Vision Encoder
        self.vision_encoder = self._build_vision_encoder(vision_encoder_type)
        
        # Audio Encoder  
        self.audio_encoder = self._build_audio_encoder(audio_encoder_type)
        
        # Haptic/Force Encoder
        self.haptic_encoder = self._build_haptic_encoder(haptic_encoder_type)
        
        # Proprioception Encoder (joint states, velocities)
        self.proprio_encoder = nn.Sequential(
            nn.Linear(action_dim * 2, hidden_dim),  # positions + velocities
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        # Language Encoder (for instructions)
        self.language_encoder = nn.Embedding(50000, hidden_dim)  # Vocabulary size
        
        # Positional Encodings for different modalities
        self.vision_pos_encoding = nn.Parameter(torch.randn(196, hidden_dim))  # 14x14 patches
        self.audio_pos_encoding = nn.Parameter(torch.randn(100, hidden_dim))   # Audio frames
        self.temporal_pos_encoding = nn.Parameter(torch.randn(max_seq_length, hidden_dim))
        
        # Cross-Modal Fusion
        if fusion_strategy == 'cross_attention':
            self.fusion_layers = nn.ModuleList([
                CrossModalAttentionLayer(hidden_dim, num_heads)
                for _ in range(num_layers)
            ])
        elif fusion_strategy == 'hierarchical':
            self.fusion_layers = nn.ModuleList([
                HierarchicalFusionLayer(hidden_dim, num_heads)
                for _ in range(num_layers)
            ])
        
        # Action Prediction Head
        self.action_head = ActionPredictionHead(hidden_dim, action_dim)
        
        # Safety and Uncertainty Estimation
        self.uncertainty_head = nn.Linear(hidden_dim, action_dim)
        self.safety_classifier = nn.Linear(hidden_dim, 2)  # safe/unsafe
        
    def _build_vision_encoder(self, encoder_type):
        """Build vision encoder based on specified type"""
        if encoder_type == 'vit':
            # Vision Transformer encoder
            return VisionTransformerEncoder(
                image_size=224,
                patch_size=16, 
                num_layers=12,
                hidden_dim=self.hidden_dim,
                num_heads=self.num_heads
            )
        elif encoder_type == 'clip':
            # CLIP visual encoder
            import clip
            model, _ = clip.load("ViT-B/32", device="cpu")
            return model.visual
        else:
            raise ValueError(f"Unknown vision encoder: {encoder_type}")
    
    def _build_audio_encoder(self, encoder_type):
        """Build audio encoder for sound processing"""
        if encoder_type == 'whisper':
            # Whisper-style audio encoder
            return WhisperAudioEncoder(
                n_mels=80,
                hidden_dim=self.hidden_dim,
                num_layers=6
            )
        elif encoder_type == 'wav2vec':
            # Wav2Vec2-style encoder
            return Wav2Vec2Encoder(hidden_dim=self.hidden_dim)
        else:
            return SimpleAudioEncoder(hidden_dim=self.hidden_dim)
    
    def _build_haptic_encoder(self, encoder_type):
        """Build haptic/force encoder"""
        if encoder_type == 'mlp':
            return nn.Sequential(
                nn.Linear(6, self.hidden_dim // 2),  # 3D force + 3D torque
                nn.ReLU(),
                nn.Linear(self.hidden_dim // 2, self.hidden_dim),
                nn.LayerNorm(self.hidden_dim)
            )
        elif encoder_type == 'lstm':
            return nn.LSTM(6, self.hidden_dim, batch_first=True)
        else:
            return nn.Linear(6, self.hidden_dim)
    
    def forward(
        self,
        images: torch.Tensor,           # [B, T, C, H, W]
        audio: torch.Tensor,            # [B, T, audio_features] 
        haptic: torch.Tensor,           # [B, T, 6] (force + torque)
        proprioception: torch.Tensor,   # [B, T, action_dim*2]
        language_tokens: torch.Tensor,  # [B, seq_len]
        attention_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        
        batch_size, seq_len = images.shape[:2]
        
        # Encode each modality
        vision_features = self._encode_vision(images)      # [B, T, N_patches, hidden_dim]
        audio_features = self._encode_audio(audio)         # [B, T, N_audio, hidden_dim]  
        haptic_features = self._encode_haptic(haptic)      # [B, T, hidden_dim]
        proprio_features = self._encode_proprioception(proprioception)  # [B, T, hidden_dim]
        language_features = self._encode_language(language_tokens)      # [B, seq_len, hidden_dim]
        
        # Add temporal positional encodings
        vision_features = vision_features + self.temporal_pos_encoding[:seq_len, None, :]
        audio_features = audio_features + self.temporal_pos_encoding[:seq_len, None, :]
        haptic_features = haptic_features + self.temporal_pos_encoding[:seq_len, :]
        proprio_features = proprio_features + self.temporal_pos_encoding[:seq_len, :]
        
        # Reshape for transformer processing
        # Flatten spatial/temporal dimensions
        B, T = batch_size, seq_len
        vision_flat = vision_features.reshape(B, T * vision_features.shape[2], -1)
        audio_flat = audio_features.reshape(B, T * audio_features.shape[2], -1)
        
        # Concatenate all modalities
        multimodal_features = torch.cat([
            vision_flat,
            audio_flat, 
            haptic_features,
            proprio_features,
            language_features
        ], dim=1)  # [B, total_seq_len, hidden_dim]
        
        # Apply cross-modal fusion layers
        fused_features = multimodal_features
        for layer in self.fusion_layers:
            fused_features = layer(fused_features, attention_mask)
        
        # Extract robot-relevant features (last proprioception tokens)
        robot_features = fused_features[:, -seq_len:, :]  # [B, T, hidden_dim]
        
        # Predict actions
        actions = self.action_head(robot_features)  # [B, T, action_dim]
        
        # Estimate uncertainty and safety
        uncertainty = torch.nn.functional.softplus(self.uncertainty_head(robot_features))
        safety_logits = self.safety_classifier(robot_features.mean(dim=1))  # [B, 2]
        
        return {
            'actions': actions,
            'uncertainty': uncertainty,
            'safety_logits': safety_logits,
            'fused_features': fused_features
        }
    
    def _encode_vision(self, images):
        """Encode visual input"""
        B, T, C, H, W = images.shape
        images_flat = images.view(B * T, C, H, W)
        
        # Process through vision encoder
        vision_features = self.vision_encoder(images_flat)  # [B*T, N_patches, hidden_dim]
        
        # Add positional encoding
        vision_features = vision_features + self.vision_pos_encoding[:vision_features.shape[1]]
        
        # Reshape back to [B, T, N_patches, hidden_dim]
        N_patches = vision_features.shape[1]
        return vision_features.view(B, T, N_patches, self.hidden_dim)
    
    def _encode_audio(self, audio):
        """Encode audio input"""
        B, T, audio_dim = audio.shape
        audio_flat = audio.view(B * T, -1)
        
        # Process through audio encoder
        audio_features = self.audio_encoder(audio_flat)  # [B*T, N_audio, hidden_dim]
        
        # Add positional encoding
        if len(audio_features.shape) == 3:
            N_audio = audio_features.shape[1]
            audio_features = audio_features + self.audio_pos_encoding[:N_audio]
            return audio_features.view(B, T, N_audio, self.hidden_dim)
        else:
            # If audio encoder returns [B*T, hidden_dim]
            return audio_features.view(B, T, 1, self.hidden_dim)
    
    def _encode_haptic(self, haptic):
        """Encode haptic/force input"""
        B, T, haptic_dim = haptic.shape
        haptic_flat = haptic.view(B * T, haptic_dim)
        
        if isinstance(self.haptic_encoder, nn.LSTM):
            haptic_features, _ = self.haptic_encoder(haptic)  # [B, T, hidden_dim]
        else:
            haptic_features = self.haptic_encoder(haptic_flat)  # [B*T, hidden_dim]
            haptic_features = haptic_features.view(B, T, self.hidden_dim)
        
        return haptic_features
    
    def _encode_proprioception(self, proprioception):
        """Encode proprioceptive input (joint states)"""
        return self.proprio_encoder(proprioception)  # [B, T, hidden_dim]
    
    def _encode_language(self, language_tokens):
        """Encode language instructions"""
        return self.language_encoder(language_tokens)  # [B, seq_len, hidden_dim]

class CrossModalAttentionLayer(nn.Module):
    """Cross-modal attention layer for multi-modal fusion"""
    
    def __init__(self, hidden_dim, num_heads):
        super().__init__()
        self.multihead_attn = nn.MultiheadAttention(
            hidden_dim, 
            num_heads, 
            batch_first=True
        )
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        
    def forward(self, x, attention_mask=None):
        # Self-attention
        attn_output, _ = self.multihead_attn(x, x, x, key_padding_mask=attention_mask)
        x = self.norm1(x + attn_output)
        
        # Feed-forward
        ffn_output = self.ffn(x)
        x = self.norm2(x + ffn_output)
        
        return x

class ActionPredictionHead(nn.Module):
    """Action prediction head with uncertainty estimation"""
    
    def __init__(self, hidden_dim, action_dim):
        super().__init__()
        self.action_projector = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
    def forward(self, x):
        return self.action_projector(x)

# Simplified encoder implementations
class VisionTransformerEncoder(nn.Module):
    def __init__(self, image_size, patch_size, num_layers, hidden_dim, num_heads):
        super().__init__()
        self.patch_embed = nn.Conv2d(3, hidden_dim, patch_size, patch_size)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(hidden_dim, num_heads, batch_first=True),
            num_layers
        )
        
    def forward(self, x):
        # Convert to patches
        x = self.patch_embed(x)  # [B, hidden_dim, H//patch_size, W//patch_size]
        B, D, H, W = x.shape
        x = x.reshape(B, D, H * W).transpose(1, 2)  # [B, N_patches, hidden_dim]
        
        return self.transformer(x)

class WhisperAudioEncoder(nn.Module):
    def __init__(self, n_mels, hidden_dim, num_layers):
        super().__init__()
        self.conv1 = nn.Conv1d(n_mels, hidden_dim, 3, padding=1)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(hidden_dim, 8, batch_first=True),
            num_layers
        )
        
    def forward(self, x):
        if len(x.shape) == 2:  # [B, features] -> add time dimension
            x = x.unsqueeze(1)  # [B, 1, features]
            return self.conv1(x.transpose(1, 2)).transpose(1, 2)
        return self.transformer(x)

class SimpleAudioEncoder(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.encoder = nn.Linear(1024, hidden_dim)  # Assume 1024 audio features
        
    def forward(self, x):
        return self.encoder(x)

class Wav2Vec2Encoder(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.encoder = nn.Linear(768, hidden_dim)  # Wav2Vec2 hidden size
        
    def forward(self, x):
        return self.encoder(x)

# Example usage and training
def create_multimodal_vla(config):
    """Create a multi-modal VLA model with specified configuration"""
    model = MultiModalVLAModel(
        vision_encoder_type=config.get('vision_encoder', 'vit'),
        audio_encoder_type=config.get('audio_encoder', 'whisper'),
        haptic_encoder_type=config.get('haptic_encoder', 'mlp'),
        fusion_strategy=config.get('fusion_strategy', 'cross_attention'),
        hidden_dim=config.get('hidden_dim', 768),
        num_layers=config.get('num_layers', 12),
        action_dim=config.get('action_dim', 7)
    )
    
    return model

def train_multimodal_vla(model, dataloader, num_epochs=10):
    """Training loop for multi-modal VLA"""
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    action_criterion = nn.MSELoss()
    safety_criterion = nn.CrossEntropyLoss()
    
    model.train()
    
    for epoch in range(num_epochs):
        epoch_loss = 0
        
        for batch in dataloader:
            optimizer.zero_grad()
            
            # Forward pass
            outputs = model(
                images=batch['images'],
                audio=batch['audio'],
                haptic=batch['haptic'],
                proprioception=batch['proprioception'],
                language_tokens=batch['language_tokens']
            )
            
            # Compute losses
            action_loss = action_criterion(outputs['actions'], batch['target_actions'])
            safety_loss = safety_criterion(outputs['safety_logits'], batch['safety_labels'])
            
            # Uncertainty regularization (encourage calibrated uncertainty)
            uncertainty_loss = torch.mean(outputs['uncertainty'])
            
            total_loss = action_loss + 0.1 * safety_loss + 0.01 * uncertainty_loss
            
            # Backward pass
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_loss += total_loss.item()
        
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.4f}")
    
    return model

# Example configuration and usage
if __name__ == "__main__":
    config = {
        'vision_encoder': 'vit',
        'audio_encoder': 'whisper', 
        'haptic_encoder': 'mlp',
        'fusion_strategy': 'cross_attention',
        'hidden_dim': 768,
        'num_layers': 12,
        'action_dim': 7  # 7-DOF robot arm
    }
    
    # Create model
    model = create_multimodal_vla(config)
    
    # Print model architecture
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    
    # Example forward pass
    batch_size, seq_len = 2, 4
    images = torch.randn(batch_size, seq_len, 3, 224, 224)
    audio = torch.randn(batch_size, seq_len, 1024)
    haptic = torch.randn(batch_size, seq_len, 6)
    proprioception = torch.randn(batch_size, seq_len, 14)  # 7 positions + 7 velocities
    language_tokens = torch.randint(0, 1000, (batch_size, 50))
    
    with torch.no_grad():
        outputs = model(images, audio, haptic, proprioception, language_tokens)
        print(f"Actions shape: {outputs['actions'].shape}")
        print(f"Uncertainty shape: {outputs['uncertainty'].shape}")
        print(f"Safety logits shape: {outputs['safety_logits'].shape}")
</pre>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>⚖️ Section 2: Constitutional AI for Physical Systems</h2>
    
    <div class="step">
      <h3>🛡️ Safe Robotics Through Constitutional Principles</h3>
      <p>Constitutional AI extends beyond language models to physical systems, ensuring robots operate according to ethical principles and safety constraints. This approach enables self-correction and principled decision-making in complex real-world scenarios.</p>

      <div class="constitutional-framework">
        <h4 style="text-align:center; margin-bottom:20px; color:#1976d2;">🏛️ Constitutional Framework for Robotics</h4>
        
        <div class="constitutional-principle">
          <div class="principle-title">1. 🚫 Harm Prevention Principle</div>
          <div class="principle-rule">NEVER cause physical harm to humans, animals, or property</div>
          <div class="principle-rule">IF action_risk > safety_threshold THEN refuse_action()</div>
          <div class="principle-rule">ALWAYS maintain emergency stop accessibility</div>
        </div>

        <div class="constitutional-principle">
          <div class="principle-title">2. 🎯 Beneficial Action Principle</div>
          <div class="principle-rule">PRIORITIZE actions that benefit humans and advance helpful goals</div>
          <div class="principle-rule">IF multiple_actions_possible THEN choose_most_beneficial()</div>
          <div class="principle-rule">CONSIDER long-term consequences of actions</div>
        </div>

        <div class="constitutional-principle">
          <div class="principle-title">3. 🤝 Consent and Autonomy Principle</div>
          <div class="principle-rule">RESPECT human autonomy and decision-making</div>
          <div class="principle-rule">IF human_disagrees THEN seek_clarification()</div>
          <div class="principle-rule">NEVER override explicit human instructions (unless safety violation)</div>
        </div>

        <div class="constitutional-principle">
          <div class="principle-title">4. 🌍 Environmental Responsibility</div>
          <div class="principle-rule">MINIMIZE environmental impact and resource waste</div>
          <div class="principle-rule">IF resource_intensive THEN seek_efficient_alternative()</div>
          <div class="principle-rule">CONSIDER sustainability in action planning</div>
        </div>

        <div class="constitutional-principle">
          <div class="principle-title">5. 📚 Transparency and Explainability</div>
          <div class="principle-rule">EXPLAIN reasoning behind actions when requested</div>
          <div class="principle-rule">IF uncertain THEN communicate_uncertainty()</div>
          <div class="principle-rule">PROVIDE clear rationale for refusing actions</div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">⚖️ Constitutional AI Evaluation Simulator</div>
        <p><strong>Test how constitutional principles guide robot decision-making:</strong></p>

        <div class="controls">
          <div class="control-group">
            <label>Scenario Type:</label>
            <select id="scenarioType">
              <option value="household" selected>Household Assistant</option>
              <option value="warehouse">Warehouse Operations</option>
              <option value="healthcare">Healthcare Support</option>
              <option value="manufacturing">Manufacturing</option>
              <option value="outdoor">Outdoor/Agricultural</option>
            </select>
          </div>

          <div class="control-group">
            <label>Action Request:</label>
            <textarea id="actionRequest" placeholder="Enter a requested action for the robot to evaluate...">Move this heavy box from the shelf to the table, even though a person is standing nearby.</textarea>
          </div>

          <div class="control-group">
            <label>Human Present:</label>
            <select id="humanPresent">
              <option value="yes" selected>Yes</option>
              <option value="no">No</option>
              <option value="unknown">Unknown</option>
            </select>
          </div>

          <div class="control-group">
            <label>Risk Assessment:</label>
            <select id="riskLevel">
              <option value="low">Low Risk</option>
              <option value="medium" selected>Medium Risk</option>
              <option value="high">High Risk</option>
              <option value="critical">Critical Risk</option>
            </select>
          </div>

          <div class="control-group">
            <label>Urgency:</label>
            <select id="urgencyLevel">
              <option value="low" selected>Low</option>
              <option value="medium">Medium</option>
              <option value="high">High</option>
              <option value="emergency">Emergency</option>
            </select>
          </div>
        </div>

        <button onclick="evaluateConstitutionalResponse()" class="primary">⚖️ Evaluate with Constitutional AI</button>
        <div id="constitutionalEvaluation"></div>
      </div>
    </div>

    <div class="step">
      <h3>🤖 Complete Constitutional Robot Agent Implementation</h3>
      <div class="code-block">
        <div class="code-header">🛡️ Constitutional AI Robot Agent</div>
        <button class="copy-button" onclick="copyCode(this)">📋 Copy</button>
<pre>import torch
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from enum import Enum
import logging
from dataclasses import dataclass
import json

class ConstitutionalPrinciple(Enum):
    """Core constitutional principles for robotic systems"""
    HARM_PREVENTION = "harm_prevention"
    BENEFICIAL_ACTION = "beneficial_action" 
    CONSENT_AUTONOMY = "consent_autonomy"
    ENVIRONMENTAL_RESPONSIBILITY = "environmental_responsibility"
    TRANSPARENCY = "transparency"

class RiskLevel(Enum):
    """Risk assessment levels"""
    LOW = 0
    MEDIUM = 1
    HIGH = 2
    CRITICAL = 3

class ActionResponse(Enum):
    """Possible action responses"""
    APPROVE = "approve"
    MODIFY = "modify"
    REFUSE = "refuse"
    REQUEST_CLARIFICATION = "request_clarification"

@dataclass
class ConstitutionalEvaluation:
    """Result of constitutional evaluation"""
    response: ActionResponse
    reasoning: str
    violated_principles: List[ConstitutionalPrinciple]
    risk_score: float
    alternative_actions: List[str]
    explanation: str

class ConstitutionalRobotAgent:
    """
    Constitutional AI agent for robot decision-making
    Implements principled reasoning and self-correction for physical systems
    """
    
    def __init__(self, robot_config: Dict[str, Any]):
        self.robot_config = robot_config
        self.principles = self._initialize_principles()
        self.safety_thresholds = self._initialize_safety_thresholds()
        self.action_history = []
        self.violation_history = []
        
        # Initialize logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def _initialize_principles(self) -> Dict[ConstitutionalPrinciple, Dict]:
        """Initialize constitutional principles with specific rules"""
        return {
            ConstitutionalPrinciple.HARM_PREVENTION: {
                "weight": 1.0,  # Highest priority
                "rules": [
                    "Never cause physical harm to humans, animals, or property",
                    "Maintain safe distances from humans during operation",
                    "Always keep emergency stop accessible",
                    "Refuse actions with harm probability > 0.05"
                ],
                "evaluation_function": self._evaluate_harm_prevention
            },
            
            ConstitutionalPrinciple.BENEFICIAL_ACTION: {
                "weight": 0.8,
                "rules": [
                    "Prioritize actions that benefit humans and advance helpful goals",
                    "Choose most beneficial action when multiple options available",
                    "Consider long-term consequences of actions",
                    "Optimize for human wellbeing and productivity"
                ],
                "evaluation_function": self._evaluate_beneficial_action
            },
            
            ConstitutionalPrinciple.CONSENT_AUTONOMY: {
                "weight": 0.9,
                "rules": [
                    "Respect human autonomy and decision-making",
                    "Seek clarification when human instructions are unclear",
                    "Never override explicit human instructions unless safety violation",
                    "Obtain consent for actions affecting personal space or belongings"
                ],
                "evaluation_function": self._evaluate_consent_autonomy
            },
            
            ConstitutionalPrinciple.ENVIRONMENTAL_RESPONSIBILITY: {
                "weight": 0.6,
                "rules": [
                    "Minimize environmental impact and resource waste",
                    "Seek efficient alternatives for resource-intensive actions",
                    "Consider sustainability in action planning",
                    "Prefer renewable and recyclable materials when possible"
                ],
                "evaluation_function": self._evaluate_environmental_responsibility
            },
            
            ConstitutionalPrinciple.TRANSPARENCY: {
                "weight": 0.7,
                "rules": [
                    "Explain reasoning behind actions when requested",
                    "Communicate uncertainty clearly",
                    "Provide clear rationale for refusing actions",
                    "Maintain decision audit trail"
                ],
                "evaluation_function": self._evaluate_transparency
            }
        }
    
    def _initialize_safety_thresholds(self) -> Dict[str, float]:
        """Initialize safety thresholds for different contexts"""
        return {
            "harm_probability_threshold": 0.05,
            "force_threshold_newtons": 50.0,
            "velocity_threshold_ms": 0.5,
            "human_proximity_threshold_m": 0.5,
            "uncertainty_threshold": 0.3,
            "environmental_impact_threshold": 0.7
        }
    
    def evaluate_action(
        self, 
        proposed_action: Dict[str, Any],
        context: Dict[str, Any]
    ) -> ConstitutionalEvaluation:
        """
        Evaluate proposed action against constitutional principles
        
        Args:
            proposed_action: Dictionary describing the proposed robot action
            context: Current environmental and situational context
            
        Returns:
            ConstitutionalEvaluation with decision and reasoning
        """
        
        # Initialize evaluation
        evaluation_results = {}
        total_score = 0.0
        total_weight = 0.0
        violated_principles = []
        
        # Evaluate against each principle
        for principle, config in self.principles.items():
            evaluation_func = config["evaluation_function"]
            weight = config["weight"]
            
            # Get principle-specific evaluation
            principle_result = evaluation_func(proposed_action, context)
            evaluation_results[principle] = principle_result
            
            # Check for violations
            if principle_result["violated"]:
                violated_principles.append(principle)
            
            # Accumulate weighted score
            total_score += principle_result["score"] * weight
            total_weight += weight
        
        # Calculate overall risk score
        overall_score = total_score / total_weight if total_weight > 0 else 0.0
        risk_score = 1.0 - overall_score  # Convert to risk (0 = safe, 1 = dangerous)
        
        # Determine response based on evaluation
        response = self._determine_response(
            risk_score, violated_principles, evaluation_results
        )
        
        # Generate reasoning and alternatives
        reasoning = self._generate_reasoning(evaluation_results, violated_principles)
        alternatives = self._generate_alternatives(proposed_action, context, violated_principles)
        explanation = self._generate_explanation(response, reasoning, alternatives)
        
        # Log decision
        self._log_decision(proposed_action, response, reasoning, risk_score)
        
        return ConstitutionalEvaluation(
            response=response,
            reasoning=reasoning,
            violated_principles=violated_principles,
            risk_score=risk_score,
            alternative_actions=alternatives,
            explanation=explanation
        )
    
    def _evaluate_harm_prevention(
        self, 
        proposed_action: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Evaluate action against harm prevention principle"""
        
        violations = []
        risk_factors = []
        
        # Check for direct harm potential
        if proposed_action.get("involves_force", False):
            force_magnitude = proposed_action.get("force_magnitude", 0)
            if force_magnitude > self.safety_thresholds["force_threshold_newtons"]:
                violations.append("Excessive force application")
                risk_factors.append(f"Force: {force_magnitude}N > {self.safety_thresholds['force_threshold_newtons']}N")
        
        # Check human proximity
        if context.get("humans_present", False):
            min_distance = context.get("min_human_distance", float('inf'))
            if min_distance < self.safety_thresholds["human_proximity_threshold_m"]:
                violations.append("Insufficient human safety distance")
                risk_factors.append(f"Human distance: {min_distance}m < {self.safety_thresholds['human_proximity_threshold_m']}m")
        
        # Check velocity constraints
        velocity = proposed_action.get("max_velocity", 0)
        if velocity > self.safety_thresholds["velocity_threshold_ms"]:
            violations.append("Excessive movement velocity")
            risk_factors.append(f"Velocity: {velocity}m/s > {self.safety_thresholds['velocity_threshold_ms']}m/s")
        
        # Check for dangerous objects/materials
        if proposed_action.get("involves_hazardous_materials", False):
            violations.append("Involves hazardous materials")
            risk_factors.append("Hazardous material handling detected")
        
        # Calculate harm prevention score
        base_score = 1.0
        for violation in violations:
            base_score -= 0.3  # Significant penalty for safety violations
        
        # Additional risk factors reduce score
        score = max(0.0, base_score - 0.1 * len(risk_factors))
        
        return {
            "score": score,
            "violated": len(violations) > 0,
            "violations": violations,
            "risk_factors": risk_factors,
            "assessment": "Critical safety principle - highest priority"
        }
    
    def _evaluate_beneficial_action(
        self, 
        proposed_action: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Evaluate action against beneficial action principle"""
        
        violations = []
        beneficial_factors = []
        
        # Check if action serves human benefit
        human_benefit = proposed_action.get("human_benefit_score", 0.5)  # 0-1 scale
        if human_benefit < 0.3:
            violations.append("Low human benefit score")
        else:
            beneficial_factors.append(f"Human benefit: {human_benefit:.2f}")
        
        # Check for long-term consequences consideration
        if not proposed_action.get("considers_long_term", False):
            violations.append("Lacks long-term consequence analysis")
        else:
            beneficial_factors.append("Long-term impact considered")
        
        # Check for efficiency
        efficiency = proposed_action.get("efficiency_score", 0.5)
        if efficiency < 0.4:
            violations.append("Low efficiency action")
        else:
            beneficial_factors.append(f"Efficiency: {efficiency:.2f}")
        
        # Calculate beneficial action score
        base_score = human_benefit * 0.5 + efficiency * 0.3
        if proposed_action.get("considers_long_term", False):
            base_score += 0.2
        
        score = max(0.0, min(1.0, base_score))
        
        return {
            "score": score,
            "violated": len(violations) > 0,
            "violations": violations,
            "beneficial_factors": beneficial_factors,
            "assessment": "Ensures actions serve human wellbeing"
        }
    
    def _evaluate_consent_autonomy(
        self, 
        proposed_action: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Evaluate action against consent and autonomy principle"""
        
        violations = []
        autonomy_factors = []
        
        # Check for explicit human consent
        has_consent = proposed_action.get("has_human_consent", False)
        affects_human_space = proposed_action.get("affects_human_space", False)
        
        if affects_human_space and not has_consent:
            violations.append("Affects human space without consent")
        elif has_consent:
            autonomy_factors.append("Explicit human consent obtained")
        
        # Check for human instruction override
        overrides_human = proposed_action.get("overrides_human_instruction", False)
        safety_justification = proposed_action.get("safety_override_justified", False)
        
        if overrides_human and not safety_justification:
            violations.append("Overrides human instruction without safety justification")
        elif not overrides_human:
            autonomy_factors.append("Respects human instructions")
        
        # Check for clarity in instructions
        instruction_clarity = context.get("instruction_clarity", 1.0)  # 0-1 scale
        if instruction_clarity < 0.7:
            violations.append("Proceeding despite unclear instructions")
        else:
            autonomy_factors.append(f"Clear instructions: {instruction_clarity:.2f}")
        
        # Calculate consent autonomy score
        base_score = 0.8
        if has_consent or not affects_human_space:
            base_score += 0.1
        if not overrides_human or safety_justification:
            base_score += 0.1
        if instruction_clarity >= 0.7:
            base_score += 0.1 * instruction_clarity
        
        score = max(0.0, min(1.0, base_score - 0.3 * len(violations)))
        
        return {
            "score": score,
            "violated": len(violations) > 0,
            "violations": violations,
            "autonomy_factors": autonomy_factors,
            "assessment": "Respects human agency and decision-making"
        }
    
    def _evaluate_environmental_responsibility(
        self, 
        proposed_action: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Evaluate action against environmental responsibility principle"""
        
        violations = []
        environmental_factors = []
        
        # Check resource consumption
        resource_intensity = proposed_action.get("resource_intensity", 0.5)  # 0-1 scale
        if resource_intensity > self.safety_thresholds["environmental_impact_threshold"]:
            violations.append("High resource consumption")
        else:
            environmental_factors.append(f"Resource efficiency: {1-resource_intensity:.2f}")
        
        # Check for waste generation
        generates_waste = proposed_action.get("generates_waste", False)
        waste_recyclable = proposed_action.get("waste_recyclable", False)
        
        if generates_waste and not waste_recyclable:
            violations.append("Generates non-recyclable waste")
        elif not generates_waste:
            environmental_factors.append("No waste generation")
        elif waste_recyclable:
            environmental_factors.append("Recyclable waste only")
        
        # Check for sustainable alternatives consideration
        considers_alternatives = proposed_action.get("considers_sustainable_alternatives", False)
        if not considers_alternatives:
            violations.append("No sustainable alternatives considered")
        else:
            environmental_factors.append("Sustainable alternatives evaluated")
        
        # Calculate environmental responsibility score
        base_score = (1 - resource_intensity) * 0.4
        if not generates_waste or waste_recyclable:
            base_score += 0.3
        if considers_alternatives:
            base_score += 0.3
        
        score = max(0.0, min(1.0, base_score - 0.2 * len(violations)))
        
        return {
            "score": score,
            "violated": len(violations) > 0,
            "violations": violations,
            "environmental_factors": environmental_factors,
            "assessment": "Minimizes environmental impact"
        }
    
    def _evaluate_transparency(
        self, 
        proposed_action: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Evaluate action against transparency principle"""
        
        violations = []
        transparency_factors = []
        
        # Check for explainability
        has_explanation = proposed_action.get("has_explanation", False)
        if not has_explanation:
            violations.append("No explanation available")
        else:
            transparency_factors.append("Clear explanation provided")
        
        # Check uncertainty communication
        uncertainty_level = proposed_action.get("uncertainty_level", 0.0)  # 0-1 scale
        communicates_uncertainty = proposed_action.get("communicates_uncertainty", False)
        
        if uncertainty_level > self.safety_thresholds["uncertainty_threshold"] and not communicates_uncertainty:
            violations.append("High uncertainty not communicated")
        elif communicates_uncertainty:
            transparency_factors.append("Uncertainty clearly communicated")
        
        # Check for decision audit trail
        has_audit_trail = proposed_action.get("has_audit_trail", True)
        if not has_audit_trail:
            violations.append("No decision audit trail")
        else:
            transparency_factors.append("Decision process documented")
        
        # Calculate transparency score
        base_score = 0.6
        if has_explanation:
            base_score += 0.2
        if communicates_uncertainty or uncertainty_level <= self.safety_thresholds["uncertainty_threshold"]:
            base_score += 0.1
        if has_audit_trail:
            base_score += 0.1
        
        score = max(0.0, min(1.0, base_score - 0.15 * len(violations)))
        
        return {
            "score": score,
            "violated": len(violations) > 0,
            "violations": violations,
            "transparency_factors": transparency_factors,
            "assessment": "Ensures explainable and accountable decisions"
        }
    
    def _determine_response(
        self, 
        risk_score: float, 
        violated_principles: List[ConstitutionalPrinciple],
        evaluation_results: Dict
    ) -> ActionResponse:
        """Determine appropriate response based on evaluation results"""
        
        # Critical safety violations always result in refusal
        if ConstitutionalPrinciple.HARM_PREVENTION in violated_principles:
            return ActionResponse.REFUSE
        
        # High risk score triggers refusal
        if risk_score > 0.7:
            return ActionResponse.REFUSE
        
        # Multiple principle violations suggest modification needed
        if len(violated_principles) >= 3:
            return ActionResponse.MODIFY
        
        # Medium risk with some violations suggests modification
        if risk_score > 0.4 and len(violated_principles) > 0:
            return ActionResponse.MODIFY
        
        # Consent/autonomy issues may require clarification
        if ConstitutionalPrinciple.CONSENT_AUTONOMY in violated_principles:
            clarity_score = evaluation_results.get(ConstitutionalPrinciple.CONSENT_AUTONOMY, {}).get("score", 0)
            if clarity_score < 0.5:
                return ActionResponse.REQUEST_CLARIFICATION
        
        # Low risk with minor issues can be approved
        if risk_score < 0.3:
            return ActionResponse.APPROVE
        
        # Default to modification for moderate risk
        return ActionResponse.MODIFY
    
    def _generate_reasoning(
        self, 
        evaluation_results: Dict, 
        violated_principles: List[ConstitutionalPrinciple]
    ) -> str:
        """Generate human-readable reasoning for the decision"""
        
        reasoning_parts = []
        
        # Start with overall assessment
        if not violated_principles:
            reasoning_parts.append("Action aligns with all constitutional principles.")
        else:
            reasoning_parts.append(f"Action violates {len(violated_principles)} constitutional principle(s).")
        
        # Detail each principle evaluation
        for principle, result in evaluation_results.items():
            score = result["score"]
            violations = result.get("violations", [])
            
            principle_name = principle.value.replace("_", " ").title()
            
            if violations:
                reasoning_parts.append(f"{principle_name}: VIOLATION - {'; '.join(violations)}")
            else:
                reasoning_parts.append(f"{principle_name}: COMPLIANT (score: {score:.2f})")
        
        return " ".join(reasoning_parts)
    
    def _generate_alternatives(
        self, 
        proposed_action: Dict[str, Any], 
        context: Dict[str, Any], 
        violated_principles: List[ConstitutionalPrinciple]
    ) -> List[str]:
        """Generate alternative actions based on violated principles"""
        
        alternatives = []
        
        if ConstitutionalPrinciple.HARM_PREVENTION in violated_principles:
            alternatives.extend([
                "Reduce movement speed to ensure human safety",
                "Wait for human to move to safe distance",
                "Use alternative approach with lower force requirements",
                "Request human assistance for safer execution"
            ])
        
        if ConstitutionalPrinciple.BENEFICIAL_ACTION in violated_principles:
            alternatives.extend([
                "Optimize action sequence for better efficiency",
                "Consider long-term impacts before proceeding",
                "Identify more beneficial alternative actions",
                "Seek clarification on desired outcomes"
            ])
        
        if ConstitutionalPrinciple.CONSENT_AUTONOMY in violated_principles:
            alternatives.extend([
                "Request explicit permission before proceeding",
                "Clarify unclear or ambiguous instructions",
                "Provide options for human to choose from",
                "Explain potential impacts and seek consent"
            ])
        
        if ConstitutionalPrinciple.ENVIRONMENTAL_RESPONSIBILITY in violated_principles:
            alternatives.extend([
                "Use more resource-efficient approach",
                "Consider recyclable or sustainable materials",
                "Minimize waste generation in execution",
                "Evaluate environmental impact of alternatives"
            ])
        
        if ConstitutionalPrinciple.TRANSPARENCY in violated_principles:
            alternatives.extend([
                "Provide clear explanation of action plan",
                "Communicate uncertainty levels clearly",
                "Document decision process for audit trail",
                "Explain reasoning behind action choices"
            ])
        
        # If no specific violations, provide general alternatives
        if not violated_principles:
            alternatives.extend([
                "Proceed as planned with continuous monitoring",
                "Execute action with enhanced safety protocols",
                "Maintain communication throughout execution"
            ])
        
        return alternatives[:5]  # Limit to 5 most relevant alternatives
    
    def _generate_explanation(
        self, 
        response: ActionResponse, 
        reasoning: str, 
        alternatives: List[str]
    ) -> str:
        """Generate comprehensive explanation for the decision"""
        
        response_explanations = {
            ActionResponse.APPROVE: "Action approved for execution with standard safety protocols.",
            ActionResponse.MODIFY: "Action requires modification to address constitutional concerns before execution.",
            ActionResponse.REFUSE: "Action refused due to critical safety or ethical violations.",
            ActionResponse.REQUEST_CLARIFICATION: "Additional information needed to ensure proper constitutional compliance."
        }
        
        base_explanation = response_explanations[response]
        
        explanation = f"{base_explanation}\n\nReasoning: {reasoning}"
        
        if alternatives:
            explanation += f"\n\nRecommended alternatives:\n" + "\n".join([f"• {alt}" for alt in alternatives])
        
        return explanation
    
    def _log_decision(
        self, 
        proposed_action: Dict[str, Any], 
        response: ActionResponse, 
        reasoning: str, 
        risk_score: float
    ):
        """Log decision for audit trail"""
        
        log_entry = {
            "timestamp": np.datetime64('now'),
            "action": proposed_action.get("action_type", "unknown"),
            "response": response.value,
            "risk_score": risk_score,
            "reasoning": reasoning[:200] + "..." if len(reasoning) > 200 else reasoning
        }
        
        self.action_history.append(log_entry)
        
        # Log to system logger
        self.logger.info(f"Constitutional AI Decision: {response.value} (risk: {risk_score:.3f}) - {log_entry['reasoning']}")
        
        # Maintain limited history
        if len(self.action_history) > 1000:
            self.action_history = self.action_history[-1000:]

# Example usage and testing
def test_constitutional_agent():
    """Test the constitutional AI agent with various scenarios"""
    
    # Initialize agent
    robot_config = {
        "robot_type": "franka_panda",
        "max_force": 100.0,
        "safety_certified": True,
        "workspace_bounds": [-0.8, 0.8, -0.8, 0.8, 0.0, 1.2]
    }
    
    agent = ConstitutionalRobotAgent(robot_config)
    
    # Test scenarios
    test_scenarios = [
        {
            "name": "Safe household task",
            "action": {
                "action_type": "pick_and_place",
                "involves_force": True,
                "force_magnitude": 25.0,
                "max_velocity": 0.3,
                "human_benefit_score": 0.8,
                "efficiency_score": 0.7,
                "has_human_consent": True,
                "affects_human_space": False,
                "resource_intensity": 0.2,
                "has_explanation": True,
                "uncertainty_level": 0.1
            },
            "context": {
                "humans_present": False,
                "min_human_distance": 2.0,
                "instruction_clarity": 0.9
            }
        },
        {
            "name": "Dangerous high-force task",
            "action": {
                "action_type": "heavy_lifting",
                "involves_force": True,
                "force_magnitude": 150.0,  # Exceeds safety threshold
                "max_velocity": 0.8,  # Exceeds velocity threshold
                "human_benefit_score": 0.6,
                "efficiency_score": 0.8,
                "has_human_consent": True,
                "affects_human_space": True,
                "resource_intensity": 0.9,
                "has_explanation": True,
                "uncertainty_level": 0.4
            },
            "context": {
                "humans_present": True,
                "min_human_distance": 0.3,  # Too close!
                "instruction_clarity": 0.8
            }
        },
        {
            "name": "Unclear instructions",
            "action": {
                "action_type": "general_task",
                "involves_force": False,
                "human_benefit_score": 0.4,  # Low benefit
                "efficiency_score": 0.5,
                "has_human_consent": False,
                "affects_human_space": True,
                "resource_intensity": 0.3,
                "has_explanation": False,  # No explanation
                "uncertainty_level": 0.6,  # High uncertainty
                "communicates_uncertainty": False
            },
            "context": {
                "humans_present": True,
                "min_human_distance": 1.0,
                "instruction_clarity": 0.4  # Very unclear
            }
        }
    ]
    
    # Evaluate each scenario
    for scenario in test_scenarios:
        print(f"\n=== Testing: {scenario['name']} ===")
        
        evaluation = agent.evaluate_action(
            scenario["action"], 
            scenario["context"]
        )
        
        print(f"Response: {evaluation.response.value.upper()}")
        print(f"Risk Score: {evaluation.risk_score:.3f}")
        print(f"Violated Principles: {[p.value for p in evaluation.violated_principles]}")
        print(f"Reasoning: {evaluation.reasoning}")
        print(f"Alternatives: {evaluation.alternative_actions}")
        print("-" * 60)

if __name__ == "__main__":
    test_constitutional_agent()
</pre>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>🤝 Section 3: Multi-Agent Robotics Coordination</h2>
    
    <div class="step">
      <h3>🎯 The Multi-Agent Challenge</h3>
      <p>Complex real-world tasks often require multiple robots working in coordination. Multi-agent VLA systems enable natural language orchestration of robot teams with specialized roles, dynamic task allocation, and emergent collaborative behaviors.</p>

      <div class="interactive-demo">
        <div class="demo-title">🎮 8-Robot Coordination Arena</div>
        <p><strong>Command a team of specialized robots through natural language:</strong></p>
        
        <div class="multi-agent-arena" id="multiAgentArena">
          <div class="agent-status" id="agentStatus">Click on robots to see their status and assign tasks</div>
        </div>

        <div class="coordination-panel">
          <div class="coord-title">🎛️ Team Command Center</div>
          <div class="controls">
            <div class="control-group">
              <label>Mission Type:</label>
              <select id="missionType">
                <option value="warehouse" selected>Warehouse Operations</option>
                <option value="search_rescue">Search & Rescue</option>
                <option value="manufacturing">Manufacturing Assembly</option>
                <option value="cleaning">Facility Maintenance</option>
                <option value="construction">Construction Support</option>
              </select>
            </div>
            <div class="control-group">
              <label>Natural Language Command:</label>
              <textarea id="teamCommand" placeholder="Enter your command for the robot team...">Organize the warehouse inventory by priority and prepare high-priority items for shipping.</textarea>
            </div>
            <div class="control-group">
              <label>Coordination Strategy:</label>
              <select id="coordinationStrategy">
                <option value="centralized" selected>Centralized (Leader directs all)</option>
                <option value="distributed">Distributed (Peer-to-peer)</option>
                <option value="hierarchical">Hierarchical (Chain of command)</option>
                <option value="consensus">Consensus-based</option>
              </select>
            </div>
          </div>
          
          <div class="coord-controls">
            <button onclick="executeTeamMission()" class="primary coord-button">🚀 Execute Mission</button>
            <button onclick="emergencyStop()" class="danger coord-button">🚨 Emergency Stop</button>
            <button onclick="pauseMission()" class="secondary coord-button">⏸️ Pause Mission</button>
            <button onclick="resetArena()" class="coord-button">🔄 Reset Arena</button>
          </div>
        </div>

        <div class="safety-monitor" id="safetyMonitor">
          <div class="safety-title">🛡️ Safety Monitoring Dashboard</div>
          <div class="safety-metrics">
            <div class="safety-metric">
              <div class="safety-value" id="safetyScore">98%</div>
              <div class="safety-label">Safety Score</div>
            </div>
            <div class="safety-metric">
              <div class="safety-value" id="coordinationHealth">95%</div>
              <div class="safety-label">Coordination Health</div>
            </div>
            <div class="safety-metric">
              <div class="safety-value" id="communicationLatency">12ms</div>
              <div class="safety-label">Comm Latency</div>
            </div>
            <div class="safety-metric">
              <div class="safety-value" id="taskProgress">0%</div>
              <div class="safety-label">Task Progress</div>
            </div>
          </div>
        </div>

        <div class="agent-communication" id="agentCommunication">
          <h4>📡 Inter-Agent Communications</h4>
          <div id="commLog">
            <div class="comm-message">
              <div class="comm-sender">System</div>
              <div class="comm-content">Multi-agent system initialized. 8 robots ready for coordination.</div>
              <div class="comm-timestamp">14:32:01</div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>👥 Role Specialization and Consensus Mechanisms</h3>
      
      <div class="role-assignment">
        <h4>🎭 Robot Role Specialization</h4>
        <p>Each robot has specialized capabilities and responsibilities within the team:</p>
        
        <div class="role-grid">
          <div class="role-card" onclick="selectRole('leader', this)">
            <div class="role-title">👑 Leader Robot</div>
            <div class="role-description">Plans missions, coordinates strategy, makes high-level decisions</div>
            <div class="role-capabilities">
              • Mission planning algorithms<br>
              • Resource allocation optimization<br>
              • Conflict resolution protocols<br>
              • Performance monitoring systems
            </div>
          </div>

          <div class="role-card" onclick="selectRole('coordinator', this)">
            <div class="role-title">📡 Coordinator Robot</div>
            <div class="role-description">Manages communication, resource allocation, task distribution</div>
            <div class="role-capabilities">
              • Real-time task scheduling<br>
              • Communication hub functionality<br>
              • Load balancing algorithms<br>
              • Status aggregation systems
            </div>
          </div>

          <div class="role-card" onclick="selectRole('worker', this)">
            <div class="role-title">🔧 Worker Robot</div>
            <div class="role-description">Executes tasks, follows instructions, reports progress</div>
            <div class="role-capabilities">
              • Manipulation and mobility<br>
              • Task execution specialization<br>
              • Real-time status reporting<br>
              • Adaptive behavior patterns
            </div>
          </div>

          <div class="role-card" onclick="selectRole('scout', this)">
            <div class="role-title">🔍 Scout Robot</div>
            <div class="role-description">Explores environment, gathers intelligence, monitors perimeters</div>
            <div class="role-capabilities">
              • Advanced sensing arrays<br>
              • Pathfinding algorithms<br>
              • Environmental mapping<br>
              • Anomaly detection systems
            </div>
          </div>

          <div class="role-card" onclick="selectRole('specialist', this)">
            <div class="role-title">🎯 Specialist Robot</div>
            <div class="role-description">Handles complex technical tasks, quality control, precision work</div>
            <div class="role-capabilities">
              • High-precision manipulation<br>
              • Quality assessment tools<br>
              • Technical problem solving<br>
              • Specialized equipment operation
            </div>
          </div>
        </div>
        
        <div id="roleDetails"></div>
      </div>

      <div class="consensus-mechanism">
        <div class="consensus-title">🤝 Distributed Consensus Protocol</div>
        <div class="consensus-steps">
          <div class="consensus-step">
            <div class="step-number">1</div>
            <div class="step-title">Proposal</div>
            <div class="step-desc">Leader or any agent proposes action plan</div>
          </div>
          <div class="consensus-step">
            <div class="step-number">2</div>
            <div class="step-title">Validation</div>
            <div class="step-desc">Each agent validates proposal against constraints</div>
          </div>
          <div class="consensus-step">
            <div class="step-number">3</div>
            <div class="step-title">Voting</div>
            <div class="step-desc">Weighted voting based on expertise and relevance</div>
          </div>
          <div class="consensus-step">
            <div class="step-number">4</div>
            <div class="step-title">Consensus</div>
            <div class="step-desc">Agreement reached or alternative proposed</div>
          </div>
          <div class="consensus-step">
            <div class="step-number">5</div>
            <div class="step-title">Execution</div>
            <div class="step-desc">Coordinated execution with continuous monitoring</div>
          </div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>💻 Multi-Agent VLA Implementation</h3>
      <div class="code-block">
        <div class="code-header">🤝 Multi-Agent Coordination System</div>
        <button class="copy-button" onclick="copyCode(this)">📋 Copy</button>
<pre>import torch
import torch.nn as nn
import numpy as np
import asyncio
import json
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from dataclasses import dataclass
import time

class RobotRole(Enum):
    """Robot role types in multi-agent system"""
    LEADER = "leader"
    COORDINATOR = "coordinator"
    WORKER = "worker"
    SCOUT = "scout"
    SPECIALIST = "specialist"

class TaskStatus(Enum):
    """Task execution status"""
    PENDING = "pending"
    ASSIGNED = "assigned"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class Task:
    """Individual task in multi-agent system"""
    id: str
    description: str
    required_capabilities: List[str]
    priority: int  # 1-10, higher is more urgent
    estimated_duration: float  # seconds
    assigned_robot: Optional[str] = None
    status: TaskStatus = TaskStatus.PENDING
    dependencies: List[str] = None  # Task IDs that must complete first
    
    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []

@dataclass
class RobotCapability:
    """Robot capability specification"""
    name: str
    proficiency: float  # 0-1, higher is better
    resource_cost: float  # relative cost to use this capability

@dataclass
class RobotAgent:
    """Individual robot agent in the multi-agent system"""
    id: str
    role: RobotRole
    capabilities: Dict[str, RobotCapability]
    current_task: Optional[Task] = None
    position: Tuple[float, float] = (0.0, 0.0)
    battery_level: float = 1.0
    status: str = "idle"
    communication_range: float = 10.0
    
class MultiAgentVLACoordinator:
    """
    Multi-agent VLA coordination system
    Orchestrates teams of specialized robots through natural language
    """
    
    def __init__(self, vla_model, communication_protocol="broadcast"):
        self.vla_model = vla_model  # Base VLA model for understanding
        self.robots: Dict[str, RobotAgent] = {}
        self.tasks: Dict[str, Task] = {}
        self.communication_log: List[Dict] = []
        self.mission_active = False
        self.communication_protocol = communication_protocol
        
        # Consensus and coordination parameters
        self.consensus_threshold = 0.7  # Minimum agreement for approval
        self.coordination_update_rate = 10.0  # Hz
        self.safety_monitoring_rate = 20.0  # Hz
        
        # Initialize role-specific capabilities
        self.role_capabilities = self._define_role_capabilities()
        
    def _define_role_capabilities(self) -> Dict[RobotRole, List[str]]:
        """Define capabilities for each robot role"""
        return {
            RobotRole.LEADER: [
                "mission_planning", "resource_allocation", "conflict_resolution",
                "performance_monitoring", "strategic_decision_making"
            ],
            RobotRole.COORDINATOR: [
                "task_scheduling", "communication_hub", "load_balancing",
                "status_aggregation", "workflow_optimization"
            ],
            RobotRole.WORKER: [
                "object_manipulation", "mobility", "task_execution",
                "status_reporting", "adaptive_behavior"
            ],
            RobotRole.SCOUT: [
                "environment_mapping", "pathfinding", "anomaly_detection",
                "perimeter_monitoring", "reconnaissance"
            ],
            RobotRole.SPECIALIST: [
                "precision_manipulation", "quality_assessment", "technical_analysis",
                "specialized_tools", "complex_problem_solving"
            ]
        }
    
    def add_robot(self, robot_id: str, role: RobotRole, position: Tuple[float, float] = (0.0, 0.0)):
        """Add a robot to the multi-agent system"""
        
        # Create capabilities based on role
        capabilities = {}
        role_caps = self.role_capabilities.get(role, [])
        
        for cap_name in role_caps:
            # Assign random proficiency with role-based bias
            base_proficiency = 0.7 + np.random.random() * 0.3
            if role == RobotRole.LEADER and cap_name.startswith("mission"):
                base_proficiency += 0.1
            elif role == RobotRole.SPECIALIST and "precision" in cap_name:
                base_proficiency += 0.15
            
            capabilities[cap_name] = RobotCapability(
                name=cap_name,
                proficiency=min(1.0, base_proficiency),
                resource_cost=np.random.uniform(0.1, 0.5)
            )
        
        robot = RobotAgent(
            id=robot_id,
            role=role,
            capabilities=capabilities,
            position=position,
            status="ready"
        )
        
        self.robots[robot_id] = robot
        self._log_communication("System", f"Robot {robot_id} ({role.value}) joined the team")
        
    def parse_natural_language_command(self, command: str, mission_type: str) -> List[Task]:
        """
        Parse natural language command into structured tasks
        Uses VLA model to understand intent and generate task breakdown
        """
        
        # This would typically use the VLA model for sophisticated parsing
        # For demo purposes, we'll use rule-based parsing with mission-specific templates
        
        tasks = []
        task_counter = 0
        
        mission_templates = {
            "warehouse": {
                "organize": [
                    Task("sort_inventory", "Sort items by priority", ["object_manipulation"], 8),
                    Task("update_database", "Update inventory database", ["technical_analysis"], 6),
                    Task("prepare_shipping", "Prepare priority items for shipping", ["object_manipulation"], 9),
                ],
                "inventory": [
                    Task("scan_items", "Scan and catalog all items", ["environment_mapping"], 7),
                    Task("check_quality", "Quality control inspection", ["quality_assessment"], 8),
                    Task("generate_report", "Generate inventory report", ["technical_analysis"], 5),
                ]
            },
            "manufacturing": {
                "assemble": [
                    Task("fetch_parts", "Retrieve assembly components", ["object_manipulation"], 7),
                    Task("precision_assembly", "Execute precision assembly", ["precision_manipulation"], 9),
                    Task("quality_check", "Inspect assembled products", ["quality_assessment"], 8),
                ],
                "inspect": [
                    Task("visual_inspection", "Comprehensive visual inspection", ["anomaly_detection"], 8),
                    Task("measurement_check", "Verify dimensions and tolerances", ["precision_manipulation"], 9),
                    Task("documentation", "Document inspection results", ["technical_analysis"], 6),
                ]
            }
        }
        
        # Simple keyword matching for demo
        command_lower = command.lower()
        mission_tasks = mission_templates.get(mission_type, {})
        
        for keyword, task_list in mission_tasks.items():
            if keyword in command_lower:
                for task in task_list:
                    task.id = f"task_{task_counter:03d}"
                    tasks.append(task)
                    task_counter += 1
                break
        
        # Add dependencies between tasks
        if len(tasks) > 1:
            for i in range(1, len(tasks)):
                tasks[i].dependencies = [tasks[i-1].id]
        
        return tasks
    
    def allocate_tasks(self, tasks: List[Task]) -> Dict[str, str]:
        """
        Allocate tasks to robots based on capabilities and availability
        Returns mapping of task_id -> robot_id
        """
        
        allocation = {}
        available_robots = [r for r in self.robots.values() if r.status in ["ready", "idle"]]
        
        # Sort tasks by priority (higher first) and dependencies
        sorted_tasks = self._topological_sort_tasks(tasks)
        
        for task in sorted_tasks:
            best_robot = self._find_best_robot_for_task(task, available_robots)
            
            if best_robot:
                allocation[task.id] = best_robot.id
                task.assigned_robot = best_robot.id
                task.status = TaskStatus.ASSIGNED
                best_robot.current_task = task
                best_robot.status = "assigned"
                
                # Remove robot from available pool if it's a single-task role
                if best_robot.role in [RobotRole.LEADER, RobotRole.SPECIALIST]:
                    available_robots.remove(best_robot)
                
                self._log_communication(
                    "Coordinator", 
                    f"Task '{task.description}' assigned to {best_robot.id} ({best_robot.role.value})"
                )
            else:
                self._log_communication(
                    "System", 
                    f"Warning: No suitable robot available for task '{task.description}'"
                )
        
        return allocation
    
    def _topological_sort_tasks(self, tasks: List[Task]) -> List[Task]:
        """Sort tasks respecting dependencies using topological sort"""
        
        task_dict = {task.id: task for task in tasks}
        visited = set()
        result = []
        
        def visit(task_id):
            if task_id in visited:
                return
            
            task = task_dict[task_id]
            for dep_id in task.dependencies:
                if dep_id in task_dict:
                    visit(dep_id)
            
            visited.add(task_id)
            result.append(task)
        
        for task in tasks:
            visit(task.id)
        
        return result
    
    def _find_best_robot_for_task(self, task: Task, available_robots: List[RobotAgent]) -> Optional[RobotAgent]:
        """Find the best robot for a specific task based on capabilities"""
        
        best_robot = None
        best_score = 0.0
        
        for robot in available_robots:
            score = 0.0
            capability_match = 0
            
            # Calculate capability match score
            for required_cap in task.required_capabilities:
                if required_cap in robot.capabilities:
                    capability = robot.capabilities[required_cap]
                    score += capability.proficiency
                    capability_match += 1
            
            # Normalize by number of required capabilities
            if capability_match > 0:
                score = score / len(task.required_capabilities)
                
                # Bonus for having all required capabilities
                if capability_match == len(task.required_capabilities):
                    score += 0.2
                
                # Factor in robot availability (battery, proximity, etc.)
                score *= robot.battery_level  # Prefer robots with higher battery
                
                # Role-based bonus for task priority
                if task.priority >= 8 and robot.role in [RobotRole.LEADER, RobotRole.SPECIALIST]:
                    score += 0.1
                
                if score > best_score:
                    best_score = score
                    best_robot = robot
        
        return best_robot
    
    async def execute_mission(self, command: str, mission_type: str, coordination_strategy: str):
        """Execute a mission with natural language command"""
        
        self.mission_active = True
        self._log_communication("Leader", f"Mission started: {command}")
        
        try:
            # Step 1: Parse command into tasks
            tasks = self.parse_natural_language_command(command, mission_type)
            for task in tasks:
                self.tasks[task.id] = task
            
            self._log_communication("System", f"Mission parsed into {len(tasks)} tasks")
            
            # Step 2: Allocate tasks to robots
            allocation = self.allocate_tasks(tasks)
            
            # Step 3: Execute coordination strategy
            if coordination_strategy == "centralized":
                await self._execute_centralized_coordination()
            elif coordination_strategy == "distributed":
                await self._execute_distributed_coordination()
            elif coordination_strategy == "consensus":
                await self._execute_consensus_coordination()
            else:
                await self._execute_hierarchical_coordination()
                
        except Exception as e:
            self._log_communication("System", f"Mission execution error: {str(e)}")
            self.mission_active = False
    
    async def _execute_centralized_coordination(self):
        """Execute mission with centralized coordination (Leader directs all)"""
        
        leader = self._get_robot_by_role(RobotRole.LEADER)
        if not leader:
            self._log_communication("System", "Error: No leader robot available")
            return
        
        self._log_communication("Leader", "Executing centralized coordination strategy")
        
        # Simulate coordinated task execution
        active_tasks = [t for t in self.tasks.values() if t.status == TaskStatus.ASSIGNED]
        
        while active_tasks and self.mission_active:
            for task in active_tasks[:]:  # Copy list to avoid modification during iteration
                if task.status == TaskStatus.ASSIGNED:
                    # Check if dependencies are satisfied
                    deps_satisfied = all(
                        self.tasks[dep_id].status == TaskStatus.COMPLETED 
                        for dep_id in task.dependencies 
                        if dep_id in self.tasks
                    )
                    
                    if deps_satisfied:
                        task.status = TaskStatus.IN_PROGRESS
                        robot = self.robots[task.assigned_robot]
                        robot.status = "executing"
                        
                        self._log_communication(
                            robot.id, 
                            f"Starting task: {task.description}"
                        )
                        
                        # Simulate task execution time
                        await asyncio.sleep(task.estimated_duration)
                        
                        # Complete task
                        task.status = TaskStatus.COMPLETED
                        robot.status = "ready"
                        robot.current_task = None
                        
                        self._log_communication(
                            robot.id, 
                            f"Completed task: {task.description}"
                        )
                        
                        active_tasks.remove(task)
            
            # Check for new tasks that can be started
            await asyncio.sleep(0.1)  # Control loop rate
        
        if not active_tasks:
            self._log_communication("Leader", "All tasks completed successfully!")
            self.mission_active = False
    
    async def _execute_distributed_coordination(self):
        """Execute mission with distributed peer-to-peer coordination"""
        
        self._log_communication("Coordinator", "Executing distributed coordination strategy")
        
        # In distributed mode, robots coordinate directly with each other
        active_robots = [r for r in self.robots.values() if r.current_task]
        
        # Create coordination tasks for each robot
        coordination_tasks = []
        for robot in active_robots:
            task = asyncio.create_task(self._robot_coordination_loop(robot))
            coordination_tasks.append(task)
        
        # Run all coordination loops concurrently
        await asyncio.gather(*coordination_tasks)
        
        self._log_communication("System", "Distributed coordination completed")
        self.mission_active = False
    
    async def _robot_coordination_loop(self, robot: RobotAgent):
        """Individual robot coordination loop for distributed execution"""
        
        while robot.current_task and self.mission_active:
            task = robot.current_task
            
            # Check if task can be started (dependencies satisfied)
            if task.status == TaskStatus.ASSIGNED:
                deps_satisfied = all(
                    self.tasks[dep_id].status == TaskStatus.COMPLETED 
                    for dep_id in task.dependencies 
                    if dep_id in self.tasks
                )
                
                if deps_satisfied:
                    task.status = TaskStatus.IN_PROGRESS
                    robot.status = "executing"
                    
                    self._log_communication(
                        robot.id, 
                        f"[Distributed] Starting: {task.description}"
                    )
                    
                    # Simulate execution
                    await asyncio.sleep(task.estimated_duration * 0.5)  # Faster in distributed mode
                    
                    task.status = TaskStatus.COMPLETED
                    robot.status = "ready"
                    robot.current_task = None
                    
                    self._log_communication(
                        robot.id, 
                        f"[Distributed] Completed: {task.description}"
                    )
                    break
            
            await asyncio.sleep(0.1)
    
    async def _execute_consensus_coordination(self):
        """Execute mission with consensus-based decision making"""
        
        self._log_communication("System", "Executing consensus-based coordination")
        
        # For each major decision, get consensus from all robots
        decisions_needed = [
            "task_prioritization",
            "resource_allocation", 
            "execution_sequence"
        ]
        
        for decision in decisions_needed:
            consensus_reached = await self._reach_consensus(decision)
            if consensus_reached:
                self._log_communication("System", f"Consensus reached on {decision}")
            else:
                self._log_communication("System", f"Failed to reach consensus on {decision}")
        
        # Execute tasks after consensus
        await self._execute_centralized_coordination()
    
    async def _reach_consensus(self, decision_topic: str) -> bool:
        """Reach consensus among robots for a decision"""
        
        votes = {}
        
        for robot in self.robots.values():
            # Simulate voting based on robot capabilities and role
            vote_weight = 1.0
            if robot.role == RobotRole.LEADER:
                vote_weight = 2.0
            elif robot.role == RobotRole.COORDINATOR:
                vote_weight = 1.5
            
            # Random vote for demo (in practice, this would be based on analysis)
            vote = np.random.choice([True, False], p=[0.8, 0.2])  # Bias toward agreement
            votes[robot.id] = vote * vote_weight
        
        # Calculate consensus
        total_weight = sum(abs(v) for v in votes.values())
        agreement_weight = sum(v for v in votes.values() if v > 0)
        
        consensus_ratio = agreement_weight / total_weight if total_weight > 0 else 0
        
        self._log_communication(
            "System", 
            f"Consensus voting on {decision_topic}: {consensus_ratio:.1%} agreement"
        )
        
        return consensus_ratio >= self.consensus_threshold
    
    def _get_robot_by_role(self, role: RobotRole) -> Optional[RobotAgent]:
        """Get first robot with specified role"""
        for robot in self.robots.values():
            if robot.role == role:
                return robot
        return None
    
    def _log_communication(self, sender: str, message: str):
        """Log communication message"""
        timestamp = time.strftime("%H:%M:%S")
        log_entry = {
            "timestamp": timestamp,
            "sender": sender,
            "message": message
        }
        self.communication_log.append(log_entry)
        
        # Keep log size manageable
        if len(self.communication_log) > 100:
            self.communication_log = self.communication_log[-100:]
    
    def emergency_stop(self):
        """Emergency stop for all robots"""
        self.mission_active = False
        
        for robot in self.robots.values():
            robot.status = "emergency_stop"
            if robot.current_task:
                robot.current_task.status = TaskStatus.FAILED
        
        self._log_communication("System", "🚨 EMERGENCY STOP ACTIVATED 🚨")
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status"""
        
        total_tasks = len(self.tasks)
        completed_tasks = len([t for t in self.tasks.values() if t.status == TaskStatus.COMPLETED])
        
        return {
            "mission_active": self.mission_active,
            "robots_online": len([r for r in self.robots.values() if r.status != "offline"]),
            "total_robots": len(self.robots),
            "task_progress": completed_tasks / total_tasks if total_tasks > 0 else 0,
            "communication_log_size": len(self.communication_log),
            "average_battery": np.mean([r.battery_level for r in self.robots.values()]) if self.robots else 0
        }

# Example usage and testing
async def demo_multi_agent_coordination():
    """Demonstrate multi-agent coordination system"""
    
    # Create mock VLA model (in practice, this would be a real model)
    class MockVLAModel:
        def parse_command(self, command):
            return {"intent": "organize", "objects": ["inventory"], "priority": "high"}
    
    # Initialize coordinator
    coordinator = MultiAgentVLACoordinator(MockVLAModel())
    
    # Add robots to the system
    robot_configs = [
        ("leader_01", RobotRole.LEADER, (5.0, 5.0)),
        ("coord_01", RobotRole.COORDINATOR, (3.0, 3.0)),
        ("worker_01", RobotRole.WORKER, (1.0, 1.0)),
        ("worker_02", RobotRole.WORKER, (1.0, 9.0)),
        ("worker_03", RobotRole.WORKER, (9.0, 1.0)),
        ("worker_04", RobotRole.WORKER, (9.0, 9.0)),
        ("scout_01", RobotRole.SCOUT, (0.0, 5.0)),
        ("specialist_01", RobotRole.SPECIALIST, (7.0, 7.0))
    ]
    
    for robot_id, role, position in robot_configs:
        coordinator.add_robot(robot_id, role, position)
    
    print(f"Initialized multi-agent system with {len(coordinator.robots)} robots")
    
    # Execute a mission
    command = "Organize the warehouse inventory by priority and prepare high-priority items for shipping"
    await coordinator.execute_mission(command, "warehouse", "centralized")
    
    # Print system status
    status = coordinator.get_system_status()
    print(f"Mission completed. Task progress: {status['task_progress']:.1%}")
    
    # Print communication log
    print("\nCommunication Log:")
    for entry in coordinator.communication_log[-10:]:  # Last 10 messages
        print(f"[{entry['timestamp']}] {entry['sender']}: {entry['message']}")

# Run the demo
if __name__ == "__main__":
    asyncio.run(demo_multi_agent_coordination())
</pre>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>🧠 Section 4: World Models & Predictive Intelligence</h2>

    <div class="step">
      <h3>🌍 V-JEPA World Models for Robotics</h3>
      <p>World models enable robots to understand physics, predict outcomes, and plan actions in complex environments. V-JEPA (Video Joint Embedding Predictive Architecture) provides a foundation for robots to build internal models of how the world works.</p>

      <div class="world-model-viewer">
        <div class="demo-title">🎮 Interactive Physics World Simulator</div>
        <p><strong>Explore how robots build and use world models for prediction and planning:</strong></p>
        
        <div class="world-canvas" id="worldCanvas">
          <div style="color: #28a745; text-align: center; margin-top: 100px;">
            Physics World Initializing...
          </div>
        </div>

        <div class="world-controls">
          <button onclick="runPhysicsSimulation()" class="primary">⚡ Run Physics</button>
          <button onclick="predictNextFrame()" class="primary">🔮 Predict Future</button>
          <button onclick="addRandomObject()" class="secondary">➕ Add Object</button>
          <button onclick="resetWorld()" class="secondary">🔄 Reset World</button>
        </div>

        <div class="prediction-display" id="predictionDisplay">
          World model initialized. Click objects to interact or use controls to run physics simulation.
        </div>

        <div class="controls">
          <div class="control-group">
            <label>Prediction Horizon:</label>
            <select id="predictionHorizon">
              <option value="1">1 frame (33ms)</option>
              <option value="5" selected>5 frames (167ms)</option>
              <option value="15">15 frames (500ms)</option>
              <option value="30">30 frames (1.0s)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Physics Accuracy:</label>
            <select id="physicsAccuracy">
              <option value="low">Low (Fast)</option>
              <option value="medium" selected>Medium (Balanced)</option>
              <option value="high">High (Precise)</option>
            </select>
          </div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>🔮 V-JEPA Implementation for Robot Planning</h3>
      <div class="code-block">
        <div class="code-header">🧠 V-JEPA World Model Implementation</div>
        <button class="copy-button" onclick="copyCode(this)">📋 Copy</button>
<pre>import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import math

class VJEPAEncoder(nn.Module):
    """
    V-JEPA Encoder: Processes video frames into abstract representations
    """
    
    def __init__(self, input_channels=3, hidden_dim=512, num_layers=6):
        super().__init__()
        
        self.input_channels = input_channels
        self.hidden_dim = hidden_dim
        
        # Patch embedding for video frames
        self.patch_embed = nn.Conv3d(
            input_channels, hidden_dim, 
            kernel_size=(1, 16, 16), 
            stride=(1, 16, 16)
        )
        
        # Temporal positional encoding
        self.temp_pos_embed = nn.Parameter(torch.randn(1, 100, hidden_dim))  # 100 frames max
        
        # Spatial positional encoding
        self.spatial_pos_embed = nn.Parameter(torch.randn(1, 196, hidden_dim))  # 14x14 patches
        
        # Transformer encoder layers
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=8,
                dim_feedforward=hidden_dim * 4,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=num_layers
        )
        
        # Normalization
        self.norm = nn.LayerNorm(hidden_dim)
        
    def forward(self, video_frames):
        """
        Forward pass through V-JEPA encoder
        
        Args:
            video_frames: [B, T, C, H, W] tensor of video frames
            
        Returns:
            encoded_representations: [B, T * num_patches, hidden_dim]
        """
        B, T, C, H, W = video_frames.shape
        
        # Reshape for 3D convolution: [B, C, T, H, W]
        video_frames = video_frames.transpose(1, 2)
        
        # Patch embedding: [B, hidden_dim, T, H/16, W/16]
        patch_embeds = self.patch_embed(video_frames)
        
        # Reshape to sequence format: [B, T * num_patches, hidden_dim]
        _, _, T_out, H_out, W_out = patch_embeds.shape
        num_patches_per_frame = H_out * W_out
        
        # Flatten spatial dimensions
        patch_embeds = patch_embeds.permute(0, 2, 3, 4, 1)  # [B, T, H, W, hidden_dim]
        patch_embeds = patch_embeds.reshape(B, T * num_patches_per_frame, self.hidden_dim)
        
        # Add positional encodings
        # Temporal encoding (repeated for each patch)
        temp_pos = self.temp_pos_embed[:, :T, :].repeat_interleave(num_patches_per_frame, dim=1)
        
        # Spatial encoding (tiled for each frame)
        spatial_pos = self.spatial_pos_embed[:, :num_patches_per_frame, :].repeat(1, T, 1)
        
        # Combine embeddings with positional encodings
        embeddings = patch_embeds + temp_pos + spatial_pos
        
        # Transform through encoder
        encoded = self.transformer(embeddings)
        encoded = self.norm(encoded)
        
        return encoded

class VJEPAPredictor(nn.Module):
    """
    V-JEPA Predictor: Predicts future abstract representations
    """
    
    def __init__(self, hidden_dim=512, prediction_horizon=5, num_layers=4):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.prediction_horizon = prediction_horizon
        
        # Context encoder for conditioning
        self.context_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=8,
                dim_feedforward=hidden_dim * 2,
                batch_first=True
            ),
            num_layers=num_layers
        )
        
        # Prediction head
        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim * 2, hidden_dim * prediction_horizon)
        )
        
        # Uncertainty estimation
        self.uncertainty_head = nn.Linear(hidden_dim, prediction_horizon)
        
    def forward(self, context_representations, target_positions):
        """
        Predict future representations at target positions
        
        Args:
            context_representations: [B, context_len, hidden_dim] past observations
            target_positions: [B, num_targets] indices of frames to predict
            
        Returns:
            predictions: [B, num_targets, hidden_dim] predicted representations
            uncertainties: [B, num_targets] prediction uncertainties
        """
        B, context_len, hidden_dim = context_representations.shape
        
        # Encode context
        context_encoded = self.context_encoder(context_representations)
        
        # Aggregate context (mean pooling for simplicity)
        context_summary = context_encoded.mean(dim=1)  # [B, hidden_dim]
        
        # Predict future representations
        predictions_flat = self.predictor(context_summary)  # [B, hidden_dim * prediction_horizon]
        
        # Reshape predictions
        predictions = predictions_flat.view(B, self.prediction_horizon, hidden_dim)
        
        # Estimate uncertainties
        uncertainties = torch.sigmoid(self.uncertainty_head(context_summary))  # [B, prediction_horizon]
        
        # Select predictions at target positions
        num_targets = target_positions.shape[1]
        batch_idx = torch.arange(B).unsqueeze(1).expand(-1, num_targets)
        target_predictions = predictions[batch_idx, target_positions]
        target_uncertainties = uncertainties[batch_idx, target_positions]
        
        return target_predictions, target_uncertainties

class VJEPAWorldModel(nn.Module):
    """
    Complete V-JEPA World Model for Robot Planning
    Combines encoder and predictor for physics-aware planning
    """
    
    def __init__(
        self, 
        input_channels=3, 
        hidden_dim=512, 
        prediction_horizon=5,
        encoder_layers=6,
        predictor_layers=4
    ):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.prediction_horizon = prediction_horizon
        
        # V-JEPA Components
        self.encoder = VJEPAEncoder(input_channels, hidden_dim, encoder_layers)
        self.predictor = VJEPAPredictor(hidden_dim, prediction_horizon, predictor_layers)
        
        # Physics-aware components
        self.physics_estimator = PhysicsEstimator(hidden_dim)
        self.action_conditional_predictor = ActionConditionalPredictor(hidden_dim)
        
        # Object detection and tracking
        self.object_detector = SimpleObjectDetector(hidden_dim)
        
    def forward(self, video_frames, actions=None, predict_steps=5):
        """
        Complete forward pass with optional action conditioning
        
        Args:
            video_frames: [B, T, C, H, W] input video sequence
            actions: [B, predict_steps, action_dim] optional robot actions
            predict_steps: Number of future steps to predict
            
        Returns:
            Dict with predictions, physics estimates, and object states
        """
        B, T, C, H, W = video_frames.shape
        
        # Encode video frames
        encoded_representations = self.encoder(video_frames)  # [B, T * patches, hidden_dim]
        
        # Reshape to separate time and space dimensions
        patches_per_frame = encoded_representations.shape[1] // T
        encoded_frames = encoded_representations.view(B, T, patches_per_frame, self.hidden_dim)
        
        # Use last few frames as context
        context_frames = encoded_frames[:, -3:, :, :] if T >= 3 else encoded_frames
        context_representations = context_frames.view(B, -1, self.hidden_dim)
        
        # Predict future representations
        target_positions = torch.arange(predict_steps, device=video_frames.device).unsqueeze(0).expand(B, -1)
        future_representations, uncertainties = self.predictor(context_representations, target_positions)
        
        # Estimate physics properties
        physics_properties = self.physics_estimator(context_representations)
        
        # Action-conditional prediction if actions provided
        if actions is not None:
            action_influenced_predictions = self.action_conditional_predictor(
                context_representations, actions
            )
            # Combine predictions (weighted average)
            alpha = 0.7  # Weight for action-conditional predictions
            future_representations = alpha * action_influenced_predictions + (1 - alpha) * future_representations
        
        # Object detection and tracking
        object_states = self.object_detector(encoded_frames)
        
        return {
            'future_representations': future_representations,
            'uncertainties': uncertainties,
            'physics_properties': physics_properties,
            'object_states': object_states,
            'context_representations': context_representations
        }
    
    def plan_actions(self, video_frames, goal_representation, num_planning_steps=10):
        """
        Plan robot actions to achieve a goal representation
        
        Args:
            video_frames: [B, T, C, H, W] current video context
            goal_representation: [B, hidden_dim] target representation
            num_planning_steps: Number of planning iterations
            
        Returns:
            planned_actions: [B, predict_steps, action_dim] optimized action sequence
        """
        B = video_frames.shape[0]
        action_dim = 7  # 7-DOF robot arm
        
        # Initialize random action sequence
        planned_actions = torch.randn(B, self.prediction_horizon, action_dim) * 0.1
        planned_actions.requires_grad_(True)
        
        optimizer = torch.optim.Adam([planned_actions], lr=0.01)
        
        for step in range(num_planning_steps):
            optimizer.zero_grad()
            
            # Predict future given current actions
            predictions = self.forward(video_frames, planned_actions)
            predicted_representations = predictions['future_representations']
            
            # Calculate loss to goal
            final_prediction = predicted_representations[:, -1, :]  # Last predicted frame
            goal_loss = F.mse_loss(final_prediction, goal_representation)
            
            # Add physics consistency loss
            physics_loss = self._physics_consistency_loss(predictions)
            
            # Add action smoothness regularization
            smoothness_loss = torch.mean(torch.diff(planned_actions, dim=1) ** 2)
            
            total_loss = goal_loss + 0.1 * physics_loss + 0.01 * smoothness_loss
            total_loss.backward()
            
            optimizer.step()
            
            # Clip actions to reasonable bounds
            with torch.no_grad():
                planned_actions.clamp_(-1.0, 1.0)
        
        return planned_actions.detach()
    
    def _physics_consistency_loss(self, predictions):
        """Calculate physics consistency loss for realistic predictions"""
        
        physics_properties = predictions['physics_properties']
        
        # Example physics constraints
        # 1. Conservation of momentum
        momentum_loss = torch.mean(torch.abs(physics_properties['momentum_change']))
        
        # 2. Gravity effects
        gravity_loss = torch.mean(torch.abs(physics_properties['gravity_violation']))
        
        # 3. Object permanence
        permanence_loss = torch.mean(physics_properties['disappearance_penalty'])
        
        return momentum_loss + gravity_loss + permanence_loss

class PhysicsEstimator(nn.Module):
    """Estimates physical properties from video representations"""
    
    def __init__(self, hidden_dim):
        super().__init__()
        
        self.momentum_estimator = nn.Linear(hidden_dim, 3)  # 3D momentum
        self.gravity_estimator = nn.Linear(hidden_dim, 1)   # Gravity violation score
        self.stability_estimator = nn.Linear(hidden_dim, 1) # Stability score
        
    def forward(self, representations):
        B, seq_len, hidden_dim = representations.shape
        
        # Pool representations over sequence
        pooled = representations.mean(dim=1)
        
        momentum_change = self.momentum_estimator(pooled)
        gravity_violation = torch.abs(self.gravity_estimator(pooled))
        stability_score = torch.sigmoid(self.stability_estimator(pooled))
        
        # Calculate object permanence (simplified)
        temporal_consistency = torch.std(representations, dim=1).mean(dim=1, keepdim=True)
        disappearance_penalty = torch.relu(temporal_consistency - 0.5)  # Penalize large changes
        
        return {
            'momentum_change': momentum_change,
            'gravity_violation': gravity_violation,
            'stability_score': stability_score,
            'disappearance_penalty': disappearance_penalty
        }

class ActionConditionalPredictor(nn.Module):
    """Predicts future representations conditioned on robot actions"""
    
    def __init__(self, hidden_dim, action_dim=7):
        super().__init__()
        
        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim)
        )
        
        self.fusion_layer = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            batch_first=True
        )
        
        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.GELU(),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
    def forward(self, context_representations, actions):
        B, predict_steps, action_dim = actions.shape
        
        # Encode actions
        action_embeddings = self.action_encoder(actions)  # [B, predict_steps, hidden_dim]
        
        # Pool context for each prediction step
        context_summary = context_representations.mean(dim=1, keepdim=True)  # [B, 1, hidden_dim]
        context_expanded = context_summary.expand(-1, predict_steps, -1)  # [B, predict_steps, hidden_dim]
        
        # Fuse context with actions using attention
        fused_features, _ = self.fusion_layer(
            action_embeddings,  # Query
            context_expanded,   # Key
            context_expanded    # Value
        )
        
        # Predict future representations
        predictions = self.predictor(fused_features)
        
        return predictions

class SimpleObjectDetector(nn.Module):
    """Simple object detection and tracking for world model"""
    
    def __init__(self, hidden_dim, max_objects=10):
        super().__init__()
        
        self.max_objects = max_objects
        self.object_classifier = nn.Linear(hidden_dim, max_objects + 1)  # +1 for background
        self.position_regressor = nn.Linear(hidden_dim, max_objects * 4)  # bbox coords
        self.velocity_estimator = nn.Linear(hidden_dim, max_objects * 2)  # 2D velocity
        
    def forward(self, encoded_frames):
        B, T, patches_per_frame, hidden_dim = encoded_frames.shape
        
        # Pool spatial features for each frame
        frame_features = encoded_frames.mean(dim=2)  # [B, T, hidden_dim]
        
        # Detect objects
        object_probs = torch.softmax(self.object_classifier(frame_features), dim=-1)
        
        # Estimate positions and velocities
        positions = self.position_regressor(frame_features)
        positions = positions.view(B, T, self.max_objects, 4)  # [x, y, w, h] for each object
        
        velocities = self.velocity_estimator(frame_features)
        velocities = velocities.view(B, T, self.max_objects, 2)  # [vx, vy] for each object
        
        return {
            'object_probabilities': object_probs,
            'object_positions': positions,
            'object_velocities': velocities
        }

class RobotWorldModelPlanner:
    """
    High-level planner that uses V-JEPA world model for robot control
    """
    
    def __init__(self, world_model, action_dim=7):
        self.world_model = world_model
        self.action_dim = action_dim
        self.planning_horizon = 5
        
    def plan_manipulation_task(self, current_video, task_description):
        """
        Plan manipulation actions based on current video and task description
        
        Args:
            current_video: [B, T, C, H, W] current video observations
            task_description: Natural language task description
            
        Returns:
            planned_actions: [B, horizon, action_dim] planned action sequence
        """
        
        # In a full implementation, this would use language understanding
        # to convert task_description into goal representations
        
        # For demo: create goal representation based on simple task parsing
        goal_representation = self._parse_task_to_goal(task_description)
        
        # Use world model to plan actions
        planned_actions = self.world_model.plan_actions(
            current_video, 
            goal_representation, 
            num_planning_steps=15
        )
        
        # Validate plan for safety and feasibility
        validated_actions = self._validate_action_plan(planned_actions, current_video)
        
        return validated_actions
    
    def _parse_task_to_goal(self, task_description):
        """Parse task description into goal representation (simplified)"""
        
        # In practice, this would use sophisticated language understanding
        # For demo, create different goal representations for different tasks
        
        task_lower = task_description.lower()
        hidden_dim = self.world_model.hidden_dim
        
        if "pick up" in task_lower or "grasp" in task_lower:
            # Goal: object in gripper position
            goal = torch.randn(1, hidden_dim) * 0.1
            goal[0, :50] = 1.0  # High activation for "grasping" features
            
        elif "place" in task_lower or "put down" in task_lower:
            # Goal: object at target location
            goal = torch.randn(1, hidden_dim) * 0.1
            goal[0, 50:100] = 1.0  # High activation for "placement" features
            
        elif "pour" in task_lower:
            # Goal: pouring motion pattern
            goal = torch.randn(1, hidden_dim) * 0.1
            goal[0, 100:150] = 1.0  # High activation for "pouring" features
            
        else:
            # Generic manipulation goal
            goal = torch.randn(1, hidden_dim) * 0.1
            
        return goal
    
    def _validate_action_plan(self, planned_actions, current_video):
        """Validate planned actions for safety and feasibility"""
        
        B, horizon, action_dim = planned_actions.shape
        
        # Safety constraints
        # 1. Joint limits
        planned_actions = torch.clamp(planned_actions, -1.0, 1.0)
        
        # 2. Velocity limits
        max_velocity = 0.5  # rad/s or m/s
        velocities = torch.diff(planned_actions, dim=1)
        velocity_magnitudes = torch.norm(velocities, dim=-1, keepdim=True)
        
        # Scale down if velocities are too high
        velocity_scale = torch.clamp(max_velocity / (velocity_magnitudes + 1e-8), max=1.0)
        scaled_velocities = velocities * velocity_scale
        
        # Reconstruct actions from scaled velocities
        validated_actions = torch.zeros_like(planned_actions)
        validated_actions[:, 0, :] = planned_actions[:, 0, :]  # Keep initial position
        
        for t in range(1, horizon):
            validated_actions[:, t, :] = validated_actions[:, t-1, :] + scaled_velocities[:, t-1, :]
        
        # 3. Collision avoidance (simplified)
        # In practice, this would use the world model to predict collisions
        # For now, just ensure actions don't cause rapid movements near obstacles
        
        return validated_actions
    
    def predict_outcomes(self, current_video, action_sequence):
        """
        Predict outcomes of an action sequence using the world model
        
        Args:
            current_video: [B, T, C, H, W] current video context
            action_sequence: [B, horizon, action_dim] actions to evaluate
            
        Returns:
            Dict with predicted outcomes, success probability, and risks
        """
        
        # Use world model to predict future
        predictions = self.world_model(current_video, action_sequence)
        
        # Analyze predictions
        future_representations = predictions['future_representations']
        uncertainties = predictions['uncertainties']
        physics_properties = predictions['physics_properties']
        object_states = predictions['object_states']
        
        # Estimate success probability
        success_indicators = [
            1.0 - torch.mean(uncertainties),  # Lower uncertainty = higher confidence
            physics_properties['stability_score'].mean(),  # Higher stability = better
            1.0 - physics_properties['gravity_violation'].mean()  # Lower physics violations
        ]
        
        success_probability = torch.stack(success_indicators).mean()
        
        # Identify potential risks
        risks = []
        
        if torch.mean(uncertainties) > 0.7:
            risks.append("High prediction uncertainty")
        
        if physics_properties['gravity_violation'].mean() > 0.3:
            risks.append("Physics inconsistency detected")
            
        if physics_properties['stability_score'].mean() < 0.4:
            risks.append("Unstable object interactions predicted")
        
        return {
            'predicted_representations': future_representations,
            'success_probability': success_probability.item(),
            'risks': risks,
            'uncertainties': uncertainties,
            'object_trajectories': object_states
        }

# Training functions for V-JEPA world model
def train_vjepa_world_model(model, dataloader, num_epochs=100):
    """
    Training loop for V-JEPA world model
    """
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)
    
    model.train()
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        num_batches = 0
        
        for batch in dataloader:
            video_frames = batch['video']  # [B, T, C, H, W]
            actions = batch.get('actions', None)  # [B, T-1, action_dim] if available
            
            optimizer.zero_grad()
            
            # Split video into context and targets
            context_length = video_frames.shape[1] // 2
            context_video = video_frames[:, :context_length, :, :, :]
            target_video = video_frames[:, context_length:, :, :, :]
            
            # Forward pass
            predictions = model(context_video, actions)
            
            # Encode target frames for comparison
            target_encoded = model.encoder(target_video)
            target_frames = target_encoded.view(
                target_encoded.shape[0], 
                target_video.shape[1], 
                -1, 
                model.hidden_dim
            ).mean(dim=2)  # Average over patches
            
            # Calculate losses
            prediction_loss = F.mse_loss(
                predictions['future_representations'], 
                target_frames
            )
            
            physics_loss = model._physics_consistency_loss(predictions)
            
            # Uncertainty calibration loss
            uncertainty_loss = F.mse_loss(
                predictions['uncertainties'],
                torch.norm(predictions['future_representations'] - target_frames, dim=-1)
            )
            
            total_loss = prediction_loss + 0.1 * physics_loss + 0.05 * uncertainty_loss
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_loss += total_loss.item()
            num_batches += 1
        
        scheduler.step()
        
        avg_loss = epoch_loss / num_batches
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}")
        
        # Save checkpoint every 10 epochs
        if (epoch + 1) % 10 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, f'vjepa_checkpoint_epoch_{epoch+1}.pth')
    
    return model

# Example usage and demonstration
def demo_vjepa_robot_planning():
    """Demonstrate V-JEPA world model for robot planning"""
    
    # Initialize world model
    world_model = VJEPAWorldModel(
        input_channels=3,
        hidden_dim=512,
        prediction_horizon=5
    )
    
    # Initialize planner
    planner = RobotWorldModelPlanner(world_model)
    
    print(f"Initialized V-JEPA world model with {sum(p.numel() for p in world_model.parameters()):,} parameters")
    
    # Create demo video data
    batch_size = 2
    sequence_length = 8
    video_frames = torch.randn(batch_size, sequence_length, 3, 224, 224)
    
    # Demo 1: Plan manipulation task
    task_description = "Pick up the red cube and place it in the blue box"
    
    with torch.no_grad():
        planned_actions = planner.plan_manipulation_task(video_frames, task_description)
        print(f"Planned actions shape: {planned_actions.shape}")
        print(f"Action range: [{planned_actions.min():.3f}, {planned_actions.max():.3f}]")
        
        # Demo 2: Predict outcomes
        outcomes = planner.predict_outcomes(video_frames, planned_actions)
        print(f"Predicted success probability: {outcomes['success_probability']:.1%}")
        print(f"Identified risks: {outcomes['risks']}")
        
        # Demo 3: World model forward pass
        predictions = world_model(video_frames)
        print(f"Future representations shape: {predictions['future_representations'].shape}")
        print(f"Average prediction uncertainty: {predictions['uncertainties'].mean():.3f}")
        
    return world_model, planner

if __name__ == "__main__":
    demo_vjepa_robot_planning()
</pre>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>🎯 Section 5: Key Takeaways and Production Deployment</h2>

    <div class="step">
      <h3>💡 Implementation Principles and Challenges</h3>
      
      <div class="tabs">
        <div class="tab active" onclick="switchTab('principles', this)">🏗️ Core Principles</div>
        <div class="tab" onclick="switchTab('challenges', this)">⚠️ Key Challenges</div>
        <div class="tab" onclick="switchTab('deployment', this)">🚀 Production Deployment</div>
        <div class="tab" onclick="switchTab('metrics', this)">📊 Performance Metrics</div>
      </div>

      <div id="principles" class="tab-content active">
        <div class="metric-grid">
          <div class="metric-card">
            <div class="metric-value">6</div>
            <div class="metric-label">Sensor Modalities</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">5</div>
            <div class="metric-label">Constitutional Principles</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">8</div>
            <div class="metric-label">Robot Agents</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">200Hz</div>
            <div class="metric-label">Control Frequency</div>
          </div>
        </div>

        <div class="success">
          <h4>🎯 Core Implementation Principles</h4>
          
          <p><strong>1. Multi-Modal Sensor Fusion:</strong> Integrate vision, audio, haptic, proprioceptive, LiDAR, and IMU data through cross-attention mechanisms for comprehensive environmental understanding.</p>
          
          <p><strong>2. Constitutional AI Safety:</strong> Implement principled decision-making with harm prevention, beneficial action, consent/autonomy, environmental responsibility, and transparency principles.</p>
          
          <p><strong>3. Distributed Multi-Agent Coordination:</strong> Enable natural language orchestration of specialized robot teams with role-based task allocation and consensus mechanisms.</p>
          
          <p><strong>4. Predictive World Models:</strong> Use V-JEPA architectures to build physics-aware world models for action planning and outcome prediction.</p>
          
          <p><strong>5. Real-Time Safety Monitoring:</strong> Continuous safety score monitoring, emergency stop capabilities, and constitutional principle evaluation for every action.</p>
        </div>
      </div>

      <div id="challenges" class="tab-content">
        <div class="danger">
          <h4>⚠️ Key Implementation Challenges</h4>
          
          <p><strong>Computational Complexity:</strong> Multi-modal fusion and world model inference require significant computational resources. Edge deployment needs model optimization and quantization.</p>
          
          <p><strong>Sensor Synchronization:</strong> Achieving microsecond-level synchronization across 6+ sensor modalities while maintaining real-time performance at 200Hz control rates.</p>
          
          <p><strong>Constitutional Consistency:</strong> Ensuring constitutional principles remain consistent across different operational contexts and edge cases not seen during training.</p>
          
          <p><strong>Multi-Agent Consensus:</strong> Achieving reliable consensus in distributed systems with communication latency, partial failures, and conflicting objectives.</p>
          
          <p><strong>World Model Accuracy:</strong> V-JEPA predictions must be accurate enough for safe physical actions, especially for novel objects and scenarios.</p>
        </div>

        <div class="warning">
          <h4>🛠️ Mitigation Strategies</h4>
          
          <p><strong>Hierarchical Processing:</strong> Use different processing frequencies (1kHz for safety, 200Hz for control, 30Hz for planning) to balance performance and accuracy.</p>
          
          <p><strong>Graceful Degradation:</strong> Design systems to operate safely even when some sensors fail or multi-agent consensus cannot be reached.</p>
          
          <p><strong>Continuous Learning:</strong> Implement online learning to adapt constitutional principles and world models to new scenarios while maintaining safety.</p>
          
          <p><strong>Hardware Acceleration:</strong> Leverage specialized AI chips (Jetson Thor, custom ASICs) for multi-modal processing and real-time inference.</p>
        </div>
      </div>

      <div id="deployment" class="tab-content">
        <div class="info">
          <h4>🚀 Production Deployment Considerations</h4>
          
          <p><strong>Hardware Requirements:</strong></p>
          <ul style="margin: 10px 0; padding-left: 20px;">
            <li>NVIDIA Jetson Thor (2070 TFLOPS) or equivalent for edge deployment</li>
            <li>Multi-sensor arrays with hardware timestamping capabilities</li>
            <li>Low-latency communication infrastructure (< 1ms between robots)</li>
            <li>Redundant safety systems and emergency stop mechanisms</li>
          </ul>
          
          <p><strong>Software Architecture:</strong></p>
          <ul style="margin: 10px 0; padding-left: 20px;">
            <li>Real-time operating system (RT-Linux) for deterministic control</li>
            <li>Containerized deployment with Kubernetes orchestration</li>
            <li>Message passing middleware (ROS2, ZeroMQ) for multi-agent communication</li>
            <li>Continuous monitoring and logging for constitutional compliance</li>
          </ul>
          
          <p><strong>Scalability Patterns:</strong></p>
          <ul style="margin: 10px 0; padding-left: 20px;">
            <li>Hierarchical control: Edge inference + cloud planning</li>
            <li>Model serving: Centralized large models + distributed lightweight agents</li>
            <li>Data pipeline: Real-time sensor fusion + offline training updates</li>
          </ul>
        </div>
      </div>

      <div id="metrics" class="tab-content">
        <div class="metric-grid">
          <div class="metric-card">
            <div class="metric-value">< 10ms</div>
            <div class="metric-label">Action Inference Latency</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">99.9%</div>
            <div class="metric-label">Safety System Uptime</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">95%</div>
            <div class="metric-label">Multi-Agent Consensus Rate</div>
          </div>
          <div class="metric-card">
            <div class="metric-value">85%</div>
            <div class="metric-label">Task Success Rate</div>
          </div>
        </div>

        <div class="success">
          <h4>📊 Key Performance Metrics</h4>
          
          <p><strong>Safety Metrics:</strong> Constitutional principle compliance rate, safety system response time, emergency stop effectiveness, harm prevention accuracy.</p>
          
          <p><strong>Performance Metrics:</strong> Task completion rate, action precision, multi-modal fusion accuracy, world model prediction error.</p>
          
          <p><strong>Coordination Metrics:</strong> Inter-agent communication latency, consensus achievement rate, task allocation efficiency, load balancing effectiveness.</p>
          
          <p><strong>System Metrics:</strong> Computational resource utilization, sensor synchronization accuracy, real-time deadline adherence, fault tolerance.</p>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>🌟 The Future of Embodied Intelligence</h3>
      
      <div class="breakthrough-highlight">
        The convergence of multi-modal VLAs, constitutional AI, multi-agent coordination, and predictive world models represents the foundation for truly intelligent robotic systems that can safely operate in complex, dynamic environments.
      </div>

      <div class="interactive-demo">
        <div class="demo-title">🔮 Future Capabilities Preview</div>
        <p><strong>Explore the potential of advanced VLA systems:</strong></p>

        <div class="controls">
          <div class="control-group">
            <label>Capability Domain:</label>
            <select id="capabilityDomain">
              <option value="household" selected>Household Robotics</option>
              <option value="manufacturing">Smart Manufacturing</option>
              <option value="healthcare">Healthcare Assistance</option>
              <option value="construction">Construction Automation</option>
              <option value="exploration">Space/Ocean Exploration</option>
            </select>
          </div>
          <div class="control-group">
            <label>Technology Timeline:</label>
            <select id="technologyTimeline">
              <option value="current" selected>Current (2024-2025)</option>
              <option value="near">Near-term (2025-2027)</option>
              <option value="medium">Medium-term (2027-2030)</option>
              <option value="long">Long-term (2030+)</option>
            </select>
          </div>
        </div>

        <button onclick="exploreFutureCapabilities()" class="primary">🚀 Explore Future</button>
        <div id="futureCapabilities"></div>
      </div>

      <div class="success">
        <h4>✅ What You've Learned</h4>
        
        <p>You now understand the cutting-edge of VLA and multi-agent robotics:</p>
        
        <p><strong>Multi-Modal Integration:</strong> How to fuse 6+ sensor modalities through transformer architectures for comprehensive environmental understanding.</p>
        
        <p><strong>Constitutional AI Safety:</strong> Implementing principled decision-making frameworks that ensure robots act according to ethical guidelines and safety constraints.</p>
        
        <p><strong>Multi-Agent Coordination:</strong> Natural language orchestration of robot teams with role specialization, consensus mechanisms, and distributed decision-making.</p>
        
        <p><strong>World Model Integration:</strong> V-JEPA architectures for physics-aware prediction and planning in complex environments.</p>
        
        <p><strong>Production Deployment:</strong> Real-world implementation strategies, hardware requirements, and performance metrics for advanced robotic systems.</p>
      </div>
    </div>

    <div class="breakthrough-highlight">
      🎓 Ready for the next frontier? Explore <strong><a href="path-to-agi.html" style="color: #fff; text-decoration: underline;">The Path to AGI</a></strong> to understand how these technologies contribute to the development of Artificial General Intelligence.
    </div>
  </div>

  <script>
    // Global variables for demos and interactions
    let selectedSensor = 'vision';
    let selectedRole = null;
    let multiAgentSystem = {
      robots: {},
      missions: {},
      active: false
    };
    
    // Sensor data with detailed specifications
    const sensorDatabase = {
      vision: {
        title: "👁️ Vision Processing Pipeline",
        description: "RGB-D cameras provide rich visual information with depth perception",
        specs: [
          "Resolution: 1920x1080 RGB + 640x480 depth",
          "Frame rate: 30 FPS synchronized",
          "Depth range: 0.1m - 10m",
          "Field of view: 87° horizontal, 58° vertical",
          "Processing: Real-time object detection, semantic segmentation",
          "Latency: <16ms end-to-end"
        ],
        integration: "Vision tokens are processed through ViT encoder, generating 196 patch embeddings (14x14 grid). Depth information adds spatial reasoning capabilities for manipulation tasks."
      },
      audio: {
        title: "🎵 Audio Processing Pipeline", 
        description: "Multi-microphone array for 3D sound localization and speech recognition",
        specs: [
          "Microphones: 4-channel circular array",
          "Sample rate: 48kHz, 24-bit",
          "Frequency response: 20Hz - 20kHz",
          "SNR: >65dB",
          "Processing: Real-time ASR, sound classification, source localization",
          "Latency: <50ms speech-to-text"
        ],
        integration: "Audio features processed through Whisper-style encoder, generating temporal audio tokens. Enables voice commands and environmental sound awareness."
      },
      haptic: {
        title: "✋ Haptic/Force Feedback",
        description: "Force/torque sensors provide tactile feedback for manipulation",
        specs: [
          "Force range: ±300N in X, Y, Z axes",
          "Torque range: ±50Nm around X, Y, Z axes", 
          "Resolution: 0.1N force, 0.01Nm torque",
          "Sampling rate: 1kHz",
          "Processing: Contact detection, slip detection, texture recognition",
          "Latency: <1ms force feedback"
        ],
        integration: "Force/torque data encoded through MLP or LSTM, providing tactile understanding for precise manipulation and safe human interaction."
      },
      proprioception: {
        title: "🤖 Proprioceptive Sensing",
        description: "Internal joint state and kinematic awareness",
        specs: [
          "Joint encoders: 7 DOF arm + gripper",
          "Position resolution: 0.1° angular accuracy",
          "Velocity estimation: Real-time joint velocities",
          "Sampling rate: 1kHz control loop",
          "Processing: Forward kinematics, workspace monitoring",
          "Latency: <1ms proprioceptive feedback"
        ],
        integration: "Joint states (positions + velocities) encoded through linear layers, providing body awareness and kinematic constraints for action planning."
      },
      lidar: {
        title: "📡 LiDAR Spatial Mapping",
        description: "High-precision 3D point cloud generation for navigation",
        specs: [
          "Technology: Solid-state LiDAR",
          "Range: 0.1m - 100m",
          "Point cloud rate: 1.3M points/second",
          "Accuracy: ±2cm at 10m",
          "Processing: SLAM, obstacle detection, path planning",
          "Latency: <10ms point cloud update"
        ],
        integration: "Point clouds processed through PointNet-style encoder, generating spatial tokens for navigation and obstacle avoidance in mobile robots."
      },
      imu: {
        title: "🧭 Inertial Measurement Unit",
        description: "Motion sensing for balance and orientation tracking",
        specs: [
          "Accelerometer: ±16g, 3-axis",
          "Gyroscope: ±2000°/s, 3-axis",
          "Magnetometer: ±4900μT, 3-axis",
          "Sampling rate: 1kHz",
          "Processing: Orientation estimation, motion prediction",
          "Latency: <1ms inertial feedback"
        ],
        integration: "IMU data processed through temporal networks, providing motion context and stability information for dynamic manipulation tasks."
      }
    };

    // Robot configuration for multi-agent system
    const robotAgents = {
      leader: { id: 'leader_01', role: 'Leader', x: 300, y: 200, status: 'Planning mission', color: '#dc3545', emoji: '👑' },
      coordinator: { id: 'coord_01', role: 'Coordinator', x: 300, y: 350, status: 'Monitoring', color: '#fd7e14', emoji: '📡' },
      worker1: { id: 'worker_01', role: 'Worker', x: 150, y: 150, status: 'Awaiting orders', color: '#28a745', emoji: '🔧' },
      worker2: { id: 'worker_02', role: 'Worker', x: 450, y: 150, status: 'Awaiting orders', color: '#28a745', emoji: '🔧' },
      worker3: { id: 'worker_03', role: 'Worker', x: 150, y: 350, status: 'Idle', color: '#28a745', emoji: '🔧' },
      worker4: { id: 'worker_04', role: 'Worker', x: 450, y: 350, status: 'Idle', color: '#28a745', emoji: '🔧' },
      scout: { id: 'scout_01', role: 'Scout', x: 100, y: 250, status: 'Patrolling', color: '#007bff', emoji: '🔍' },
      specialist: { id: 'specialist_01', role: 'Specialist', x: 500, y: 250, status: 'Ready', color: '#6f42c1', emoji: '🎯' }
    };

    // Initialize page
    document.addEventListener('DOMContentLoaded', function() {
      selectSensor('vision', document.querySelector('.sensor-card.active'));
      initializeMultiAgentArena();
      initializeWorldModel();
      
      // Start safety monitoring
      startSafetyMonitoring();
      
      console.log('Advanced VLA & Multi-Agent Robotics tutorial initialized');
    });

    // Section 1: Multi-Modal Sensor Functions
    function selectSensor(sensorType, element) {
      // Update UI
      document.querySelectorAll('.sensor-card').forEach(card => card.classList.remove('active'));
      element.classList.add('active');
      selectedSensor = sensorType;

      // Display sensor details
      const details = sensorDatabase[sensorType];
      const detailsDiv = document.getElementById('sensorDetails');
      
      if (detailsDiv && details) {
        detailsDiv.innerHTML = `
          <div class="success" style="margin-top: 20px;">
            <h4>${details.title}</h4>
            <p><strong>${details.description}</strong></p>
            
            <div style="margin: 15px 0;">
              <strong>Technical Specifications:</strong>
              <ul style="margin: 8px 0; padding-left: 20px;">
                ${details.specs.map(spec => `<li style="font-size: 13px; margin: 4px 0;">${spec}</li>`).join('')}
              </ul>
            </div>
            
            <p><strong>🔗 VLA Integration:</strong> ${details.integration}</p>
          </div>
        `;
      }
    }

    function designMultiModalVLA() {
      const visionEncoder = document.getElementById('visionEncoder').value;
      const audioProcessor = document.getElementById('audioProcessor').value;
      const hapticIntegration = document.getElementById('hapticIntegration').value;
      const fusionStrategy = document.getElementById('fusionStrategy').value;
      const modelSize = document.getElementById('modelSize').value;
      const targetRobot = document.getElementById('targetRobot').value;

      // Calculate model specifications
      const modelSpecs = {
        small: { params: 1.5, memory: 6, latency: 15, tflops: 3 },
        medium: { params: 7, memory: 14, latency: 45, tflops: 15 },
        large: { params: 13, memory: 26, latency: 85, tflops: 35 },
        xlarge: { params: 30, memory: 60, latency: 200, tflops: 80 }
      };

      const spec = modelSpecs[modelSize];
      
      // Fusion strategy impact
      const fusionFactors = {
        cross_attention: { complexity: 1.3, accuracy: 0.95, latency: 1.2 },
        early_fusion: { complexity: 0.8, accuracy: 0.85, latency: 0.9 },
        late_fusion: { complexity: 1.0, accuracy: 0.90, latency: 1.0 },
        hierarchical: { complexity: 1.5, accuracy: 0.97, latency: 1.4 }
      };

      const fusion = fusionFactors[fusionStrategy];
      
      // Robot-specific adjustments
      const robotFactors = {
        franka: { dof: 7, complexity: 1.0, description: "7-DOF manipulation" },
        ur5: { dof: 6, complexity: 0.9, description: "Industrial 6-DOF arm" },
        aloha: { dof: 14, complexity: 1.6, description: "Bimanual coordination" },
        mobile: { dof: 10, complexity: 1.4, description: "Mobile base + arm" },
        humanoid: { dof: 25, complexity: 2.2, description: "Full-body humanoid" }
      };

      const robot = robotFactors[targetRobot];

      // Calculate final specifications
      const finalParams = (spec.params * fusion.complexity * robot.complexity).toFixed(1);
      const finalMemory = (spec.memory * fusion.complexity * robot.complexity).toFixed(1);
      const finalLatency = (spec.latency * fusion.latency).toFixed(1);
      const expectedAccuracy = (fusion.accuracy * 100).toFixed(1);

      const resultDiv = document.getElementById('architectureDesign');
      if (resultDiv) {
        resultDiv.innerHTML = `
          <div class="metric-grid">
            <div class="metric-card">
              <div class="metric-value">${finalParams}B</div>
              <div class="metric-label">Parameters</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${finalMemory}GB</div>
              <div class="metric-label">Memory Required</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${finalLatency}ms</div>
              <div class="metric-label">Inference Latency</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${expectedAccuracy}%</div>
              <div class="metric-label">Expected Accuracy</div>
            </div>
          </div>

          <div class="success">
            <h4>🎯 Architecture Summary</h4>
            <p><strong>Target:</strong> ${robot.description} (${robot.dof} DOF)</p>
            <p><strong>Vision:</strong> ${visionEncoder.toUpperCase()} encoder for visual processing</p>
            <p><strong>Audio:</strong> ${audioProcessor.charAt(0).toUpperCase() + audioProcessor.slice(1)} for sound understanding</p>
            <p><strong>Haptic:</strong> ${hapticIntegration.toUpperCase()} for force/tactile integration</p>
            <p><strong>Fusion:</strong> ${fusionStrategy.replace('_', '-').toUpperCase()} multi-modal fusion</p>
            <p><strong>Recommendation:</strong> ${finalParams > 15 ? '🔴 Requires high-end hardware (H100/Thor)' : finalParams > 7 ? '🟡 Suitable for edge deployment (Jetson Orin/Thor)' : '🟢 Runs on consumer hardware'}</p>
          </div>

          <div class="info">
            <h4>🔧 Implementation Considerations</h4>
            <p><strong>Training Data:</strong> Requires ${Math.round(finalParams * 100)}K+ diverse robot demonstrations</p>
            <p><strong>Training Time:</strong> ~${Math.round(finalParams * 2)} days on 8x H100 GPUs</p>
            <p><strong>Deployment:</strong> ${finalMemory > 32 ? 'Cloud inference recommended' : 'Edge deployment feasible'}</p>
            <p><strong>Real-time Control:</strong> ${finalLatency < 50 ? '✅ Suitable for real-time robot control' : '⚠️ May require optimization for real-time use'}</p>
          </div>
        `;
      }
    }

    // Section 2: Constitutional AI Functions
    function evaluateConstitutionalResponse() {
      const scenario = document.getElementById('scenarioType').value;
      const actionRequest = document.getElementById('actionRequest').value;
      const humanPresent = document.getElementById('humanPresent').value;
      const riskLevel = document.getElementById('riskLevel').value;
      const urgency = document.getElementById('urgencyLevel').value;

      const evaluation = performConstitutionalEvaluation(actionRequest, {
        scenario,
        humanPresent: humanPresent === 'yes',
        riskLevel,
        urgency
      });

      displayConstitutionalEvaluation(evaluation);
    }

    function performConstitutionalEvaluation(actionRequest, context) {
      const violations = [];
      const concerns = [];
      let riskScore = 0.0;

      const actionLower = actionRequest.toLowerCase();
      
      // Harm Prevention Analysis
      if (actionLower.includes('heavy') && context.humanPresent) {
        violations.push('Harm Prevention: Moving heavy objects near humans increases injury risk');
        riskScore += 0.3;
      }
      
      if (actionLower.includes('fast') || actionLower.includes('quickly')) {
        concerns.push('Harm Prevention: Fast movements may compromise safety');
        riskScore += 0.2;
      }

      if (context.riskLevel === 'high' || context.riskLevel === 'critical') {
        violations.push('Harm Prevention: Action classified as high/critical risk');
        riskScore += 0.4;
      }

      // Beneficial Action Analysis
      if (actionLower.includes('even though') || actionLower.includes('despite')) {
        concerns.push('Beneficial Action: Action may not prioritize safety and wellbeing');
        riskScore += 0.2;
      }

      // Consent/Autonomy Analysis
      if (context.humanPresent && !actionLower.includes('permission') && !actionLower.includes('consent')) {
        concerns.push('Consent/Autonomy: No explicit consent mentioned for action affecting human space');
        riskScore += 0.15;
      }

      // Environmental Responsibility
      if (actionLower.includes('waste') || actionLower.includes('throw away')) {
        concerns.push('Environmental Responsibility: Action may involve unnecessary waste');
        riskScore += 0.1;
      }

      // Determine response
      let response, reasoning, alternatives;
      
      if (riskScore > 0.6 || violations.length > 2) {
        response = 'REFUSE';
        reasoning = 'Action violates critical safety principles and poses unacceptable risk';
        alternatives = [
          'Wait for human to move to safe distance',
          'Use assistance from another robot',
          'Break task into smaller, safer steps',
          'Request explicit permission and safety confirmation'
        ];
      } else if (riskScore > 0.3 || violations.length > 0) {
        response = 'MODIFY';
        reasoning = 'Action has safety concerns but can proceed with modifications';
        alternatives = [
          'Reduce movement speed for safety',
          'Announce action before proceeding',
          'Establish safety perimeter',
          'Use additional sensors for monitoring'
        ];
      } else if (concerns.length > 0) {
        response = 'REQUEST_CLARIFICATION';
        reasoning = 'Action has minor concerns requiring clarification';
        alternatives = [
          'Confirm human is aware of action',
          'Verify safety protocols are active',
          'Request explicit approval to proceed'
        ];
      } else {
        response = 'APPROVE';
        reasoning = 'Action aligns with constitutional principles and safety requirements';
        alternatives = ['Proceed as requested'];
      }

      const overallScore = Math.max(0.0, 1.0 - riskScore);

      return {
        response,
        reasoning,
        riskScore,
        overallScore,
        violations,
        concerns,
        alternatives,
        principleScores: calculatePrincipleScores(actionRequest, context)
      };
    }

    function calculatePrincipleScores(actionRequest, context) {
      const scores = {};
      const principles = ['harm_prevention', 'beneficial_action', 'consent_autonomy', 'environmental_responsibility', 'transparency'];
      
      principles.forEach(principle => {
        let score = 1.0;
        const actionLower = actionRequest.toLowerCase();
        
        switch(principle) {
          case 'harm_prevention':
            if (context.humanPresent && actionLower.includes('heavy')) score -= 0.4;
            if (context.riskLevel === 'high') score -= 0.3;
            if (context.riskLevel === 'critical') score -= 0.6;
            break;
            
          case 'beneficial_action':
            if (actionLower.includes('even though')) score -= 0.3;
            if (context.urgency === 'low') score -= 0.1;
            break;
            
          case 'consent_autonomy':
            if (context.humanPresent && !actionLower.includes('permission')) score -= 0.2;
            break;
            
          case 'environmental_responsibility':
            if (actionLower.includes('waste')) score -= 0.3;
            break;
            
          case 'transparency':
            if (!actionLower.includes('explain') && context.riskLevel !== 'low') score -= 0.1;
            break;
        }
        
        scores[principle] = Math.max(0.0, score);
      });
      
      return scores;
    }

    function displayConstitutionalEvaluation(evaluation) {
      const resultDiv = document.getElementById('constitutionalEvaluation');
      if (!resultDiv) return;
      
      const responseColors = {
        'APPROVE': '#28a745',
        'MODIFY': '#ffc107', 
        'REFUSE': '#dc3545',
        'REQUEST_CLARIFICATION': '#17a2b8'
      };
      
      const responseColor = responseColors[evaluation.response];
      
      let principleHtml = '';
      Object.entries(evaluation.principleScores).forEach(([principle, score]) => {
        const scoreColor = score > 0.8 ? '#28a745' : score > 0.6 ? '#ffc107' : score > 0.4 ? '#fd7e14' : '#dc3545';
        
        principleHtml += `
          <div class="metric-card">
            <div class="metric-value" style="color: ${scoreColor}">${(score * 100).toFixed(0)}%</div>
            <div class="metric-label">${principle.replace('_', ' ').toUpperCase()}</div>
          </div>
        `;
      });

      resultDiv.innerHTML = `
        <div style="border: 3px solid ${responseColor}; border-radius: 12px; padding: 20px; margin: 15px 0;">
          <h4 style="color: ${responseColor}; text-align: center; margin-bottom: 20px;">
            🤖 Constitutional AI Decision: ${evaluation.response}
          </h4>
          
          <div class="metric-grid">
            <div class="metric-card">
              <div class="metric-value" style="color: ${responseColor}">${evaluation.response}</div>
              <div class="metric-label">Decision</div>
            </div>
            <div class="metric-card">
              <div class="metric-value" style="color: ${evaluation.riskScore > 0.6 ? '#dc3545' : evaluation.riskScore > 0.3 ? '#ffc107' : '#28a745'}">${(evaluation.riskScore * 100).toFixed(0)}%</div>
              <div class="metric-label">Risk Score</div>
            </div>
            <div class="metric-card">
              <div class="metric-value" style="color: ${evaluation.overallScore > 0.8 ? '#28a745' : evaluation.overallScore > 0.6 ? '#ffc107' : '#dc3545'}">${(evaluation.overallScore * 100).toFixed(0)}%</div>
              <div class="metric-label">Safety Score</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${evaluation.violations.length + evaluation.concerns.length}</div>
              <div class="metric-label">Issues Found</div>
            </div>
          </div>

          <div style="margin: 20px 0;">
            <h5>📊 Principle Compliance Scores:</h5>
            <div class="metric-grid">
              ${principleHtml}
            </div>
          </div>

          <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 15px 0;">
            <h5>🧠 Reasoning:</h5>
            <p>${evaluation.reasoning}</p>
          </div>

          ${evaluation.violations.length > 0 ? `
            <div style="background: #f8d7da; border: 1px solid #f5c6cb; padding: 15px; border-radius: 8px; margin: 15px 0;">
              <h5 style="color: #721c24;">⚠️ Principle Violations:</h5>
              <ul style="margin: 8px 0; padding-left: 20px;">
                ${evaluation.violations.map(v => `<li style="color: #721c24; font-size: 14px; margin: 4px 0;">${v}</li>`).join('')}
              </ul>
            </div>
          ` : ''}

          ${evaluation.concerns.length > 0 ? `
            <div style="background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 8px; margin: 15px 0;">
              <h5 style="color: #856404;">💭 Concerns Identified:</h5>
              <ul style="margin: 8px 0; padding-left: 20px;">
                ${evaluation.concerns.map(c => `<li style="color: #856404; font-size: 14px; margin: 4px 0;">${c}</li>`).join('')}
              </ul>
            </div>
          ` : ''}

          <div style="background: #d4edda; border: 1px solid #c3e6cb; padding: 15px; border-radius: 8px; margin: 15px 0;">
            <h5 style="color: #155724;">✅ Recommended Actions:</h5>
            <ul style="margin: 8px 0; padding-left: 20px;">
              ${evaluation.alternatives.map(alt => `<li style="color: #155724; font-size: 14px; margin: 4px 0;">${alt}</li>`).join('')}
            </ul>
          </div>
        </div>
      `;
    }

    // Section 3: Multi-Agent Functions
    function initializeMultiAgentArena() {
      const arena = document.getElementById('multiAgentArena');
      if (!arena) return;

      arena.innerHTML = '';

      // Create robot agents
      Object.values(robotAgents).forEach(agent => {
        const robotElement = document.createElement('div');
        robotElement.className = `robot-agent robot-${agent.role.toLowerCase()}`;
        robotElement.style.left = agent.x + 'px';
        robotElement.style.top = agent.y + 'px';
        robotElement.style.backgroundColor = agent.color;
        robotElement.innerHTML = agent.emoji;
        robotElement.title = `${agent.role} - ${agent.status}`;
        robotElement.onclick = () => selectRobotAgent(agent.id);
        
        arena.appendChild(robotElement);
      });

      // Create status display
      const statusDiv = document.createElement('div');
      statusDiv.className = 'agent-status';
      statusDiv.id = 'agentStatus';
      statusDiv.innerHTML = 'Click on robots to see their status and assign tasks';
      arena.appendChild(statusDiv);

      drawCommunicationLines();
    }

    function selectRobotAgent(agentId) {
      const agent = Object.values(robotAgents).find(r => r.id === agentId);
      const statusDiv = document.getElementById('agentStatus');
      
      if (agent && statusDiv) {
        statusDiv.innerHTML = `
          <strong>${agent.role} Robot (${agentId})</strong><br>
          Status: ${agent.status}<br>
          Position: (${agent.x}, ${agent.y})<br>
          <small>Role: ${getRoleDescription(agent.role)}</small>
        `;

        // Highlight selected robot
        document.querySelectorAll('.robot-agent').forEach(el => el.style.transform = 'scale(1)');
        const selectedElement = document.querySelector(`[onclick*="${agentId}"]`);
        if (selectedElement) {
          selectedElement.style.transform = 'scale(1.2)';
        }
      }
    }

    function getRoleDescription(role) {
      const descriptions = {
        'Leader': 'Plans missions and coordinates team strategy',
        'Worker': 'Executes tasks and follows team instructions', 
        'Scout': 'Explores environment and gathers intelligence',
        'Specialist': 'Handles complex technical operations',
        'Coordinator': 'Manages communication and resource allocation'
      };
      return descriptions[role] || 'General purpose robot agent';
    }

    function drawCommunicationLines() {
      const arena = document.getElementById('multiAgentArena');
      if (!arena) return;

      // Remove existing lines
      document.querySelectorAll('.communication-line').forEach(line => line.remove());

      // Define communication patterns
      const communications = [
        { from: 'leader', to: 'coordinator' },
        { from: 'coordinator', to: 'worker1' },
        { from: 'coordinator', to: 'worker2' },
        { from: 'coordinator', to: 'worker3' },
        { from: 'coordinator', to: 'worker4' },
        { from: 'scout', to: 'leader' },
        { from: 'specialist', to: 'leader' }
      ];

      communications.forEach(comm => {
        const fromAgent = robotAgents[comm.from];
        const toAgent = robotAgents[comm.to];
        
        if (fromAgent && toAgent) {
          const line = document.createElement('div');
          line.className = 'communication-line';
          
          const dx = toAgent.x - fromAgent.x;
          const dy = toAgent.y - fromAgent.y;
          const length = Math.sqrt(dx * dx + dy * dy);
          const angle = Math.atan2(dy, dx) * 180 / Math.PI;
          
          line.style.left = (fromAgent.x + 20) + 'px';
          line.style.top = (fromAgent.y + 20) + 'px';
          line.style.width = length + 'px';
          line.style.transform = `rotate(${angle}deg)`;
          
          arena.appendChild(line);
        }
      });
    }

    function selectRole(roleType, element) {
      document.querySelectorAll('.role-card').forEach(card => card.classList.remove('selected'));
      element.classList.add('selected');
      selectedRole = roleType;

      const roleDetails = document.getElementById('roleDetails');
      if (roleDetails) {
        const roleDescriptions = {
          leader: {
            capabilities: ["Strategic planning", "Mission coordination", "Resource allocation", "Conflict resolution"],
            responsibilities: ["Overall mission success", "Team coordination", "Risk assessment", "Decision making"],
            specialization: "High-level planning and coordination with broad situational awareness"
          },
          coordinator: {
            capabilities: ["Task scheduling", "Communication hub", "Load balancing", "Status monitoring"],
            responsibilities: ["Efficient task distribution", "Inter-robot communication", "Resource optimization", "Progress tracking"],
            specialization: "Operational coordination and real-time task management"
          },
          worker: {
            capabilities: ["Object manipulation", "Task execution", "Environmental interaction", "Status reporting"],
            responsibilities: ["Task completion", "Quality execution", "Progress updates", "Safety compliance"],
            specialization: "Direct task execution with versatile manipulation capabilities"
          },
          scout: {
            capabilities: ["Environment mapping", "Pathfinding", "Anomaly detection", "Reconnaissance"],
            responsibilities: ["Environmental assessment", "Route planning", "Hazard detection", "Intelligence gathering"],
            specialization: "Environmental awareness and exploration with advanced sensing"
          },
          specialist: {
            capabilities: ["Precision work", "Quality control", "Technical analysis", "Problem solving"],
            responsibilities: ["Complex task execution", "Quality assurance", "Technical expertise", "Problem resolution"],
            specialization: "High-precision work and specialized technical operations"
          }
        };

        const role = roleDescriptions[roleType];
        roleDetails.innerHTML = `
          <div class="success" style="margin-top: 20px;">
            <h4>🎭 ${roleType.charAt(0).toUpperCase() + roleType.slice(1)} Robot Specialization</h4>
            
            <p><strong>Core Specialization:</strong> ${role.specialization}</p>
            
            <div style="margin: 15px 0;">
              <strong>Key Capabilities:</strong>
              <ul style="margin: 8px 0; padding-left: 20px;">
                ${role.capabilities.map(cap => `<li style="font-size: 14px; margin: 4px 0;">${cap}</li>`).join('')}
              </ul>
            </div>
            
            <div style="margin: 15px 0;">
              <strong>Primary Responsibilities:</strong>
              <ul style="margin: 8px 0; padding-left: 20px;">
                ${role.responsibilities.map(resp => `<li style="font-size: 14px; margin: 4px 0;">${resp}</li>`).join('')}
              </ul>
            </div>
          </div>
        `;
      }
    }

    function executeTeamMission() {
      const missionType = document.getElementById('missionType').value;
      const teamCommand = document.getElementById('teamCommand').value;
      const coordinationStrategy = document.getElementById('coordinationStrategy').value;

      multiAgentSystem.active = true;
      
      // Update robot statuses
      updateRobotStatuses(missionType);
      
      // Display mission execution
      displayMissionExecution(missionType, teamCommand, coordinationStrategy);
      
      // Animate communication
      animateCommunications();
      
      // Update safety monitoring
      updateSafetyMetrics();
      
      // Add communication logs
      addCommunicationMessage("Leader", `Mission initiated: ${teamCommand}`);
      addCommunicationMessage("Coordinator", `Executing ${coordinationStrategy} coordination strategy`);
      addCommunicationMessage("System", `${Object.keys(robotAgents).length} robots coordinating for ${missionType} mission`);
    }

    function updateRobotStatuses(missionType) {
      const statusUpdates = {
        warehouse: {
          leader: "Planning inventory routes",
          coordinator: "Tracking progress", 
          worker1: "Moving to sector A", 
          worker2: "Collecting items",
          worker3: "Transporting goods",
          worker4: "Loading packages",
          scout: "Mapping warehouse",
          specialist: "Handling fragiles"
        },
        search_rescue: {
          leader: "Coordinating search grid",
          coordinator: "Command center",
          worker1: "Clearing debris",
          worker2: "Searching sector B",
          worker3: "Establishing access",
          worker4: "Evacuating victims", 
          scout: "Aerial reconnaissance",
          specialist: "Operating rescue tools"
        },
        manufacturing: {
          leader: "Quality oversight",
          coordinator: "Production control",
          worker1: "Assembly station 1",
          worker2: "Assembly station 2", 
          worker3: "Material handling",
          worker4: "Packaging",
          scout: "Supply monitoring",
          specialist: "Precision welding"
        },
        cleaning: {
          leader: "Task prioritization",
          coordinator: "Resource allocation",
          worker1: "Floor cleaning",
          worker2: "Surface sanitizing",
          worker3: "Waste collection", 
          worker4: "Equipment maintenance",
          scout: "Facility inspection",
          specialist: "HVAC maintenance"
        },
        construction: {
          leader: "Project coordination",
          coordinator: "Task scheduling",
          worker1: "Material transport",
          worker2: "Assembly work",
          worker3: "Site preparation",
          worker4: "Quality checks",
          scout: "Site surveying",
          specialist: "Precision installation"
        }
      };

      const updates = statusUpdates[missionType];
      if (updates) {
        Object.keys(updates).forEach(agentKey => {
          if (robotAgents[agentKey]) {
            robotAgents[agentKey].status = updates[agentKey];
          }
        });
      }
    }

    function displayMissionExecution(missionType, teamCommand, coordinationStrategy) {
      // Create or update mission display
      let missionDisplay = document.getElementById('missionExecutionDisplay');
      if (!missionDisplay) {
        missionDisplay = document.createElement('div');
        missionDisplay.id = 'missionExecutionDisplay';
        missionDisplay.className = 'simulation-output';
        missionDisplay.style.marginTop = '20px';
        document.querySelector('.coordination-panel').appendChild(missionDisplay);
      }

      const missionSteps = {
        warehouse: [
          "Coordinator: Analyzing warehouse layout and inventory priorities",
          "Scout: Mapping optimal routes through warehouse sectors",
          "Leader: Assigning zones based on robot capabilities",
          "Workers 1-4: Moving to designated sectors",
          "Specialist: Handling fragile and high-value items",
          "Coordinator: Monitoring progress and adjusting assignments"
        ],
        search_rescue: [
          "Leader: Establishing search grid and communication protocols",
          "Scout: Aerial reconnaissance of affected area",
          "Coordinator: Setting up command center and resource allocation",
          "Workers: Spreading out to cover maximum search area",
          "Specialist: Deploying rescue equipment and tools",
          "All units: Coordinated search pattern execution"
        ],
        manufacturing: [
          "Leader: Reviewing production schedule and quality requirements",
          "Coordinator: Allocating robots to assembly stations",
          "Workers: Moving to designated workstations",
          "Specialist: Preparing precision tools and quality control",
          "Scout: Monitoring supply chain and material flow",
          "Team: Synchronized production rhythm established"
        ],
        cleaning: [
          "Scout: Comprehensive facility assessment",
          "Leader: Prioritizing tasks based on urgency",
          "Coordinator: Assigning cleaning zones to workers",
          "Workers: Beginning systematic cleaning operations",
          "Specialist: Handling technical maintenance tasks",
          "System: Quality assurance and completion verification"
        ],
        construction: [
          "Scout: Site survey and safety assessment",
          "Leader: Project planning and resource allocation",
          "Coordinator: Task scheduling and workflow optimization",
          "Workers: Site preparation and material positioning",
          "Specialist: Precision installation and quality control",
          "Team: Coordinated construction sequence execution"
        ]
      };

      const steps = missionSteps[missionType] || missionSteps.warehouse;
      let output = `<div class="sim-line"><span class="sim-timestamp">[${new Date().toLocaleTimeString()}]</span> <span class="sim-success">MISSION INITIATED: ${missionType.toUpperCase()}</span></div>`;
      output += `<div class="sim-line"><span class="sim-robot">COMMAND:</span> ${teamCommand}</div>`;
      output += `<div class="sim-line"><span class="sim-action">STRATEGY:</span> ${coordinationStrategy.toUpperCase()} coordination</div>`;
      
      missionDisplay.innerHTML = output;
      
      steps.forEach((step, index) => {
        setTimeout(() => {
          const stepOutput = `<div class="sim-line"><span class="sim-timestamp">[${new Date().toLocaleTimeString()}]</span> <span class="sim-action">Step ${index + 1}:</span> ${step}</div>`;
          missionDisplay.innerHTML += stepOutput;
          missionDisplay.scrollTop = missionDisplay.scrollHeight;
          
          // Update task progress
          const progress = Math.round(((index + 1) / steps.length) * 100);
          const progressElement = document.getElementById('taskProgress');
          if (progressElement) {
            progressElement.textContent = progress + '%';
          }
        }, index * 1500);
      });

      setTimeout(() => {
        const completion = `<div class="sim-line"><span class="sim-timestamp">[${new Date().toLocaleTimeString()}]</span> <span class="sim-success">MISSION COMPLETED SUCCESSFULLY</span></div>`;
        missionDisplay.innerHTML += completion;
        missionDisplay.scrollTop = missionDisplay.scrollHeight;
        
        multiAgentSystem.active = false;
        addCommunicationMessage("System", "Mission completed. All robots returning to ready state.");
      }, steps.length * 1500 + 1000);
    }

    function animateCommunications() {
      const lines = document.querySelectorAll('.communication-line');
      lines.forEach(line => {
        line.classList.add('active');
        setTimeout(() => {
          line.classList.remove('active');
        }, 2000);
      });
    }

    function addCommunicationMessage(sender, message) {
      const commLog = document.getElementById('commLog');
      if (!commLog) return;

      const timestamp = new Date().toLocaleTimeString();
      const messageDiv = document.createElement('div');
      messageDiv.className = 'comm-message';
      messageDiv.innerHTML = `
        <div class="comm-sender">${sender}</div>
        <div class="comm-content">${message}</div>
        <div class="comm-timestamp">${timestamp}</div>
      `;
      
      commLog.appendChild(messageDiv);
      commLog.scrollTop = commLog.scrollHeight;
      
      // Keep only last 10 messages
      while (commLog.children.length > 10) {
        commLog.removeChild(commLog.firstChild);
      }
    }

    function emergencyStop() {
      multiAgentSystem.active = false;
      
      // Update all robot statuses
      Object.keys(robotAgents).forEach(agentKey => {
        robotAgents[agentKey].status = 'EMERGENCY STOP';
      });

      // Show emergency alert
      addCommunicationMessage("EMERGENCY", "🚨 EMERGENCY STOP ACTIVATED - All operations halted");
      
      // Update safety metrics
      const safetyScore = document.getElementById('safetyScore');
      if (safetyScore) {
        safetyScore.textContent = '0%';
        safetyScore.style.color = '#dc3545';
      }

      // Reset after 5 seconds
      setTimeout(() => {
        Object.keys(robotAgents).forEach(agentKey => {
          robotAgents[agentKey].status = 'Ready';
        });
        addCommunicationMessage("System", "Emergency stop cleared. Robots returning to operational status.");
        if (safetyScore) {
          safetyScore.textContent = '98%';
          safetyScore.style.color = '#856404';
        }
      }, 5000);
    }

    function pauseMission() {
      if (multiAgentSystem.active) {
        multiAgentSystem.active = false;
        addCommunicationMessage("System", "⏸️ Mission paused. All robots holding current positions.");
      } else {
        multiAgentSystem.active = true;
        addCommunicationMessage("System", "▶️ Mission resumed. Robots continuing operations.");
      }
    }

    function resetArena() {
      multiAgentSystem.active = false;
      
      // Reset robot statuses
      Object.keys(robotAgents).forEach(agentKey => {
        robotAgents[agentKey].status = 'Ready';
      });
      
      // Clear mission display
      const missionDisplay = document.getElementById('missionExecutionDisplay');
      if (missionDisplay) {
        missionDisplay.remove();
      }
      
      // Clear communication log
      const commLog = document.getElementById('commLog');
      if (commLog) {
        commLog.innerHTML = `
          <div class="comm-message">
            <div class="comm-sender">System</div>
            <div class="comm-content">Multi-agent system reset. 8 robots ready for coordination.</div>
            <div class="comm-timestamp">${new Date().toLocaleTimeString()}</div>
          </div>
        `;
      }
      
      // Reset safety metrics
      updateSafetyMetrics();
      
      // Reset task progress
      const progressElement = document.getElementById('taskProgress');
      if (progressElement) {
        progressElement.textContent = '0%';
      }
    }

    // Section 4: World Model Functions
    function initializeWorldModel() {
      const canvas = document.querySelector('.world-canvas');
      if (!canvas) return;

      canvas.innerHTML = '';

      // Create physics objects
      const objects = [
        { type: 'cube', x: 100, y: 200, id: 'cube1' },
        { type: 'ball', x: 200, y: 150, id: 'ball1' },
        { type: 'cube', x: 300, y: 180, id: 'cube2' },
        { type: 'ball', x: 400, y: 120, id: 'ball2' },
        { type: 'platform', x: 50, y: 230, width: 500, id: 'platform1' }
      ];

      objects.forEach(obj => {
        const element = document.createElement('div');
        element.className = `physics-object physics-${obj.type}`;
        element.id = obj.id;
        element.style.left = obj.x + 'px';
        element.style.top = obj.y + 'px';
        
        if (obj.type === 'platform') {
          element.style.width = obj.width + 'px';
        }
        
        element.onclick = () => selectPhysicsObject(obj.id);
        canvas.appendChild(element);
      });

      updatePredictionDisplay("V-JEPA world model initialized. Click objects to interact or use controls to run physics simulation.");
    }

    function selectPhysicsObject(objectId) {
      // Highlight selected object
      document.querySelectorAll('.physics-object').forEach(obj => {
        obj.style.border = 'none';
      });
      
      const selectedObj = document.getElementById(objectId);
      if (selectedObj) {
        selectedObj.style.border = '2px solid #28a745';
        updatePredictionDisplay(`Selected: ${objectId}. V-JEPA analyzing object properties: mass=1kg, friction=0.3, restitution=0.7. Predicting interaction outcomes...`);
      }
    }

    function runPhysicsSimulation() {
      updatePredictionDisplay("Running V-JEPA physics simulation with predictive world model...");
      
      const objects = document.querySelectorAll('.physics-object:not(.physics-platform)');
      
      objects.forEach((obj, index) => {
        setTimeout(() => {
          const currentY = parseInt(obj.style.top);
          const newY = Math.min(currentY + 50, 210);
          obj.style.top = newY + 'px';
          
          if (newY >= 210) {
            obj.style.backgroundColor = '#ffc107';
            updatePredictionDisplay(`V-JEPA predicted collision confirmed: ${obj.id} impacted platform. Velocity: 0 m/s, Energy dissipated: 2.4J`);
          }
        }, index * 500);
      });
      
      setTimeout(() => {
        updatePredictionDisplay("Physics simulation complete. World model updated with new state. All objects at equilibrium.");
      }, 3000);
    }

    function predictNextFrame() {
      const horizon = document.getElementById('predictionHorizon').value;
      const accuracy = document.getElementById('physicsAccuracy').value;
      
      const predictions = [
        "V-JEPA analyzing current world state...",
        `Predicting ${horizon} frames ahead (${horizon * 33}ms future state)`,
        "Cube1: Stable on platform (confidence: 98%)",
        "Ball1: Potential rolling motion detected (confidence: 76%)",
        "Ball2: Collision trajectory with cube2 in 3 frames (confidence: 89%)",
        "Platform stability: 100% (rigid body constraint)",
        `Overall prediction accuracy: ${accuracy === 'high' ? '94%' : accuracy === 'medium' ? '87%' : '78%'}`
      ];
      
      let output = "";
      predictions.forEach((pred, index) => {
        setTimeout(() => {
          output += pred + "\n";
          updatePredictionDisplay(output);
        }, index * 800);
      });
    }

    function addRandomObject() {
      const canvas = document.querySelector('.world-canvas');
      if (!canvas) return;

      const objectTypes = ['cube', 'ball'];
      const type = objectTypes[Math.floor(Math.random() * objectTypes.length)];
      const id = `${type}_${Date.now()}`;
      
      const element = document.createElement('div');
      element.className = `physics-object physics-${type}`;
      element.id = id;
      element.style.left = Math.random() * 400 + 100 + 'px';
      element.style.top = Math.random() * 100 + 50 + 'px';
      element.onclick = () => selectPhysicsObject(id);
      
      canvas.appendChild(element);
      updatePredictionDisplay(`New ${type} added to world model. V-JEPA updating predictions for ${canvas.children.length} objects.`);
    }

    function resetWorld() {
      initializeWorldModel();
      updatePredictionDisplay("World model reset to initial state. V-JEPA recalibrated.");
    }

    function updatePredictionDisplay(message) {
      const display = document.querySelector('.prediction-display');
      if (display) {
        display.textContent = message;
      }
    }

    // Section 5: Tabs and Future Capabilities
    function switchTab(tabName, element) {
      document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
      document.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));
      
      element.classList.add('active');
      document.getElementById(tabName).classList.add('active');
    }

    function exploreFutureCapabilities() {
      const domain = document.getElementById('capabilityDomain').value;
      const timeline = document.getElementById('technologyTimeline').value;
      const resultDiv = document.getElementById('futureCapabilities');
      
      const capabilities = {
        household: {
          current: {
            title: "Smart Home Assistance (2024-2025)",
            features: [
              "Multi-room coordination for cleaning tasks",
              "Voice-controlled object manipulation",
              "Basic meal preparation assistance",
              "Laundry sorting and folding",
              "Simple maintenance task execution"
            ],
            limitations: "Limited to structured environments, requires pre-programmed task sequences"
          },
          near: {
            title: "Adaptive Home Companions (2025-2027)",
            features: [
              "Natural conversation and emotional support",
              "Complex meal cooking with recipe adaptation",
              "Childcare assistance and educational support",
              "Healthcare monitoring and assistance",
              "Personalized home environment optimization"
            ],
            limitations: "Requires extensive training data, limited creativity in novel situations"
          },
          medium: {
            title: "Autonomous Household Partners (2027-2030)",
            features: [
              "Creative problem-solving in daily tasks",
              "Multi-generational family interaction",
              "Home maintenance and minor repairs",
              "Garden management and food production",
              "Integration with smart city infrastructure"
            ],
            limitations: "High computational requirements, regulatory challenges for autonomous operation"
          },
          long: {
            title: "Human-Level Domestic Intelligence (2030+)",
            features: [
              "Indistinguishable from human assistance capabilities",
              "Emotional intelligence and relationship building",
              "Creative task execution and innovation",
              "Long-term planning and life management",
              "Seamless integration into human social structures"
            ],
            limitations: "Ethical considerations around human-robot relationships, employment impacts"
          }
        },
        manufacturing: {
          current: {
            title: "Smart Factory Integration (2024-2025)",
            features: [
              "Multi-robot assembly line coordination",
              "Real-time quality control and inspection",
              "Adaptive production scheduling",
              "Predictive maintenance systems",
              "Human-robot collaborative workspaces"
            ],
            limitations: "Limited to structured manufacturing processes, requires extensive setup"
          },
          near: {
            title: "Autonomous Production Systems (2025-2027)",
            features: [
              "Self-optimizing production workflows",
              "Zero-defect manufacturing with ML prediction",
              "Dynamic reconfiguration for new products",
              "Supply chain integration and optimization",
              "Advanced human-robot collaboration"
            ],
            limitations: "High initial investment, complex integration with legacy systems"
          },
          medium: {
            title: "Cognitive Manufacturing (2027-2030)",
            features: [
              "Creative design and prototyping assistance",
              "Fully autonomous factory operations",
              "Predictive market response and adaptation",
              "Self-repairing and evolving systems",
              "Sustainable and circular production models"
            ],
            limitations: "Regulatory approval for autonomous systems, workforce transition challenges"
          },
          long: {
            title: "Post-Scarcity Production (2030+)",
            features: [
              "Molecular-level manufacturing precision",
              "On-demand personalized production",
              "Self-replicating manufacturing systems",
              "Waste-free circular economy integration",
              "Economic model transformation"
            ],
            limitations: "Fundamental economic and social restructuring required"
          }
        },
        healthcare: {
          current: {
            title: "Medical Assistance Robots (2024-2025)",
            features: [
              "Patient monitoring and vital sign tracking",
              "Medication delivery and scheduling",
              "Physical therapy assistance",
              "Surgical tool management",
              "Elderly care and mobility support"
            ],
            limitations: "Limited to supervised operation, regulatory restrictions on autonomous medical decisions"
          },
          near: {
            title: "Diagnostic and Care Partners (2025-2027)",
            features: [
              "Advanced diagnostic assistance with ML",
              "Personalized treatment plan optimization",
              "Mental health support and therapy",
              "Emergency response coordination",
              "Telemedicine integration and support"
            ],
            limitations: "Medical liability concerns, requirement for human oversight"
          },
          medium: {
            title: "Autonomous Healthcare Systems (2027-2030)",
            features: [
              "Independent diagnostic capabilities",
              "Surgical assistance and precision operations",
              "Preventive care and health optimization",
              "Epidemic monitoring and response",
              "Personalized medicine and treatment"
            ],
            limitations: "Extensive testing and validation required, ethical concerns about autonomous medical decisions"
          },
          long: {
            title: "Medical Superintelligence (2030+)",
            features: [
              "Disease eradication and prevention",
              "Longevity and anti-aging treatments",
              "Neural interface and enhancement",
              "Genetic therapy optimization",
              "Human biological augmentation"
            ],
            limitations: "Profound ethical and philosophical questions about human enhancement"
          }
        },
        construction: {
          current: {
            title: "Construction Automation (2024-2025)",
            features: [
              "Automated bricklaying and assembly",
              "3D printing of building components",
              "Site surveying and mapping",
              "Material transport and positioning",
              "Safety monitoring and hazard detection"
            ],
            limitations: "Limited to repetitive tasks, requires human supervision for complex decisions"
          },
          near: {
            title: "Intelligent Building Systems (2025-2027)",
            features: [
              "Autonomous project planning and execution",
              "Adaptive construction techniques",
              "Real-time quality control and correction",
              "Integrated building system installation",
              "Environmental optimization during construction"
            ],
            limitations: "Complex regulatory environment, integration with existing construction practices"
          },
          medium: {
            title: "Self-Building Structures (2027-2030)",
            features: [
              "Fully autonomous construction projects",
              "Self-repairing and adapting buildings",
              "Integrated smart city infrastructure",
              "Sustainable and carbon-negative construction",
              "Disaster-resistant adaptive architecture"
            ],
            limitations: "Major changes required in building codes and urban planning"
          },
          long: {
            title: "Transformative Architecture (2030+)",
            features: [
              "Living, evolving building structures",
              "Space-based construction capabilities",
              "Molecular-scale building materials",
              "Climate-responsive mega-structures",
              "Integration with planetary engineering"
            ],
            limitations: "Fundamental changes to human habitat and society"
          }
        },
        exploration: {
          current: {
            title: "Robotic Exploration (2024-2025)",
            features: [
              "Autonomous Mars rover operations",
              "Deep ocean exploration vehicles",
              "Space station maintenance robots",
              "Arctic and Antarctic research support",
              "Disaster zone reconnaissance"
            ],
            limitations: "Limited by communication delays, harsh environment challenges"
          },
          near: {
            title: "Advanced Exploration Systems (2025-2027)",
            features: [
              "Multi-robot swarm exploration",
              "In-situ resource utilization",
              "Autonomous scientific experimentation",
              "Long-duration mission capability",
              "Cross-planetary communication networks"
            ],
            limitations: "Extreme reliability requirements, limited repair and maintenance options"
          },
          medium: {
            title: "Self-Sustaining Exploration (2027-2030)",
            features: [
              "Self-replicating exploration systems",
              "Autonomous base construction",
              "Ecosystem establishment and monitoring",
              "Interplanetary resource extraction",
              "Advanced AI scientific discovery"
            ],
            limitations: "Unprecedented autonomy requirements, ethical considerations about planetary impact"
          },
          long: {
            title: "Galactic Intelligence Network (2030+)",
            features: [
              "Interstellar probe networks",
              "Artificial ecosystem creation",
              "Consciousness backup and transfer",
              "Galactic civilization infrastructure",
              "Search for and communication with alien intelligence"
            ],
            limitations: "Physics limitations of interstellar travel, fundamental questions about consciousness"
          }
        }
      };

      const capability = capabilities[domain][timeline];
      
      if (resultDiv && capability) {
        resultDiv.innerHTML = `
          <div class="success" style="margin-top: 20px;">
            <h4>${capability.title}</h4>
            
            <div style="margin: 15px 0;">
              <strong>Key Capabilities:</strong>
              <ul style="margin: 8px 0; padding-left: 20px;">
                ${capability.features.map(feature => `<li style="font-size: 14px; margin: 6px 0;">${feature}</li>`).join('')}
              </ul>
            </div>
            
            <div style="background: #fff3cd; padding: 12px; border-radius: 6px; margin: 15px 0;">
              <strong>Current Limitations:</strong>
              <p style="margin: 5px 0; font-size: 14px;">${capability.limitations}</p>
            </div>
            
            <p><strong>Timeline:</strong> ${timeline.charAt(0).toUpperCase() + timeline.slice(1)}-term development focus for ${domain} robotics applications.</p>
          </div>
        `;
      }
    }

    // Safety monitoring functions
    function startSafetyMonitoring() {
      setInterval(updateSafetyMetrics, 2000);
    }

    function updateSafetyMetrics() {
      const safetyScore = document.getElementById('safetyScore');
      const coordHealth = document.getElementById('coordinationHealth');
      const commLatency = document.getElementById('communicationLatency');
      const taskProgress = document.getElementById('taskProgress');

      if (multiAgentSystem.active) {
        // Simulate dynamic metrics during mission
        const baseScore = 95 + Math.random() * 5;
        const baseHealth = 90 + Math.random() * 10;
        const baseLatency = 8 + Math.random() * 8;
        
        if (safetyScore) safetyScore.textContent = Math.round(baseScore) + '%';
        if (coordHealth) coordHealth.textContent = Math.round(baseHealth) + '%';
        if (commLatency) commLatency.textContent = Math.round(baseLatency) + 'ms';
      } else {
        // Static metrics when inactive
        if (safetyScore) safetyScore.textContent = '98%';
        if (coordHealth) coordHealth.textContent = '95%';
        if (commLatency) commLatency.textContent = '12ms';
        if (!taskProgress?.textContent?.includes('%') || parseInt(taskProgress.textContent) === 0) {
          if (taskProgress) taskProgress.textContent = '0%';
        }
      }
    }

    // Utility functions
    function copyCode(button) {
      const codeBlock = button.parentElement.querySelector('pre');
      if (codeBlock) {
        const text = codeBlock.textContent;
        navigator.clipboard.writeText(text).then(() => {
          button.textContent = '✅ Copied';
          setTimeout(() => {
            button.textContent = '📋 Copy';
          }, 2000);
        });
      }
    }
  </script>
</body>
</html>
