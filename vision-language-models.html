<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Vision-Language Models: GPT-4V, Gemini, Claude</title>
  <style>
    body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:1200px;margin:0 auto;padding:20px;background:linear-gradient(135deg,#1a1a1a 0%,#2d2d2d 100%);color:#e0e0e0;line-height:1.6}
    .container{background:#fff;color:#2d2d2d;border-radius:20px;padding:30px;margin:20px 0;border:1px solid #e0e0e0;box-shadow:0 4px 20px rgba(0,0,0,.1)}
    .nav-bar{background:#2d2d2d;color:#fff;padding:15px 30px;border-radius:15px;margin:20px 0;display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:15px}
    .nav-home,.nav-prev,.nav-next{background:#fff;color:#2d2d2d;padding:8px 16px;border-radius:6px;text-decoration:none;font-weight:bold;transition:all .3s}
    .nav-next{background:#28a745;color:#fff}
    .nav-home:hover,.nav-prev:hover,.nav-next:hover{transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    .nav-title{font-size:1.2em;font-weight:bold;flex:1;min-width:300px}
    .step{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;margin:15px 0;border-radius:15px;border-left:4px solid #2d2d2d}
    .interactive-demo{background:#f8f9fa;border:2px solid #e9ecef;border-radius:15px;padding:25px;margin:20px 0}
    .demo-title{font-size:1.3em;font-weight:bold;margin-bottom:20px;text-align:center;color:#2d2d2d}
    .controls{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .control-group{display:flex;flex-direction:column;gap:5px}
    .control-group label{font-weight:bold;font-size:14px;color:#2d2d2d}
    button{background:#2d2d2d;border:none;color:#fff;padding:12px 24px;border-radius:8px;cursor:pointer;font-weight:bold;transition:all .3s;margin:5px}
    button:hover{background:#1a1a1a;transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    button.primary{background:#28a745}
    button.primary:hover{background:#218838}
    input,select,textarea{padding:8px 12px;border:1px solid #dadce0;border-radius:6px;background:#fff;color:#2d2d2d}
    textarea{min-height:70px;resize:vertical}
    .math-formula{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;text-align:center;font-size:16px;box-shadow:0 2px 8px rgba(0,0,0,.1)}
    .code-block{background:#2d2d2d;color:#e0e0e0;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;font-size:14px;overflow:auto;position:relative}
    .code-header{background:#1a1a1a;color:#28a745;padding:8px 15px;margin:-20px -20px 15px -20px;border-radius:8px 8px 0 0;font-weight:bold;font-size:12px}
    .copy-button{position:absolute;top:10px;right:10px;background:#28a745;color:#fff;border:none;padding:4px 8px;border-radius:4px;cursor:pointer;font-size:10px}
    .architecture-flow{display:flex;justify-content:space-between;align-items:center;margin:20px 0;padding:20px;background:#fff;border-radius:12px;border:2px solid #e9ecef;flex-wrap:wrap;gap:15px}
    .arch-component{background:#f8f9fa;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center;min-width:120px;flex:1}
    .arch-component.vision{border-color:#dc3545}
    .arch-component.language{border-color:#007bff}
    .arch-component.fusion{border-color:#28a745}
    .arch-component.output{border-color:#ffc107}
    .arch-arrow{font-size:24px;color:#28a745;font-weight:bold}
    .model-comparison{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:20px;margin:20px 0}
    .model-card{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;transition:all .3s}
    .model-card:hover{border-color:#28a745;transform:translateY(-3px);box-shadow:0 6px 20px rgba(0,0,0,.1)}
    .model-card.selected{border-color:#28a745;background:#d4edda}
    .model-name{font-size:1.3em;font-weight:bold;margin-bottom:10px;color:#2d2d2d}
    .model-specs{background:#f8f9fa;padding:12px;border-radius:6px;margin:10px 0;font-size:12px;font-family:'Courier New',monospace}
    .model-capabilities{margin:15px 0}
    .capability{display:flex;align-items:center;margin:5px 0;font-size:14px}
    .capability-icon{margin-right:8px;font-size:16px}
    .token-flow{display:flex;flex-wrap:wrap;gap:8px;margin:15px 0;padding:15px;background:#f8f9fa;border-radius:8px}
    .token{padding:8px 12px;background:#e9ecef;border-radius:6px;font-family:'Courier New',monospace;font-size:12px;border:2px solid transparent}
    .token.text{border-color:#007bff;background:#cce7ff}
    .token.image{border-color:#dc3545;background:#ffcccc}
    .token.special{border-color:#28a745;background:#ccffcc}
    .attention-matrix{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;margin:15px 0}
    .matrix-grid{display:grid;gap:2px;margin:10px 0}
    .matrix-cell{padding:6px;text-align:center;font-size:10px;border-radius:3px;font-weight:bold}
    .attention-high{background:#dc3545;color:#fff}
    .attention-medium{background:#ffc107;color:#000}
    .attention-low{background:#e9ecef;color:#495057}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(150px,1fr));gap:15px;margin:20px 0}
    .metric-card{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center}
    .metric-value{font-size:1.5em;font-weight:bold;color:#2d2d2d;margin-bottom:5px}
    .metric-label{font-size:12px;color:#666}
    .benchmark-table{width:100%;border-collapse:collapse;margin:15px 0;background:#fff;border:1px solid #e9ecef;border-radius:8px;overflow:hidden}
    .benchmark-table th{background:#2d2d2d;color:#fff;padding:12px;text-align:center;font-weight:bold}
    .benchmark-table td{padding:12px;text-align:center;border-bottom:1px solid #e9ecef}
    .score-excellent{background:#d4edda;color:#155724;font-weight:bold}
    .score-good{background:#d1ecf1;color:#0c5460;font-weight:bold}
    .score-average{background:#fff3cd;color:#856404;font-weight:bold}
    .score-poor{background:#f8d7da;color:#721c24;font-weight:bold}
    .conversation-demo{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0}
    .message{margin:10px 0;padding:15px;border-radius:8px;max-width:80%}
    .message.user{background:#e3f2fd;margin-left:auto;border-bottom-right-radius:4px}
    .message.assistant{background:#f3e5f5;margin-right:auto;border-bottom-left-radius:4px}
    .message.system{background:#fff3e0;margin:10px auto;text-align:center;font-style:italic;max-width:60%}
    .image-placeholder{width:200px;height:150px;background:linear-gradient(45deg,#e9ecef,#dee2e6);border-radius:8px;display:flex;align-items:center;justify-content:center;margin:10px 0;font-weight:bold;color:#495057}
    .fusion-strategies{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:15px;margin:20px 0}
    .fusion-card{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;cursor:pointer;transition:all .3s}
    .fusion-card:hover{border-color:#28a745;transform:translateY(-2px)}
    .fusion-card.selected{border-color:#28a745;background:#d4edda}
    .fusion-title{font-weight:bold;margin-bottom:8px;color:#2d2d2d}
    .fusion-description{font-size:13px;color:#666;margin-bottom:8px}
    .fusion-math{background:#f8f9fa;padding:8px;border-radius:4px;font-size:11px;font-family:'Courier New',monospace}
    .info{background:#d1ecf1;border-left:4px solid #17a2b8;color:#0c5460;padding:15px;border-radius:8px;margin:15px 0}
    .success{background:#d4edda;border-left:4px solid #28a745;color:#155724;padding:15px;border-radius:8px;margin:15px 0}
    .warning{background:#fff3cd;border-left:4px solid #ffc107;color:#856404;padding:15px;border-radius:8px;margin:15px 0}
    .breakthrough-highlight{background:linear-gradient(135deg,#28a745,#20c997);color:#fff;padding:20px;border-radius:12px;text-align:center;margin:20px 0;font-size:18px;font-weight:bold;box-shadow:0 4px 15px rgba(40,167,69,.3)}
    .tabs{display:flex;margin-bottom:20px;border-bottom:2px solid #e9ecef}
    .tab{padding:12px 24px;cursor:pointer;border-bottom:3px solid transparent;font-weight:bold;transition:all .3s}
    .tab:hover{background:#f8f9fa}
    .tab.active{border-bottom-color:#28a745;background:#d4edda;color:#155724}
    .tab-content{display:none}
    .tab-content.active{display:block}
    @keyframes pulse{0%{transform:scale(1);opacity:.8}50%{transform:scale(1.05);opacity:1}100%{transform:scale(1);opacity:.8}}
    .processing-animation{animation:pulse 2s ease-in-out infinite}
    .timeline{position:relative;margin:20px 0}
    .timeline-item{display:flex;align-items:center;margin:15px 0;padding:15px;background:#fff;border-radius:8px;border-left:4px solid #28a745}
    .timeline-date{font-weight:bold;min-width:100px;color:#28a745}
    .timeline-content{flex:1;margin-left:15px}
    .capability-matrix{display:grid;grid-template-columns:200px repeat(4,1fr);gap:1px;background:#dee2e6;border-radius:8px;overflow:hidden;margin:20px 0}
    .capability-header{background:#2d2d2d;color:#fff;padding:12px;font-weight:bold;text-align:center}
    .capability-cell{background:#fff;padding:10px;text-align:center;font-size:12px}
    .performance-indicator{display:inline-block;width:12px;height:12px;border-radius:50%;margin:0 2px}
    .perf-excellent{background:#28a745}
    .perf-good{background:#17a2b8}
    .perf-average{background:#ffc107}
    .perf-poor{background:#dc3545}
  </style>
</head>
<body>
  <div class="nav-bar">
    <div class="nav-title">üëÅÔ∏è Vision-Language Models: GPT-4V, Gemini, Claude</div>
    <a href="index.html" class="nav-home">üè† Home</a>
    <a href="clip-architecture.html" class="nav-prev">‚Üê CLIP Architecture</a>
    <a href="vision-language-action.html" class="nav-next">Next: Vision-Language-Action ‚Üí</a>
  </div>

  <div class="container">
    <h1>üëÅÔ∏è Vision-Language Models: The Multimodal AI Revolution</h1>
    <p>Building on CLIP's foundation, modern <strong>Vision-Language Models (VLMs)</strong> like GPT-4V, Gemini, and Claude have revolutionized how AI systems understand and reason about images. These models don't just classify or retrieve - they <strong>converse, analyze, and reason</strong> about visual content using natural language, enabling applications from document analysis to creative assistance.</p>
    
    <div class="breakthrough-highlight">
      üåü From CLIP's breakthrough (2021) to conversational vision AI (2023): How joint embeddings evolved into multimodal reasoning systems that can see, understand, and discuss any image!
    </div>
  </div>

  <div class="container">
    <h2>üèóÔ∏è Architecture Evolution: From CLIP to Conversational AI</h2>
<div class="step">
      <h3>üîÑ The Architectural Revolution</h3>
      <p>While CLIP created a shared embedding space, modern VLMs integrate vision directly into the <strong>language model's token stream</strong>. Instead of separate encoders, images become part of the conversational flow alongside text tokens.</p>

      <div class="architecture-flow">
        <div class="arch-component vision">
          <h4>üì∑ Vision Encoder</h4>
          <div>ViT or CNN</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Processes image patches<br>
            ‚Ä¢ Outputs visual tokens<br>
            ‚Ä¢ ~1000 tokens per image
          </div>
        </div>
        
        <div class="arch-arrow">‚Üí</div>
        
        <div class="arch-component fusion">
          <h4>üîó Vision-Language Fusion</h4>
          <div>Token Integration</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Visual tokens + text tokens<br>
            ‚Ä¢ Cross-modal attention<br>
            ‚Ä¢ Unified sequence modeling
          </div>
        </div>
        
        <div class="arch-arrow">‚Üí</div>
        
        <div class="arch-component language">
          <h4>üß† Language Model</h4>
          <div>Transformer LLM</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ GPT, Gemini, Claude architecture<br>
            ‚Ä¢ Processes mixed modality tokens<br>
            ‚Ä¢ Generates text responses
          </div>
        </div>
        
        <div class="arch-arrow">‚Üí</div>
        
        <div class="arch-component output">
          <h4>üí¨ Response Generation</h4>
          <div>Natural Language</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Contextual understanding<br>
            ‚Ä¢ Reasoning and analysis<br>
            ‚Ä¢ Conversational responses
          </div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üîç Detailed Architecture Analysis</div>

        <div class="controls">
          <div class="control-group">
            <label>Analysis Focus:</label>
            <select id="analysisLevel">
              <option value="architecture" selected>Architecture Details</option>
              <option value="dimensions">Dimension Alignment</option>
              <option value="training-data">Training Data Sources</option>
              <option value="tokens">Token Processing</option>
              <option value="attention">Attention Mechanisms</option>
            </select>
          </div>
          <div class="control-group">
            <label>Technical Detail Level:</label>
            <select id="detailLevel">
              <option value="high" selected>High (Mathematical)</option>
              <option value="medium">Medium (Conceptual)</option>
              <option value="practical">Practical (Implementation)</option>
            </select>
          </div>
        </div>

        <div class="model-comparison">
          <div class="model-card" onclick="selectModel('gpt4v', this)">
            <div class="model-name">ü§ñ GPT-4V (OpenAI)</div>
            <div class="model-specs">
              <strong>Core Architecture:</strong> GPT-4 Transformer + Custom Vision Encoder<br>
              <strong>Vision Component:</strong> Modified ViT-G/14 (1.3B params)<br>
              <strong>Language Model:</strong> GPT-4 (~1.36T params)<br>
              <strong>Dimension Strategy:</strong> Vision 1280D ‚Üí Projection ‚Üí Language 12,288D<br>
              <strong>Context Window:</strong> 128K tokens (includes ~1,700 visual tokens)<br>
              <strong>Vision Resolution:</strong> Up to 2048√ó2048 (adaptive tiling)<br>
              <strong>Training Data:</strong> ~10B image-text pairs (estimated)<br>
              <strong>Integration:</strong> Learned projection + cross-attention layers
            </div>
          </div>

          <div class="model-card" onclick="selectModel('gemini', this)">
            <div class="model-name">üíé Gemini Ultra (Google)</div>
            <div class="model-specs">
              <strong>Core Architecture:</strong> Native multimodal Pathways Transformer<br>
              <strong>Vision Component:</strong> Integrated ViT-22B with temporal modeling<br>
              <strong>Language Model:</strong> PaLM-2 derivative (~540B params unified)<br>
              <strong>Dimension Strategy:</strong> Unified 2048D across all modalities<br>
              <strong>Context Window:</strong> 2M tokens (multimodal context mixing)<br>
              <strong>Vision Resolution:</strong> Up to 4096√ó4096 + video frames<br>
              <strong>Training Data:</strong> ~100B+ multimodal examples (web + YouTube)<br>
              <strong>Integration:</strong> Native joint training from initialization
            </div>
          </div>

          <div class="model-card" onclick="selectModel('claude', this)">
            <div class="model-name">ü§ñ Claude 3 Opus (Anthropic)</div>
            <div class="model-specs">
              <strong>Core Architecture:</strong> Constitutional AI Transformer + Vision Module<br>
              <strong>Vision Component:</strong> ViT-L/14 with constitutional alignment<br>
              <strong>Language Model:</strong> Claude 3 transformer (~175B params)<br>
              <strong>Dimension Strategy:</strong> Vision 1024D ‚Üí Projection ‚Üí Language 4096D<br>
              <strong>Context Window:</strong> 200K tokens (optimized for documents)<br>
              <strong>Vision Resolution:</strong> Up to 8000√ó8000 (document focus)<br>
              <strong>Training Data:</strong> ~5B filtered, high-quality pairs<br>
              <strong>Integration:</strong> Constitutional safety-aware fusion
            </div>
          </div>
        </div>

        <div id="architectureDetails"></div>
      </div>
    </div>

    <script>
      // Enhanced selectModel function with complete architecture details
      function selectModel(modelId, element) {
        selectedModel = modelId;
        
        // Update visual selection
        document.querySelectorAll('.model-card').forEach(card => card.classList.remove('selected'));
        element.classList.add('selected');
        
        // Show detailed architecture information based on analysis level
        const analysisLevel = document.getElementById('analysisLevel')?.value || 'architecture';
        const detailLevel = document.getElementById('detailLevel')?.value || 'high';
        
        const architectureDetails = {
          'gpt4v': {
            architecture: {
              high: `
                <div class="step">
                  <h4>üîç GPT-4V Architecture Deep Dive</h4>
                  
                  <div class="math-formula">
                    <strong>GPT-4V Processing Pipeline:</strong><br><br>
                    
                    <strong>1. Vision Tokenization:</strong><br>
                    Image(H√óW√ó3) ‚Üí Patches(N√óP¬≤√ó3) ‚Üí ViT ‚Üí Visual_Tokens(N√ó1280)<br>
                    Where N ‚âà 1700 tokens for high-res images<br><br>
                    
                    <strong>2. Projection to LLM Space:</strong><br>
                    V_projected = MLP(V_visual) ‚àà ‚Ñù^(N√ó12288)<br>
                    MLP: 1280 ‚Üí 4096 ‚Üí 12288 (with GELU activation)<br><br>
                    
                    <strong>3. Token Stream Integration:</strong><br>
                    Sequence = [V_1, V_2, ..., V_1700, T_1, T_2, ..., T_M]<br>
                    Total length ‚â§ 128K tokens<br><br>
                    
                    <strong>4. Cross-Modal Processing:</strong><br>
                    Attention(Q, K, V) where Q, K, V span visual + text tokens
                  </div>
                  
                  <div class="architecture-flow" style="flex-direction: column;">
                    <div style="display: flex; justify-content: space-between; width: 100%; margin-bottom: 15px;">
                      <div class="arch-component vision">
                        <h5>üì∑ Custom ViT-G/14</h5>
                        <div style="font-size: 11px;">
                          ‚Ä¢ Parameters: 1.3B<br>
                          ‚Ä¢ Layers: 40<br>
                          ‚Ä¢ Hidden size: 1280<br>
                          ‚Ä¢ Attention heads: 16<br>
                          ‚Ä¢ Patch size: 14√ó14<br>
                          ‚Ä¢ Max resolution: 2048px
                        </div>
                      </div>
                      <div class="arch-arrow">‚Üí</div>
                      <div class="arch-component fusion">
                        <h5>üîó Vision Projector</h5>
                        <div style="font-size: 11px;">
                          ‚Ä¢ Input: 1280D<br>
                          ‚Ä¢ Hidden: 4096D<br>
                          ‚Ä¢ Output: 12,288D<br>
                          ‚Ä¢ Params: ~52M<br>
                          ‚Ä¢ Activation: GELU<br>
                          ‚Ä¢ Dropout: 0.1
                        </div>
                      </div>
                      <div class="arch-arrow">‚Üí</div>
                      <div class="arch-component language">
                        <h5>üß† GPT-4 Transformer</h5>
                        <div style="font-size: 11px;">
                          ‚Ä¢ Parameters: 1.36T<br>
                          ‚Ä¢ Layers: 120 (estimated)<br>
                          ‚Ä¢ Hidden size: 12,288<br>
                          ‚Ä¢ Attention heads: 128<br>
                          ‚Ä¢ Feed-forward: 49,152<br>
                          ‚Ä¢ Context: 128K tokens
                        </div>
                      </div>
                    </div>
                  </div>
                </div>`
            },
            dimensions: {
              high: `
                <div class="step">
                  <h4>üìê GPT-4V Dimension Alignment Strategy</h4>
                  
                  <div class="math-formula">
                    <strong>The Dimension Mismatch Problem:</strong><br><br>
                    Vision Encoder Output: V ‚àà ‚Ñù^(N√ó1280)<br>
                    Language Model Expected: T ‚àà ‚Ñù^(M√ó12288)<br>
                    <strong>Challenge:</strong> 1280 ‚â† 12,288 dimensions!<br><br>
                    
                    <strong>GPT-4V Solution - Learned Projection:</strong><br>
                    Projection: f: ‚Ñù^1280 ‚Üí ‚Ñù^12288<br>
                    Implementation: f(v) = GELU(Linear‚ÇÇ(GELU(Linear‚ÇÅ(v))))<br>
                    Where: Linear‚ÇÅ: 1280‚Üí4096, Linear‚ÇÇ: 4096‚Üí12288<br><br>
                    
                    <strong>Why This Works:</strong><br>
                    ‚Ä¢ Preserves visual information richness<br>
                    ‚Ä¢ Learns optimal mapping during training<br>
                    ‚Ä¢ Allows independent vision/language optimization
                  </div>
                  
                  <div class="code-block">
                    <div class="code-header">üíª GPT-4V-Style Dimension Alignment</div>
                    <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>class VisionProjector(nn.Module):
    def __init__(self, vision_dim=1280, language_dim=12288, hidden_dim=4096):
        super().__init__()
        self.projector = nn.Sequential(
            nn.Linear(vision_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, language_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
    
    def forward(self, vision_features):
        # vision_features: [batch, num_patches, 1280]
        # output: [batch, num_patches, 12288] - ready for GPT-4
        return self.projector(vision_features)

# Usage in forward pass
vision_output = clip_vision_encoder(image)  # [B, 257, 1280]
vision_tokens = vision_projector(vision_output[:, 1:])  # Remove CLS, project patches

# Now compatible with GPT-4 text tokens (12,288D)
text_embeddings = gpt4_embeddings(text_tokens)  # [B, seq_len, 12288]
mixed_sequence = torch.cat([vision_tokens, text_embeddings], dim=1)
</pre>
                  </div>
                  
                  <div class="warning">
                    <strong>‚ö†Ô∏è Projection Trade-offs:</strong><br>
                    ‚Ä¢ <strong>Parameters:</strong> Adds ~52M trainable parameters<br>
                    ‚Ä¢ <strong>Information bottleneck:</strong> Could lose visual details<br>
                    ‚Ä¢ <strong>Training complexity:</strong> Must learn optimal mapping<br>
                    ‚Ä¢ <strong>Memory overhead:</strong> Extra computation and storage<br><br>
                    
                    <strong>‚úÖ Benefits:</strong><br>
                    ‚Ä¢ Uses proven pre-trained components (CLIP + GPT-4)<br>
                    ‚Ä¢ Flexible - can swap vision/language components<br>
                    ‚Ä¢ Optimal dimensions for each modality<br>
                    ‚Ä¢ Easier debugging and analysis
                  </div>
                </div>`
            },
            'training-data': {
              high: `
                <div class="step">
                  <h4>üìä GPT-4V Training Data Composition (Estimated)</h4>
                  
                  <div class="metric-grid">
                    <div class="metric-card">
                      <div class="metric-value">~10B</div>
                      <div class="metric-label">Total Image-Text Pairs</div>
                    </div>
                    <div class="metric-card">
                      <div class="metric-value">üîí</div>
                      <div class="metric-label">Transparency Level</div>
                    </div>
                    <div class="metric-card">
                      <div class="metric-value">$100M+</div>
                      <div class="metric-label">Estimated Training Cost</div>
                    </div>
                    <div class="metric-card">
                      <div class="metric-value">Mixed</div>
                      <div class="metric-label">Copyright Status</div>
                    </div>
                  </div>
                  
                  <div style="display: grid; gap: 15px; margin: 20px 0;">
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #dc3545;">
                      <strong>üåê Web-Crawled Data (~70%)</strong><br>
                      ‚Ä¢ CommonCrawl image-text pairs<br>
                      ‚Ä¢ Social media with alt-text (Twitter, Reddit)<br>
                      ‚Ä¢ News websites with captions<br>
                      ‚Ä¢ E-commerce product descriptions<br>
                      ‚Ä¢ Wikipedia images with context
                    </div>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007bff;">
                      <strong>üìö Licensed & Academic (~15%)</strong><br>
                      ‚Ä¢ Academic papers and figures<br>
                      ‚Ä¢ Licensed stock photography<br>
                      ‚Ä¢ Educational content partnerships<br>
                      ‚Ä¢ Museum and gallery collections<br>
                      ‚Ä¢ Government and public domain data
                    </div>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #28a745;">
                      <strong>üéØ Instruction-Tuned (~10%)</strong><br>
                      ‚Ä¢ Human-annotated visual QA<br>
                      ‚Ä¢ RLHF preference data<br>
                      ‚Ä¢ Safety-filtered examples<br>
                      ‚Ä¢ Chain-of-thought visual reasoning<br>
                      ‚Ä¢ Conversation-style image discussions
                    </div>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #ffc107;">
                      <strong>ü§ñ Synthetic & Generated (~5%)</strong><br>
                      ‚Ä¢ DALL-E generated training pairs<br>
                      ‚Ä¢ Programmatically created diagrams<br>
                      ‚Ä¢ Augmented existing datasets<br>
                      ‚Ä¢ Simulated environments<br>
                      ‚Ä¢ AI-generated educational content
                    </div>
                  </div>
                  
                  <div class="warning">
                    <strong>üö® GPT-4V Training Data Concerns:</strong><br>
                    ‚Ä¢ <strong>Copyright Issues:</strong> Extensive use of web-scraped content without explicit permission<br>
                    ‚Ä¢ <strong>Artist Concerns:</strong> Training on copyrighted artwork and photography<br>
                    ‚Ä¢ <strong>Transparency Gap:</strong> OpenAI provides minimal details about data sources<br>
                    ‚Ä¢ <strong>Legal Challenges:</strong> Multiple ongoing lawsuits over training data usage<br>
                    ‚Ä¢ <strong>Bias Potential:</strong> Web data inherits internet biases and representation issues
                  </div>
                </div>`
            },
            tokens: {
              high: `
                <div class="step">
                  <h4>üé≠ GPT-4V Token Processing Strategy</h4>
                  <div class="token-flow">
                    <div class="token special">[IMG_START]</div>
                    <div class="token image">patch_0_0</div>
                    <div class="token image">patch_0_1</div>
                    <div class="token image">...</div>
                    <div class="token image">patch_23_23</div>
                    <div class="token special">[IMG_END]</div>
                    <div class="token text">Describe</div>
                    <div class="token text">this</div>
                    <div class="token text">image</div>
                  </div>
                  
                  <div class="info">
                    <strong>üéØ Token Strategy:</strong><br>
                    ‚Ä¢ High-res images: Up to 1,700 visual tokens<br>
                    ‚Ä¢ Adaptive tokenization: More tokens for complex regions<br>
                    ‚Ä¢ Special boundary tokens: [IMG_START] and [IMG_END]<br>
                    ‚Ä¢ Position encoding: Both spatial (2D) and sequence (1D) positions
                  </div>
                </div>`
            }
          },
          'gemini': {
            dimensions: {
              high: `
                <div class="step">
                  <h4>üìê Gemini's Unified Dimension Strategy</h4>
                  
                  <div class="math-formula">
                    <strong>Native Multimodal Architecture:</strong><br><br>
                    
                    <strong>All Modality Tokens:</strong> d_model = 2048<br>
                    Text_tokens ‚àà ‚Ñù^(M√ó2048)<br>
                    Image_tokens ‚àà ‚Ñù^(N√ó2048)<br>
                    Video_tokens ‚àà ‚Ñù^(T√óN√ó2048)<br>
                    Audio_tokens ‚àà ‚Ñù^(L√ó2048)<br><br>
                    
                    <strong>No Projection Needed:</strong><br>
                    Mixed_Sequence = [t‚ÇÅ, t‚ÇÇ, v‚ÇÅ, v‚ÇÇ, ..., v_N, t‚ÇÉ, t‚ÇÑ]<br>
                    All tokens have identical dimensionality<br><br>
                    
                    <strong>Unified Attention:</strong><br>
                    A = Softmax((QK^T)/‚àö2048)<br>
                    Where Q, K, V can be ANY modality combination
                  </div>
                  
                  <div class="code-block">
                    <div class="code-header">üíª Gemini-Style Unified Processing</div>
                    <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>class GeminiMultimodalModel(nn.Module):
    def __init__(self, d_model=2048, num_layers=64):
        super().__init__()
        
        # All encoders output same dimension
        self.text_embedding = nn.Embedding(256000, d_model)
        self.image_encoder = ViT(output_dim=d_model)  # Native 2048D
        self.video_encoder = ViT3D(output_dim=d_model)  # 3D patches
        self.audio_encoder = Wav2Vec(output_dim=d_model)
        
        # Single unified transformer
        self.transformer_layers = nn.ModuleList([
            TransformerLayer(d_model) for _ in range(num_layers)
        ])
        
        # MoE routing for efficiency
        self.expert_router = MoERouter(d_model, num_experts=8)
    
    def forward(self, batch):
        all_tokens = []
        
        # Process each modality to same dimension
        if 'text' in batch:
            text_tokens = self.text_embedding(batch['text'])
            all_tokens.append(text_tokens)
        
        if 'images' in batch:
            image_tokens = self.image_encoder(batch['images'])
            all_tokens.append(image_tokens)
        
        if 'video' in batch:
            video_tokens = self.video_encoder(batch['video'])
            all_tokens.append(video_tokens)
        
        # Concatenate all modalities - same dimension!
        unified_sequence = torch.cat(all_tokens, dim=1)  # [B, total_len, 2048]
        
        # Process through unified transformer
        for layer in self.transformer_layers:
            unified_sequence = layer(unified_sequence)
        
        return unified_sequence

# Key advantage: No dimension conversion needed!
</pre>
                  </div>
                  
                  <div class="success">
                    <strong>üåü Gemini's Unified Dimension Benefits:</strong><br>
                    ‚Ä¢ <strong>No Information Loss:</strong> No projection bottlenecks between modalities<br>
                    ‚Ä¢ <strong>Perfect Integration:</strong> All modalities treated equally by attention<br>
                    ‚Ä¢ <strong>Simpler Architecture:</strong> No additional projection parameters<br>
                    ‚Ä¢ <strong>Joint Optimization:</strong> End-to-end training across all modalities<br>
                    ‚Ä¢ <strong>Scalability:</strong> Easy to add new modalities (audio, 3D, etc.)<br><br>
                    
                    <strong>‚ö†Ô∏è Trade-offs:</strong><br>
                    ‚Ä¢ Vision encoder constrained to 2048D (may not be optimal)<br>
                    ‚Ä¢ Cannot leverage separately pre-trained components easily<br>
                    ‚Ä¢ Requires massive compute for joint training from scratch
                  </div>
                </div>`
            },
            'training-data': {
              high: `
                <div class="step">
                  <h4>üìä Gemini Training Data Composition</h4>
                  
                  <div class="metric-grid">
                    <div class="metric-card">
                      <div class="metric-value">~100B+</div>
                      <div class="metric-label">Total Multimodal Examples</div>
                    </div>
                    <div class="metric-card">
                      <div class="metric-value">üåê</div>
                      <div class="metric-label">Primary: Web + YouTube</div>
                    </div>
                    <div class="metric-card">
                      <div class="metric-value">$50M+</div>
                      <div class="metric-label">Estimated Training Cost</div>
                    </div>
                    <div class="metric-card">
                      <div class="metric-value">100+</div>
                      <div class="metric-label">Languages Supported</div>
                    </div>
                  </div>
                  
                  <div style="display: grid; gap: 15px; margin: 20px 0;">
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #dc3545;">
                      <strong>üé• YouTube Video Data (~40%)</strong><br>
                      ‚Ä¢ Billions of hours with captions/transcripts<br>
                      ‚Ä¢ Educational content (Khan Academy, Coursera)<br>
                      ‚Ä¢ News and documentary footage<br>
                      ‚Ä¢ User-generated content with permissions<br>
                      ‚Ä¢ Multi-language video content
                    </div>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007bff;">
                      <strong>üåê Google Web Index (~35%)</strong><br>
                      ‚Ä¢ Google Images with surrounding context<br>
                      ‚Ä¢ Web pages with embedded images<br>
                      ‚Ä¢ Google Books with illustrations<br>
                      ‚Ä¢ News articles with photos<br>
                      ‚Ä¢ Shopping/product imagery
                    </div>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #28a745;">
                      <strong>üìö Academic & Research (~15%)</strong><br>
                      ‚Ä¢ ArXiv papers with figures<br>
                      ‚Ä¢ Scientific journals and datasets<br>
                      ‚Ä¢ Educational textbooks<br>
                      ‚Ä¢ Research institution collaborations<br>
                      ‚Ä¢ Government open data
                    </div>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #ffc107;">
                      <strong>üè¢ Google Internal Data (~10%)</strong><br>
                      ‚Ä¢ Google Photos (with user consent)<br>
                      ‚Ä¢ Street View imagery<br>
                      ‚Ä¢ Google Earth satellite data<br>
                      ‚Ä¢ Android camera data (anonymized)<br>
                      ‚Ä¢ Google Workspace documents
                    </div>
                  </div>
                  
                  <div class="info">
                    <strong>üéØ Gemini's Data Advantages:</strong><br>
                    ‚Ä¢ <strong>Scale:</strong> Access to Google's massive web crawl and YouTube<br>
                    ‚Ä¢ <strong>Quality:</strong> Sophisticated filtering using Google's search quality systems<br>
                    ‚Ä¢ <strong>Diversity:</strong> 100+ languages and global cultural representation<br>
                    ‚Ä¢ <strong>Temporal:</strong> Video data provides temporal reasoning capabilities<br>
                    ‚Ä¢ <strong>Multimodal Context:</strong> Rich context from web page structure and video transcripts
                  </div>
                </div>`
            }
          },
          'claude': {
            dimensions: {
              high: `
                <div class="step">
                  <h4>üìê Claude 3 Constitutional Dimension Strategy</h4>
                  
                  <div class="math-formula">
                    <strong>Constitutional Dimension Alignment:</strong><br><br>
                    
                    <strong>Vision Processing:</strong><br>
                    V<sub>raw</sub> = ViT-L/14(image) ‚àà ‚Ñù^(257√ó1024)<br>
                    Safety_mask = ConstitutionalFilter(V<sub>raw</sub>) ‚àà {0,1}^257<br>
                    V<sub>safe</sub> = V<sub>raw</sub> ‚äô Safety_mask<br><br>
                    
                    <strong>Projection with Safety:</strong><br>
                    V<sub>projected</sub> = Constitutional_MLP(V<sub>safe</sub>) ‚àà ‚Ñù^(257√ó4096)<br>
                    Where Constitutional_MLP enforces helpful/harmless constraints<br><br>
                    
                    <strong>Integration Strategy:</strong><br>
                    Tokens = [V<sub>1</sub>, ..., V<sub>256</sub>, T<sub>1</sub>, ..., T<sub>M</sub>]<br>
                    All tokens ‚àà ‚Ñù^4096 after constitutional processing
                  </div>
                  
                  <div class="code-block">
                    <div class="code-header">üíª Claude-Style Constitutional Vision</div>
                    <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>class ConstitutionalVisionProjector(nn.Module):
    def __init__(self, vision_dim=1024, language_dim=4096):
        super().__init__()
        
        # Safety classifier for visual content
        self.safety_classifier = nn.Sequential(
            nn.Linear(vision_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 128),
            nn.ReLU(), 
            nn.Linear(128, 3)  # safe, unsafe, uncertain
        )
        
        # Constitutional projection
        self.projector = nn.Sequential(
            nn.Linear(vision_dim, 2048),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(2048, language_dim),
            # Constitutional constraint: encourage helpful representations
            ConstitutionalNorm(language_dim)
        )
    
    def forward(self, vision_features):
        # Step 1: Safety assessment
        safety_scores = self.safety_classifier(vision_features)
        safety_mask = torch.softmax(safety_scores, dim=-1)[:, :, 0]  # Safe probability
        
        # Step 2: Apply safety filtering
        filtered_features = vision_features * safety_mask.unsqueeze(-1)
        
        # Step 3: Project to language dimension with constitutional constraints
        projected = self.projector(filtered_features)
        
        return projected, safety_mask
</pre>
                  </div>
                  
                  <div class="info">
                    <strong>üõ°Ô∏è Constitutional Architecture Benefits:</strong><br>
                    ‚Ä¢ <strong>Built-in Safety:</strong> Safety considerations integrated at architecture level<br>
                    ‚Ä¢ <strong>Uncertainty Modeling:</strong> Explicit handling of visual ambiguity<br>
                    ‚Ä¢ <strong>Educational Focus:</strong> Optimized for helpful, educational responses<br>
                    ‚Ä¢ <strong>Bias Mitigation:</strong> Constitutional training reduces harmful biases
                  </div>
                </div>`
            },
            'training-data': {
              high: `
                <div class="step">
                  <h4>üìä Claude 3 Training Data Philosophy</h4>
                  
                  <div class="metric-grid">
                    <div class="metric-card">
                      <div class="metric-value">~5B</div>
                      <div class="metric-label">High-Quality Pairs</div>
                    </div>
                    <div class="metric-card">
                      <div class="metric-value">üõ°Ô∏è</div>
                      <div class="metric-label">Safety-First Curation</div>
                    </div>
                    <div class="metric-card">
                      <div class="metric-value">$30M+</div>
                      <div class="metric-label">Estimated Training Cost</div>
                    </div>
                    <div class="metric-card">
                      <div class="metric-value">High</div>
                      <div class="metric-label">Ethical Standards</div>
                    </div>
                  </div>
                  
                  <div style="display: grid; gap: 15px; margin: 20px 0;">
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #28a745;">
                      <strong>üìö Educational & Academic (~40%)</strong><br>
                      ‚Ä¢ Textbooks with diagrams and illustrations<br>
                      ‚Ä¢ Educational video content with permissions<br>
                      ‚Ä¢ Academic papers and research figures<br>
                      ‚Ä¢ Museum collections with proper licensing<br>
                      ‚Ä¢ Educational institution partnerships
                    </div>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007bff;">
                      <strong>üîç Curated Web Content (~30%)</strong><br>
                      ‚Ä¢ High-quality, fact-checked sources<br>
                      ‚Ä¢ News organizations with licensing<br>
                      ‚Ä¢ Government and public domain data<br>
                      ‚Ä¢ Wikipedia with attribution<br>
                      ‚Ä¢ Filtered CommonCrawl (heavy screening)
                    </div>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #ffc107;">
                      <strong>üéØ Constitutional AI Data (~20%)</strong><br>
                      ‚Ä¢ Human-feedback curated examples<br>
                      ‚Ä¢ Constitutional AI training pairs<br>
                      ‚Ä¢ Safety-aligned visual conversations<br>
                      ‚Ä¢ Bias-tested and corrected datasets<br>
                      ‚Ä¢ Diverse cultural representation verification
                    </div>
                    
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #17a2b8;">
                      <strong>ü§ù Licensed Partnerships (~10%)</strong><br>
                      ‚Ä¢ Stock photography with proper licensing<br>
                      ‚Ä¢ Content creator partnerships<br>
                      ‚Ä¢ Educational publisher collaborations<br>
                      ‚Ä¢ Creative Commons with attribution<br>
                      ‚Ä¢ Opt-in user contributions
                    </div>
                  </div>
                  
                  <div class="success">
                    <strong>‚úÖ Claude 3's Ethical Data Approach:</strong><br>
                    ‚Ä¢ <strong>Quality over Quantity:</strong> Smaller but higher-quality training set<br>
                    ‚Ä¢ <strong>Explicit Consent:</strong> Focus on licensed and permissioned content<br>
                    ‚Ä¢ <strong>Bias Mitigation:</strong> Extensive testing and correction for representation<br>
                    ‚Ä¢ <strong>Constitutional Filtering:</strong> All training data screened for constitutional principles
                  </div>
                </div>`
            }
          }
        };

        const details = architectureDetails[modelId];
        if (details && details[analysisLevel]) {
          document.getElementById('architectureDetails').innerHTML = details[analysisLevel][detailLevel];
        }
      }

      // Copy code function
      function copyCode(button) {
        const codeBlock = button.nextElementSibling;
        if (codeBlock && codeBlock.tagName === 'PRE') {
          const text = codeBlock.textContent;
          navigator.clipboard.writeText(text).then(() => {
            const originalText = button.textContent;
            button.textContent = '‚úÖ Copied';
            setTimeout(() => {
              button.textContent = originalText;
            }, 2000);
          });
        }
      }
    </script>
    </div>

    <div class="step">
      <h3>üéØ Key Architectural Differences from CLIP</h3>
      
      <div class="warning">
        <strong>üîÑ CLIP vs Modern VLMs:</strong><br><br>
        
        <strong>CLIP (2021):</strong><br>
        ‚Ä¢ Separate vision + text encoders<br>
        ‚Ä¢ Fixed-size embeddings (512D)<br>
        ‚Ä¢ Contrastive learning only<br>
        ‚Ä¢ Zero-shot classification focus<br><br>
        
        <strong>Modern VLMs (2023+):</strong><br>
        ‚Ä¢ Vision integrated into LLM token stream<br>
        ‚Ä¢ Variable-length visual representations<br>
        ‚Ä¢ Instruction tuning + RLHF<br>
        ‚Ä¢ Conversational reasoning focus
      </div>
    </div>
  </div>

<div class="container">
    <h2>üåü Open Source Vision-Language Models</h2>
    <div class="step">
      <h3>üöÄ The Democratization of Multimodal AI</h3>
      <p>While closed-source models like GPT-4V grab headlines, a <strong>thriving open-source ecosystem</strong> is rapidly closing the performance gap while offering complete transparency, customizability, and cost-effectiveness. These models provide full access to architectures, training code, and datasets - enabling research, customization, and deployment without API dependencies.</p>

      <div class="breakthrough-highlight">
        üèÜ Open Source Revolution: LLaVA-1.6 achieving near GPT-4V performance at 1/200th the training cost ($500K vs $100M+)!
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üîç Open Source VLM Explorer</div>

        <div class="controls">
          <div class="control-group">
            <label>Model Category:</label>
            <select id="openSourceCategory">
              <option value="all" selected>All Models</option>
              <option value="llava">LLaVA Family</option>
              <option value="blip">BLIP Family</option>
              <option value="instructblip">InstructBLIP</option>
              <option value="minigpt">MiniGPT Family</option>
              <option value="efficient">Efficient Models</option>
            </select>
          </div>
          <div class="control-group">
            <label>Comparison Metric:</label>
            <select id="comparisonMetric">
              <option value="performance" selected>Performance vs Cost</option>
              <option value="parameters">Parameter Efficiency</option>
              <option value="training">Training Requirements</option>
              <option value="capabilities">Capability Analysis</option>
            </select>
          </div>
          <div class="control-group">
            <label>Use Case:</label>
            <select id="useCase">
              <option value="research" selected>Research & Experimentation</option>
              <option value="production">Production Deployment</option>
              <option value="education">Educational Use</option>
              <option value="customization">Domain Customization</option>
            </select>
          </div>
        </div>

        <button onclick="exploreOpenSource()" class="primary">üîç Explore Open Source Models</button>
        <div id="openSourceResults"></div>
      </div>
    </div>

    <div class="step">
      <h3>üèóÔ∏è Open Source Architecture Deep Dive</h3>
      
      <div class="model-comparison">
        <div class="model-card" onclick="selectOpenSourceModel('llava', this)">
          <div class="model-name">ü¶ô LLaVA-1.6 (34B)</div>
          <div class="model-specs">
            <strong>Architecture:</strong> Vicuna-34B + CLIP ViT-L/14<br>
            <strong>Innovation:</strong> Simple but effective vision-language instruction tuning<br>
            <strong>Training Cost:</strong> ~$500K (vs $100M+ for GPT-4V)<br>
            <strong>Performance:</strong> 69.5 MMBench, 81.6% VQAv2<br>
            <strong>Open Components:</strong> Full code, weights, training data<br>
            <strong>Customization:</strong> Easy fine-tuning for specific domains
          </div>
          <div class="model-capabilities">
            <div class="capability">
              <span class="capability-icon">üìä</span>
              <span>Competitive general performance</span>
            </div>
            <div class="capability">
              <span class="capability-icon">üí∞</span>
              <span>Ultra-low training cost</span>
            </div>
            <div class="capability">
              <span class="capability-icon">üîß</span>
              <span>Easy customization</span>
            </div>
            <div class="capability">
              <span class="capability-icon">üìö</span>
              <span>Excellent documentation</span>
            </div>
          </div>
        </div>

        <div class="model-card" onclick="selectOpenSourceModel('instructblip', this)">
          <div class="model-name">üéØ InstructBLIP (13B)</div>
          <div class="model-specs">
            <strong>Architecture:</strong> BLIP-2 + Flan-T5/Vicuna<br>
            <strong>Innovation:</strong> Instruction-aware query transformer (Q-Former)<br>
            <strong>Training Cost:</strong> ~$200K<br>
            <strong>Performance:</strong> 64.2 MMBench, 78.9% VQAv2<br>
            <strong>Open Components:</strong> Code, weights, evaluation scripts<br>
            <strong>Strength:</strong> Superior instruction following
          </div>
          <div class="model-capabilities">
            <div class="capability">
              <span class="capability-icon">üéØ</span>
              <span>Excellent instruction following</span>
            </div>
            <div class="capability">
              <span class="capability-icon">üîÄ</span>
              <span>Flexible Q-Former architecture</span>
            </div>
            <div class="capability">
              <span class="capability-icon">‚ö°</span>
              <span>Efficient training pipeline</span>
            </div>
            <div class="capability">
              <span class="capability-icon">üî¨</span>
              <span>Research-friendly</span>
            </div>
          </div>
        </div>

        <div class="model-card" onclick="selectOpenSourceModel('minigpt4', this)">
          <div class="model-name">‚ö° MiniGPT-4 (13B)</div>
          <div class="model-specs">
            <strong>Architecture:</strong> Vicuna + Eva-CLIP + Q-Former<br>
            <strong>Innovation:</strong> Minimal training for conversation ability<br>
            <strong>Training Cost:</strong> ~$100K (extremely efficient)<br>
            <strong>Performance:</strong> 58.7 MMBench, 75.4% VQAv2<br>
            <strong>Open Components:</strong> Complete pipeline + demos<br>
            <strong>Efficiency:</strong> Only ~5M trainable parameters for alignment
          </div>
          <div class="model-capabilities">
            <div class="capability">
              <span class="capability-icon">üí°</span>
              <span>Minimal training approach</span>
            </div>
            <div class="capability">
              <span class="capability-icon">üöÄ</span>
              <span>Fast training (hours vs days)</span>
            </div>
            <div class="capability">
              <span class="capability-icon">üé®</span>
              <span>Creative conversation ability</span>
            </div>
            <div class="capability">
              <span class="capability-icon">üîì</span>
              <span>Completely open</span>
            </div>
          </div>
        </div>
      </div>

      <div id="openSourceModelDetails"></div>
    </div>

    <div class="step">
      <h3>üìä Open vs Closed Source Comparison</h3>
      
      <div class="interactive-demo">
        <div class="demo-title">‚öñÔ∏è Performance vs Cost Analysis</div>

        <div class="controls">
          <div class="control-group">
            <label>Benchmark:</label>
            <select id="benchmarkType">
              <option value="mmmu" selected>MMMU (Multimodal Understanding)</option>
              <option value="vqav2">VQAv2 (Visual Question Answering)</option>
              <option value="gqa">GQA (Scene Graph QA)</option>
              <option value="textvqa">TextVQA (OCR Reasoning)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Cost Factor:</label>
            <select id="costFactor">
              <option value="training" selected>Training Cost</option>
              <option value="inference">Inference Cost</option>
              <option value="total">Total Cost of Ownership</option>
            </select>
          </div>
        </div>

        <button onclick="generateComparison()">üìä Generate Comparison</button>
        <div id="comparisonChart"></div>
      </div>
    </div>

    <div class="step">
      <h3>üíª Implementation: Train Your Own VLM</h3>

      <div class="tabs">
        <div class="tab active" onclick="switchOpenSourceTab('llava-training', this)">LLaVA Training</div>
        <div class="tab" onclick="switchOpenSourceTab('blip-training', this)">BLIP-2 Training</div>
        <div class="tab" onclick="switchOpenSourceTab('custom-training', this)">Custom Architecture</div>
        <div class="tab" onclick="switchOpenSourceTab('deployment', this)">Deployment</div>
      </div>

      <div id="llava-training" class="tab-content active">
        <div class="code-block">
          <div class="code-header">ü¶ô LLaVA-Style Training Pipeline</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre># Complete LLaVA training pipeline
git clone https://github.com/haotian-liu/LLaVA.git
cd LLaVA

# 1. Environment setup
conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip
pip install -e .
pip install -e ".[train]"
pip install flash-attn --no-build-isolation

# 2. Data preparation
# Download LLaVA training data
wget https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_v1_5_mix665k.json

# Prepare your custom dataset (JSON format)
# Format: {"id": "unique_id", "image": "path/to/image.jpg", 
#          "conversations": [{"from": "human", "value": "<image>\nQuestion"},
#                          {"from": "gpt", "value": "Answer"}]}

# 3. Model training (Stage 1: Pretraining)
torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \
    llava/train/train_mem.py \
    --deepspeed ./scripts/zero3.json \
    --model_name_or_path lmsys/vicuna-7b-v1.5 \
    --version v1 \
    --data_path path/to/pretrain_data.json \
    --image_folder path/to/images \
    --vision_tower openai/clip-vit-large-patch14-336 \
    --mm_projector_type mlp2x_gelu \
    --tune_mm_mlp_adapter True \
    --mm_vision_select_layer -2 \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token False \
    --bf16 True \
    --output_dir ./checkpoints/llava-v1.5-7b-pretrain \
    --num_train_epochs 1 \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 24000 \
    --save_total_limit 1 \
    --learning_rate 1e-3 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --tf32 True \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --dataloader_num_workers 4 \
    --lazy_preprocess True \
    --report_to wandb

# 4. Stage 2: Fine-tuning
torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \
    llava/train/train_mem.py \
    --deepspeed ./scripts/zero3.json \
    --model_name_or_path lmsys/vicuna-7b-v1.5 \
    --version v1 \
    --data_path path/to/finetune_data.json \
    --image_folder path/to/images \
    --vision_tower openai/clip-vit-large-patch14-336 \
    --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin \
    --mm_projector_type mlp2x_gelu \
    --mm_vision_select_layer -2 \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token False \
    --image_aspect_ratio pad \
    --group_by_modality_length True \
    --bf16 True \
    --output_dir ./checkpoints/llava-v1.5-7b \
    --num_train_epochs 1 \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 50000 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --tf32 True \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --dataloader_num_workers 4 \
    --lazy_preprocess True \
    --report_to wandb

# 5. Evaluation
python -m llava.eval.model_vqa_loader \
    --model-path ./checkpoints/llava-v1.5-7b \
    --question-file ./playground/data/eval/textvqa/llava_textvqa_val_v051_ocr.jsonl \
    --image-folder ./playground/data/eval/textvqa/train_images \
    --answers-file ./playground/data/eval/textvqa/answers/llava-v1.5-7b.jsonl \
    --temperature 0 \
    --conv-mode vicuna_v1

echo "üéâ LLaVA training complete!"
echo "üí∞ Total cost: ~$500K for 34B model (8x A100 GPUs for ~1 week)"
echo "üìä Expected performance: ~69.5 MMBench, ~81.6% VQAv2"
</pre>
        </div>
      </div>

      <div id="blip-training" class="tab-content">
        <div class="code-block">
          <div class="code-header">üéØ BLIP-2 Training Setup</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre># BLIP-2 training pipeline
git clone https://github.com/salesforce/LAVIS.git
cd LAVIS

# Environment setup
pip install -e .

# Training script for InstructBLIP
python -m torch.distributed.run --nproc_per_node=8 train.py \
  --cfg-path lavis/projects/instructblip/train/vicuna7b_instruct.yaml

# Custom configuration for your data
# Create custom config YAML:
model:
  arch: blip2_vicuna_instruct
  model_type: vicuna7b
  load_finetuned: False
  load_pretrained: True
  pretrained: "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth"
  
datasets:
  custom_vqa:
    vis_processor:
      train:
        name: "blip2_image_train"
        image_size: 224
    text_processor:
      train:
        name: "blip_question"
    data_type: images
    build_info:
      annotations:
        train:
          url: path/to/your_train_data.json
          storage: path/to/your_images/
      images:
        storage: path/to/your_images/

run:
  task: vqa
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-5
  min_lr: 0
  warmup_lr: 1e-8
  warmup_steps: 1000
  weight_decay: 0.05
  max_epoch: 5
  batch_size_train: 16
  batch_size_eval: 8
  num_workers: 4
  accum_grad_iters: 1
  
  seed: 42
  output_dir: "output/instructblip_custom"
  
  amp: True
  resume_ckpt_path: null
  
  evaluate: False 
  train_splits: ["train"]
  
  device: "cuda"
  world_size: 1
  dist_url: "env://"
  distributed: True

echo "üéØ InstructBLIP training configured!"
echo "üí° Key advantage: Q-Former allows flexible vision-text interaction"
</pre>
        </div>
      </div>

      <div id="custom-training" class="tab-content">
        <div class="code-block">
          <div class="code-header">üõ†Ô∏è Build Your Own VLM Architecture</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import torch
import torch.nn as nn
from transformers import (
    CLIPVisionModel, CLIPImageProcessor,
    LlamaForCausalLM, LlamaTokenizer
)

class CustomVLM(nn.Module):
    """
    Custom Vision-Language Model - Build your own!
    """
    def __init__(self, 
                 vision_model="openai/clip-vit-base-patch32",
                 language_model="meta-llama/Llama-2-7b-hf",
                 projection_type="mlp"):
        super().__init__()
        
        # Vision components
        self.vision_model = CLIPVisionModel.from_pretrained(vision_model)
        self.vision_processor = CLIPImageProcessor.from_pretrained(vision_model)
        
        # Language components  
        self.language_model = LlamaForCausalLM.from_pretrained(language_model)
        self.tokenizer = LlamaTokenizer.from_pretrained(language_model)
        
        # Vision-Language projection
        vision_dim = self.vision_model.config.hidden_size
        language_dim = self.language_model.config.hidden_size
        
        if projection_type == "mlp":
            self.projector = nn.Sequential(
                nn.Linear(vision_dim, language_dim),
                nn.GELU(),
                nn.Linear(language_dim, language_dim)
            )
        elif projection_type == "q_former":
            # Implement Q-Former style projection
            self.projector = self._build_q_former(vision_dim, language_dim)
        
        # Special tokens
        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        self.tokenizer.add_tokens(['<image>'])
        self.language_model.resize_token_embeddings(len(self.tokenizer))
        
        # Freeze vision model initially
        for param in self.vision_model.parameters():
            param.requires_grad = False
    
    def _build_q_former(self, vision_dim, language_dim, num_queries=32):
        """Build Q-Former style cross-attention"""
        from transformers import BertConfig, BertLMHeadModel
        
        # Q-Former configuration
        encoder_config = BertConfig(
            vocab_size=30522,
            hidden_size=768,
            num_hidden_layers=12,
            num_attention_heads=12,
            intermediate_size=3072,
            add_cross_attention=True,
        )
        
        q_former = BertLMHeadModel(config=encoder_config)
        
        # Learnable query tokens
        self.query_tokens = nn.Parameter(
            torch.zeros(1, num_queries, 768)
        )
        self.query_tokens.data.normal_(mean=0.0, std=0.02)
        
        return q_former
    
    def forward(self, images, text_input):
        """Forward pass through custom VLM"""
        batch_size = images.size(0)
        
        # Process images
        with torch.no_grad():
            vision_outputs = self.vision_model(images)
            image_features = vision_outputs.last_hidden_state
        
        # Project to language space
        projected_features = self.projector(image_features)
        
        # Process text
        text_tokens = self.tokenizer(
            text_input, 
            return_tensors="pt", 
            padding=True, 
            truncation=True
        )
        
        # Find image token positions and replace with visual features
        # (Implementation details for token replacement)
        
        # Forward through language model
        outputs = self.language_model(
            inputs_embeds=combined_embeddings,
            attention_mask=attention_mask
        )
        
        return outputs

# Training setup
def train_custom_vlm():
    model = CustomVLM()
    
    # Your custom training loop
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=1e-4, 
        weight_decay=0.05
    )
    
    for epoch in range(num_epochs):
        for batch in dataloader:
            images, texts, labels = batch
            
            outputs = model(images, texts)
            loss = compute_loss(outputs, labels)
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
    
    return model

# Usage
model = train_custom_vlm()
print("üõ†Ô∏è Custom VLM trained successfully!")
</pre>
        </div>
      </div>

      <div id="deployment" class="tab-content">
        <div class="code-block">
          <div class="code-header">üöÄ Production Deployment</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre># Production deployment setup
import gradio as gr
from transformers import LlavaForConditionalGeneration, AutoProcessor
import torch

class VLMInferenceServer:
    def __init__(self, model_path="llava-hf/llava-1.5-7b-hf"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Load model and processor
        self.model = LlavaForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.processor = AutoProcessor.from_pretrained(model_path)
    
    def inference(self, image, text_prompt):
        """Run inference on image + text"""
        prompt = f"USER: <image>\n{text_prompt} ASSISTANT:"
        
        inputs = self.processor(
            prompt, 
            image, 
            return_tensors='pt'
        ).to(self.device, torch.float16)
        
        with torch.no_grad():
            output = self.model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=False
            )
        
        response = self.processor.decode(
            output[0][2:], 
            skip_special_tokens=True
        )
        
        return response

# Gradio interface
vlm_server = VLMInferenceServer()

def process_image_text(image, text):
    if image is None:
        return "Please upload an image."
    return vlm_server.inference(image, text)

# Create Gradio app
demo = gr.Interface(
    fn=process_image_text,
    inputs=[
        gr.Image(type="pil", label="Upload Image"),
        gr.Textbox(label="Ask a question about the image", 
                  placeholder="Describe this image in detail...")
    ],
    outputs=gr.Textbox(label="AI Response"),
    title="ü¶ô LLaVA Vision-Language Model",
    description="Upload an image and ask questions about it!",
    examples=[
        ["example1.jpg", "What's happening in this image?"],
        ["example2.jpg", "What colors do you see?"],
        ["example3.jpg", "Describe the scene in detail."]
    ]
)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True  # Creates public link
    )

# Docker deployment
"""
# Dockerfile
FROM pytorch/pytorch:2.1.0-cuda11.8-devel

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 7860

CMD ["python", "app.py"]
"""

# Kubernetes deployment
"""
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vlm-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: vlm
  template:
    metadata:
      labels:
        app: vlm
    spec:
      containers:
      - name: vlm
        image: your-registry/vlm:latest
        ports:
        - containerPort: 7860
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            memory: "8Gi"
            cpu: "2"
"""

print("üöÄ VLM deployment ready!")
print("üí∞ Deployment costs: ~$0.10/hour (vs $10+/hour for GPT-4V API)")
print("üîß Full control over model, data, and infrastructure")
</pre>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>üéØ Open Source Success Stories</h3>
      
      <div class="timeline">
        <div class="timeline-item">
          <div class="timeline-date">2023 Q2</div>
          <div class="timeline-content">
            <h4>ü¶ô LLaVA Launch</h4>
            <p>First competitive open-source VLM achieves 85%+ of GPT-4V performance with simple architecture. Demonstrates that effective VLMs don't require massive proprietary infrastructure.</p>
          </div>
        </div>
        
        <div class="timeline-item">
          <div class="timeline-date">2023 Q4</div>
          <div class="timeline-content">
            <h4>üéØ InstructBLIP Breakthrough</h4>
            <p>Salesforce's Q-Former architecture shows how to effectively bridge vision and language modalities, inspiring numerous follow-up models and research directions.</p>
          </div>
        </div>
        
        <div class="timeline-item">
          <div class="timeline-date">2024 Q1</div>
          <div class="timeline-content">
            <h4>‚ö° MiniGPT-4 Efficiency</h4>
            <p>Demonstrates that high-quality VLM capabilities can be achieved with minimal training - just 5M parameters for vision-language alignment, training in hours instead of days.</p>
          </div>
        </div>
        
        <div class="timeline-item">
          <div class="timeline-date">2024 Q3</div>
          <div class="timeline-content">
            <h4>üöÄ LLaVA-1.6 Performance Leap</h4>
            <p>Closes gap with GPT-4V on many benchmarks while maintaining complete transparency. Proves open-source can compete with the best closed-source systems.</p>
          </div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>üîÆ The Open Source Advantage</h3>
      
      <div class="success">
        <strong>‚úÖ Why Choose Open Source VLMs:</strong><br><br>
        
        <strong>üî¨ Research & Development:</strong><br>
        ‚Ä¢ Complete architecture transparency for research and improvement<br>
        ‚Ä¢ Ability to experiment with novel architectures and training methods<br>
        ‚Ä¢ Academic collaboration and reproducible research<br>
        ‚Ä¢ Understanding of model behavior and failure modes<br><br>
        
        <strong>üí∞ Cost Effectiveness:</strong><br>
        ‚Ä¢ Training costs 100-1000x lower than closed-source alternatives<br>
        ‚Ä¢ No ongoing API fees - pay only for compute infrastructure<br>
        ‚Ä¢ Bulk processing without per-request charges<br>
        ‚Ä¢ Predictable operational expenses<br><br>
        
        <strong>üîß Customization & Control:</strong><br>
        ‚Ä¢ Fine-tune for specific domains (medical, legal, scientific)<br>
        ‚Ä¢ Modify architectures for specific hardware constraints<br>
        ‚Ä¢ Control over training data and model behavior<br>
        ‚Ä¢ Deploy anywhere without external dependencies<br><br>
        
        <strong>üõ°Ô∏è Privacy & Security:</strong><br>
        ‚Ä¢ Keep sensitive data on-premises<br>
        ‚Ä¢ No data sharing with external APIs<br>
        ‚Ä¢ Full audit trail of model decisions<br>
        ‚Ä¢ Compliance with data governance requirements
      </div>

      <div class="warning">
        <strong>‚ö†Ô∏è Open Source Considerations:</strong><br>
        ‚Ä¢ Requires more technical expertise to deploy and maintain<br>
        ‚Ä¢ Need to manage your own compute infrastructure<br>
        ‚Ä¢ Performance may lag cutting-edge closed-source models<br>
        ‚Ä¢ Responsible for model safety and bias mitigation<br>
        ‚Ä¢ Limited customer support compared to commercial products
      </div>
    </div>

    <div class="step">
      <h3>üéì Key Takeaways: Open Source VLMs</h3>
      
      <div class="breakthrough-highlight">
        üåü The Future is Open: Open source VLMs are rapidly approaching parity with closed-source systems while offering transparency, customization, and cost-effectiveness that commercial APIs cannot match!
      </div>

      <div class="info">
        <strong>üéØ Choosing Your VLM Strategy:</strong><br><br>
        
        <strong>Choose Closed-Source (GPT-4V, Gemini, Claude) for:</strong><br>
        ‚Ä¢ Rapid prototyping and getting started quickly<br>
        ‚Ä¢ Maximum performance on cutting-edge capabilities<br>
        ‚Ä¢ Teams without ML infrastructure expertise<br>
        ‚Ä¢ Applications requiring the absolute latest features<br><br>
        
        <strong>Choose Open-Source (LLaVA, InstructBLIP, etc.) for:</strong><br>
        ‚Ä¢ Research and academic projects<br>
        ‚Ä¢ Cost-sensitive production deployments<br>
        ‚Ä¢ Custom domain applications<br>
        ‚Ä¢ Privacy-critical use cases<br>
        ‚Ä¢ Long-term strategic control over AI capabilities<br><br>
        
        <strong>üí° Hybrid Approach:</strong><br>
        Many organizations start with closed-source APIs for prototyping, then transition to fine-tuned open-source models for production once requirements are clear!
      </div>
    </div>
  </div>
	
  <div class="container">
    <h2>üîó Visual Token Integration: The Technical Core</h2>
    <div class="step">
      <h3>üß© From Pixels to Tokens: The Mathematical Pipeline</h3>
      <p>The key innovation in modern VLMs is treating <strong>image patches as tokens</strong> that can be processed alongside text tokens in a unified transformer architecture. This enables seamless multimodal reasoning.</p>

      <div class="math-formula">
        <strong>Visual Token Integration Pipeline:</strong><br><br>
        
        <strong>1. Image Patch Extraction:</strong><br>
        X<sub>img</sub> ‚àà ‚Ñù<sup>H√óW√óC</sup> ‚Üí {p‚ÇÅ, p‚ÇÇ, ..., p<sub>N</sub>} where N = (H√óW)/P¬≤<br>
        <em>Split image into patches of size P√óP (typically 14√ó14 or 16√ó16)</em><br><br>
        
        <strong>2. Visual Token Encoding:</strong><br>
        v<sub>i</sub> = Linear(Flatten(p<sub>i</sub>)) + PE<sub>i</sub> ‚àà ‚Ñù<sup>d</sup><br>
        <em>Each patch becomes a d-dimensional token with positional encoding</em><br><br>
        
        <strong>3. Token Sequence Construction:</strong><br>
        Tokens = [v‚ÇÅ, v‚ÇÇ, ..., v<sub>N</sub>, w‚ÇÅ, w‚ÇÇ, ..., w<sub>M</sub>]<br>
        <em>Concatenate visual tokens with text tokens</em><br><br>
        
        <strong>4. Unified Attention:</strong><br>
        Attention(Q, K, V) where Q, K, V include both visual and text tokens<br>
        <em>Cross-modal attention enables reasoning across modalities</em>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üé≠ Visual Token Visualization</div>
        <p><strong>Explore how images become token sequences:</strong> See how a 224√ó224 image gets converted into hundreds of tokens that flow through the language model.</p>

        <div class="controls">
          <div class="control-group">
            <label>Image Resolution:</label>
            <select id="imageResolution">
              <option value="224">224√ó224 (Standard)</option>
              <option value="336" selected>336√ó336 (High-res)</option>
              <option value="448">448√ó448 (Ultra-high-res)</option>
              <option value="512">512√ó512 (Maximum)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Patch Size:</label>
            <select id="patchSize">
              <option value="14" selected>14√ó14 (GPT-4V style)</option>
              <option value="16">16√ó16 (ViT standard)</option>
              <option value="32">32√ó32 (Efficient)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Text Input:</label>
            <textarea id="textInput" rows="3" placeholder="Describe this image in detail">Describe this image in detail</textarea>
          </div>
        </div>

        <button onclick="generateTokenSequence()" class="primary">üé≠ Generate Token Sequence</button>
        <div id="tokenVisualization"></div>
      </div>
    </div>

    <div class="step">
      <h3>‚ö° Fusion Strategies: How Vision and Language Interact</h3>
      <p>Different VLMs use different strategies to combine visual and textual information. The fusion method dramatically impacts model capabilities and computational efficiency.</p>

      <div class="interactive-demo">
        <div class="demo-title">üîÄ Fusion Strategy Explorer</div>

        <div class="fusion-strategies">
          <div class="fusion-card" onclick="selectFusion('early', this)">
            <div class="fusion-title">üîÄ Early Fusion</div>
            <div class="fusion-description">Mix visual and text tokens from the start</div>
            <div class="fusion-math">
              Input: [IMG‚ÇÅ, IMG‚ÇÇ, ..., TXT‚ÇÅ, TXT‚ÇÇ, ...]<br>
              All tokens processed together
            </div>
          </div>
          
          <div class="fusion-card" onclick="selectFusion('late', this)">
            <div class="fusion-title">üîö Late Fusion</div>
            <div class="fusion-description">Process modalities separately, combine at end</div>
            <div class="fusion-math">
              Vision: f_v(IMG) ‚Üí v_embed<br>
              Text: f_t(TXT) ‚Üí t_embed<br>
              Fusion: g(v_embed, t_embed)
            </div>
          </div>
          
          <div class="fusion-card" onclick="selectFusion('cross', this)">
            <div class="fusion-title">üîÑ Cross-Attention</div>
            <div class="fusion-description">Dedicated cross-modal attention layers</div>
            <div class="fusion-math">
              Q_text, K_vision, V_vision<br>
              Attention(Q_t, K_v, V_v)
            </div>
          </div>
          
          <div class="fusion-card" onclick="selectFusion('adaptive', this)">
            <div class="fusion-title">üéõÔ∏è Adaptive Fusion</div>
            <div class="fusion-description">Learn when and how to fuse modalities</div>
            <div class="fusion-math">
              Œ± = Sigmoid(W[v;t])<br>
              Output = Œ±*v + (1-Œ±)*t
            </div>
          </div>
        </div>

        <div id="fusionAnalysis"></div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üß† Attention Mechanisms: Cross-Modal Understanding</h2>
    <div class="step">
      <h3>üëÅÔ∏è How Models "See" and "Understand" Together</h3>
      <p>The magic of VLMs lies in their attention mechanisms - how they learn to focus on relevant parts of images while processing text, and how visual information influences text generation.</p>

      <div class="interactive-demo">
        <div class="demo-title">üîç Cross-Modal Attention Visualizer</div>

        <div class="controls">
          <div class="control-group">
            <label>Query Text:</label>
            <input type="text" id="queryText" value="What color is the car?" placeholder="Enter your question">
          </div>
          <div class="control-group">
            <label>Image Content:</label>
            <select id="imageContent">
              <option value="car" selected>Red car on street</option>
              <option value="nature">Forest landscape</option>
              <option value="chart">Business chart</option>
              <option value="text">Document with text</option>
            </select>
          </div>
          <div class="control-group">
            <label>Attention Head:</label>
            <select id="attentionHead">
              <option value="1">Head 1 (Object focus)</option>
              <option value="2" selected>Head 2 (Color/texture)</option>
              <option value="3">Head 3 (Spatial relations)</option>
              <option value="4">Head 4 (Text reading)</option>
            </select>
          </div>
        </div>

        <button onclick="visualizeAttention()">üîç Visualize Cross-Modal Attention</button>
        <div id="attentionVisualization"></div>
      </div>
    </div>

    <div class="step">
      <h3>üìä Attention Pattern Analysis</h3>
      <div class="math-formula">
        <strong>Cross-Modal Attention Mathematics:</strong><br><br>
        
        <strong>Text-to-Vision Attention:</strong><br>
        Q<sub>text</sub> = W<sub>Q</sub> √ó H<sub>text</sub><br>
        K<sub>vision</sub> = W<sub>K</sub> √ó H<sub>vision</sub><br>
        V<sub>vision</sub> = W<sub>V</sub> √ó H<sub>vision</sub><br><br>
        
        <strong>Attention Scores:</strong><br>
        A<sub>t‚Üív</sub> = Softmax(Q<sub>text</sub> √ó K<sub>vision</sub><sup>T</sup> / ‚àöd)<br><br>
        
        <strong>Attended Visual Features:</strong><br>
        V<sub>attended</sub> = A<sub>t‚Üív</sub> √ó V<sub>vision</sub><br><br>
        
        <strong>Key Insight:</strong> Each text token can attend to different image regions,<br>
        enabling fine-grained visual reasoning!
      </div>
    </div>
  </div>
<!-- Add this section after the "Attention Mechanisms" section and before "Performance Benchmarks" -->

<div class="container">
  <h2>üõ°Ô∏è Constitutional AI for Vision-Language Models</h2>
  <div class="step">
    <h3>‚öñÔ∏è What is Constitutional AI?</h3>
    <p>Constitutional AI (CAI) is a training methodology that teaches AI systems to follow a set of principles (a "constitution") and <strong>self-correct their behavior</strong>. Instead of relying entirely on human feedback, models learn to critique and improve their own responses based on explicit moral and behavioral guidelines.</p>

    <div class="breakthrough-highlight">
      üåü Key Insight: Rather than "What do humans prefer?", Constitutional AI asks "What principles should guide AI behavior?" and teaches models to apply those principles consistently.
    </div>

    <div class="interactive-demo">
      <div class="demo-title">üèõÔ∏è Constitutional AI Foundation</div>
      
      <div class="tabs">
        <div class="tab active" onclick="switchCAITab('basics', this)">üèóÔ∏è CAI Basics</div>
        <div class="tab" onclick="switchCAITab('process', this)">üîÑ Training Process</div>
        <div class="tab" onclick="switchCAITab('principles', this)">üìú Constitution</div>
        <div class="tab" onclick="switchCAITab('comparison', this)">‚öñÔ∏è CAI vs RLHF</div>
      </div>

      <div id="basics" class="tab-content active">
        <div class="math-formula">
          <strong>Traditional RLHF Problem:</strong><br><br>
          Human Feedback: "This response is better" ‚Üí ‚ùì <em>But why?</em><br><br>
          
          <strong>Constitutional AI Solution:</strong><br><br>
          Constitution: "Be helpful, harmless, and honest"<br>
          Model learns: <em>How</em> to evaluate responses against principles<br>
          Self-correction: Model improves its own outputs
        </div>
        
        <div class="architecture-flow">
          <div class="arch-component vision">
            <h4>üìú Constitution</h4>
            <div>Written Principles</div>
            <div style="font-size:12px;margin-top:5px">
              ‚Ä¢ Be helpful & educational<br>
              ‚Ä¢ Avoid harm & bias<br>
              ‚Ä¢ Respect privacy & safety
            </div>
          </div>
          
          <div class="arch-arrow">‚Üí</div>
          
          <div class="arch-component fusion">
            <h4>ü§ñ AI Self-Critique</h4>
            <div>Evaluate Own Response</div>
            <div style="font-size:12px;margin-top:5px">
              ‚Ä¢ Apply principles<br>
              ‚Ä¢ Identify issues<br>
              ‚Ä¢ Generate improvements
            </div>
          </div>
          
          <div class="arch-arrow">‚Üí</div>
          
          <div class="arch-component output">
            <h4>‚ú® Improved Response</h4>
            <div>Self-Corrected Output</div>
            <div style="font-size:12px;margin-top:5px">
              ‚Ä¢ Better aligned<br>
              ‚Ä¢ More helpful<br>
              ‚Ä¢ Safer & more educational
            </div>
          </div>
        </div>
      </div>

      <div id="process" class="tab-content">
        <div class="step">
          <h4>üîÑ Constitutional AI Training Process</h4>
          
          <div class="timeline">
            <div class="timeline-item">
              <div class="timeline-date">Step 1</div>
              <div class="timeline-content">
                <h5>üìù Initial Response Generation</h5>
                <p>Model generates initial response to prompt using standard training</p>
              </div>
            </div>
            
            <div class="timeline-item">
              <div class="timeline-date">Step 2</div>
              <div class="timeline-content">
                <h5>üîç Constitutional Critique</h5>
                <p>Model evaluates its own response against constitutional principles</p>
              </div>
            </div>
            
            <div class="timeline-item">
              <div class="timeline-date">Step 3</div>
              <div class="timeline-content">
                <h5>‚úèÔ∏è Self-Revision</h5>
                <p>Model generates improved response based on its self-critique</p>
              </div>
            </div>
            
            <div class="timeline-item">
              <div class="timeline-date">Step 4</div>
              <div class="timeline-content">
                <h5>üìà Reinforcement Learning</h5>
                <p>Model learns from its own improved responses via RL</p>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div id="principles" class="tab-content">
        <div class="step">
          <h4>üìú Example Constitutional Principles</h4>
          
          <div style="display: grid; gap: 15px; margin: 20px 0;">
            <div style="background: #d4edda; padding: 15px; border-radius: 8px; border-left: 4px solid #28a745;">
              <strong>üéØ Helpfulness Principles</strong><br>
              ‚Ä¢ "Choose the response that is most helpful and educational"<br>
              ‚Ä¢ "Provide specific, actionable information when appropriate"<br>
              ‚Ä¢ "Acknowledge uncertainty rather than guessing"
            </div>
            
            <div style="background: #fff3cd; padding: 15px; border-radius: 8px; border-left: 4px solid #ffc107;">
              <strong>üõ°Ô∏è Harmlessness Principles</strong><br>
              ‚Ä¢ "Avoid content that could be used to harm others"<br>
              ‚Ä¢ "Don't provide information for illegal activities"<br>
              ‚Ä¢ "Be especially careful with content involving minors"
            </div>
            
            <div style="background: #d1ecf1; padding: 15px; border-radius: 8px; border-left: 4px solid #17a2b8;">
              <strong>üíé Honesty Principles</strong><br>
              ‚Ä¢ "Be truthful and accurate in your responses"<br>
              ‚Ä¢ "Admit when you don't know something"<br>
              ‚Ä¢ "Correct mistakes when identified"
            </div>
          </div>
        </div>
      </div>

      <div id="comparison" class="tab-content">
        <table class="benchmark-table">
          <tr>
            <th>Aspect</th>
            <th>Traditional RLHF</th>
            <th>Constitutional AI</th>
          </tr>
          <tr>
            <td><strong>Feedback Source</strong></td>
            <td>Human preferences</td>
            <td>AI self-evaluation + principles</td>
          </tr>
          <tr>
            <td><strong>Scalability</strong></td>
            <td>Limited by human bandwidth</td>
            <td>Scales with model capability</td>
          </tr>
          <tr>
            <td><strong>Consistency</strong></td>
            <td>Variable human judgment</td>
            <td>Consistent principle application</td>
          </tr>
          <tr>
            <td><strong>Transparency</strong></td>
            <td>Opaque human preferences</td>
            <td>Explicit written principles</td>
          </tr>
          <tr>
            <td><strong>Cost</strong></td>
            <td>High (human labor)</td>
            <td>Lower (automated)</td>
          </tr>
        </table>
      </div>
    </div>
  </div>

  <div class="step">
    <h3>üëÅÔ∏è Constitutional AI for Vision: The Complexity Explosion</h3>
    <p>Applying Constitutional AI to VLMs introduces <strong>multimodal challenges</strong> that don't exist in text-only systems. The model must evaluate both visual content and its textual responses about that content.</p>

    <div class="interactive-demo">
      <div class="demo-title">üîç VLM Constitutional Challenge Explorer</div>

      <div class="controls">
        <div class="control-group">
          <label>Challenge Type:</label>
          <select id="challengeType">
            <option value="hallucination" selected>Visual Hallucination</option>
            <option value="privacy">Privacy & Identification</option>
            <option value="bias">Cultural Bias</option>
            <option value="medical">Medical/Safety Claims</option>
            <option value="accuracy">Factual Accuracy</option>
          </select>
        </div>
        <div class="control-group">
          <label>Model Scenario:</label>
          <select id="modelScenario">
            <option value="before" selected>Before Constitutional Training</option>
            <option value="critique">During Self-Critique</option>
            <option value="after">After Constitutional Training</option>
          </select>
        </div>
      </div>

      <button onclick="demonstrateConstitutionalChallenge()" class="primary">üé≠ Show Constitutional AI in Action</button>
      <div id="constitutionalDemo"></div>
    </div>
  </div>

  <div class="step">
    <h3>üî¨ VLM Constitutional Training Pipeline</h3>
    
    <div class="math-formula">
      <strong>VLM Constitutional AI Training Mathematics:</strong><br><br>
      
      <strong>Phase 1 - Supervised Constitutional Learning:</strong><br>
      ‚Ñí<sub>SL</sub> = -ùîº<sub>(img,txt,y)</sub>[log P(y<sub>revised</sub> | img, txt, critique(y<sub>initial</sub>))]<br>
      <em>Train on self-revised responses based on constitutional critique</em><br><br>
      
      <strong>Phase 2 - Constitutional RL:</strong><br>
      ‚Ñí<sub>CAI</sub> = ùîº<sub>œÄ</sub>[R<sub>constitutional</sub>(img, txt, response)] - Œ≤ KL(œÄ, œÄ<sub>SL</sub>)<br>
      <em>Where R<sub>constitutional</sub> = Constitutional_Judge(img, txt, response)</em><br><br>
      
      <strong>Constitutional Reward Model:</strong><br>
      R<sub>constitutional</sub> = Œ±¬∑R<sub>visual_accuracy</sub> + Œ≤¬∑R<sub>safety</sub> + Œ≥¬∑R<sub>helpfulness</sub><br>
      <em>Multi-objective optimization across constitutional principles</em>
    </div>

    <div class="code-block">
      <div class="code-header">üõ°Ô∏è VLM Constitutional Training Implementation</div>
      <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>class VLMConstitutionalTrainer:
    def __init__(self, vlm_model, constitution):
        self.vlm = vlm_model
        self.constitution = constitution
        
        # Visual safety classifiers
        self.visual_safety_classifier = self._build_safety_classifier()
        self.privacy_detector = self._build_privacy_detector()
        
        # Constitutional reward models
        self.visual_accuracy_judge = self._build_accuracy_judge()
        self.helpfulness_judge = self._build_helpfulness_judge()
        
    def constitutional_training_step(self, image, prompt):
        """Single constitutional training step"""
        
        # 1. Generate initial response
        initial_response = self.vlm.generate(image, prompt)
        
        # 2. Multi-dimensional constitutional critique
        critique = self.comprehensive_critique(image, prompt, initial_response)
        
        # 3. Self-revision based on critique
        revision_prompt = f"""
        Based on this critique of my response:
        {critique}
        
        Please provide an improved response to: {prompt}
        [Image shown]
        
        Focus on being more accurate, helpful, and safe.
        """
        
        revised_response = self.vlm.generate(image, revision_prompt)
        
        # 4. Constitutional reward calculation
        reward = self.calculate_constitutional_reward(
            image, prompt, revised_response
        )
        
        return {
            'initial': initial_response,
            'critique': critique,
            'revised': revised_response,
            'reward': reward
        }
    
    def comprehensive_critique(self, image, prompt, response):
        """Multi-faceted constitutional critique for VLMs"""
        
        # Visual accuracy critique
        visual_critique = self._critique_visual_accuracy(image, response)
        
        # Safety critique
        safety_critique = self._critique_safety(image, prompt, response)
        
        # Helpfulness critique
        help_critique = self._critique_helpfulness(prompt, response)
        
        # Privacy critique
        privacy_critique = self._critique_privacy(image, response)
        
        return {
            'visual_accuracy': visual_critique,
            'safety': safety_critique,
            'helpfulness': help_critique,
            'privacy': privacy_critique
        }
    
    def _critique_visual_accuracy(self, image, response):
        """Evaluate factual accuracy of visual descriptions"""
        accuracy_prompt = f"""
        Constitutional Principle: "Describe only what you can actually see"
        
        My response: {response}
        
        Critique questions:
        1. Did I describe anything not visible in the image?
        2. Did I miss important visual details?
        3. Did I make unsupported inferences?
        4. How can I improve factual accuracy?
        """
        
        return self.vlm.generate(image, accuracy_prompt)
    
    def _critique_safety(self, image, prompt, response):
        """Evaluate safety implications"""
        safety_prompt = f"""
        Constitutional Principles: 
        - "Avoid content that could cause harm"
        - "Don't identify private individuals"
        - "Don't provide medical diagnoses"
        
        Original question: {prompt}
        My response: {response}
        
        Safety evaluation:
        1. Could my response cause harm if followed?
        2. Did I identify someone who should remain private?
        3. Did I provide medical advice inappropriately?
        4. How can I be more responsible?
        """
        
        return self.vlm.generate(image, safety_prompt)
    
    def _critique_helpfulness(self, prompt, response):
        """Evaluate educational value and helpfulness"""
        help_prompt = f"""
        Constitutional Principle: "Be maximally helpful and educational"
        
        User question: {prompt}
        My response: {response}
        
        Helpfulness evaluation:
        1. Did I directly address the user's question?
        2. Could I provide more educational context?
        3. Is my response at the appropriate detail level?
        4. How can I be more useful?
        """
        
        return self.vlm.generate(None, help_prompt)  # Text-only critique
    
    def calculate_constitutional_reward(self, image, prompt, response):
        """Calculate multi-dimensional constitutional reward"""
        
        # Component rewards
        accuracy_reward = self.visual_accuracy_judge(image, response)
        safety_reward = self.safety_judge(image, prompt, response)
        helpfulness_reward = self.helpfulness_judge(prompt, response)
        
        # Weighted combination (adjust weights based on application)
        total_reward = (
            0.4 * accuracy_reward +      # Visual accuracy is critical
            0.35 * safety_reward +       # Safety is paramount
            0.25 * helpfulness_reward    # Helpfulness completes the picture
        )
        
        return total_reward

# Example usage in training loop
trainer = VLMConstitutionalTrainer(vlm_model, CLAUDE_CONSTITUTION)

for epoch in range(num_epochs):
    for batch in constitutional_training_data:
        for image, prompt in batch:
            result = trainer.constitutional_training_step(image, prompt)
            
            # Train model on constitutionally-improved responses
            loss = compute_loss(result['revised'], target_response)
            
            # Add constitutional reward to loss
            total_loss = loss - lambda_constitutional * result['reward']
            total_loss.backward()
</pre>
    </div>
  </div>

  <div class="step">
    <h3>üëÅÔ∏è VLM-Specific Constitutional Challenges</h3>
    <p>Vision-Language Models face unique constitutional challenges that don't exist in text-only systems. These require specialized principles and training approaches.</p>

    <div class="interactive-demo">
      <div class="demo-title">üéØ VLM Constitutional Challenges</div>

      <div class="controls">
        <div class="control-group">
          <label>Challenge Category:</label>
          <select id="vlmChallenge">
            <option value="hallucination" selected>Visual Hallucination</option>
            <option value="privacy">Privacy & Face Recognition</option>
            <option value="medical">Medical Image Analysis</option>
            <option value="bias">Cultural & Social Bias</option>
            <option value="misinformation">Visual Misinformation</option>
          </select>
        </div>
        <div class="control-group">
          <label>Constitutional Response:</label>
          <select id="constitutionalApproach">
            <option value="prevention" selected>Prevention Strategy</option>
            <option value="detection">Detection Method</option>
            <option value="correction">Correction Process</option>
          </select>
        </div>
      </div>

      <button onclick="exploreVLMChallenge()" class="primary">üîç Explore Constitutional Solution</button>
      <div id="vlmChallengeResults"></div>
    </div>
  </div>

  <div class="step">
    <h3>üõ°Ô∏è Constitutional AI in Production VLMs</h3>
    
    <div class="model-comparison">
      <div class="model-card" onclick="selectConstitutionalModel('claude', this)">
        <div class="model-name">ü§ñ Claude's Constitutional Vision</div>
        <div class="model-specs">
          <strong>Approach:</strong> Built-in safety at architecture level<br>
          <strong>Training:</strong> Constitutional principles from initialization<br>
          <strong>Safety Features:</strong> Visual content filtering, privacy protection<br>
          <strong>Self-Correction:</strong> Real-time constitutional evaluation<br>
          <strong>Transparency:</strong> Explicit about constitutional reasoning
        </div>
      </div>

      <div class="model-card" onclick="selectConstitutionalModel('gpt4v', this)">
        <div class="model-name">ü§ñ GPT-4V's Safety Approach</div>
        <div class="model-specs">
          <strong>Approach:</strong> Post-hoc safety filtering + RLHF<br>
          <strong>Training:</strong> Traditional RLHF with safety considerations<br>
          <strong>Safety Features:</strong> Content policy enforcement, usage monitoring<br>
          <strong>Self-Correction:</strong> Limited constitutional self-awareness<br>
          <strong>Transparency:</strong> Minimal insight into safety mechanisms
        </div>
      </div>

      <div class="model-card" onclick="selectConstitutionalModel('gemini', this)">
        <div class="model-name">üíé Gemini's Responsible AI</div>
        <div class="model-specs">
          <strong>Approach:</strong> Multi-layered safety + constitutional elements<br>
          <strong>Training:</strong> Responsible AI principles integrated<br>
          <strong>Safety Features:</strong> Advanced harm detection, cultural sensitivity<br>
          <strong>Self-Correction:</strong> Emerging constitutional capabilities<br>
          <strong>Transparency:</strong> Detailed safety documentation
        </div>
      </div>
    </div>

    <div id="constitutionalModelAnalysis"></div>
  </div>

  <div class="step">
    <h3>üé≠ Interactive Constitutional AI Demo</h3>
    
    <div class="interactive-demo">
      <div class="demo-title">üé¨ Constitutional AI Self-Correction Simulation</div>
      <p>See how Constitutional AI helps VLMs self-correct problematic responses in real-time.</p>

      <div class="controls">
        <div class="control-group">
          <label>Scenario Type:</label>
          <select id="scenarioType">
            <option value="hallucination" selected>Hallucination Correction</option>
            <option value="privacy">Privacy Protection</option>
            <option value="bias">Bias Mitigation</option>
            <option value="educational">Educational Enhancement</option>
          </select>
        </div>
        <div class="control-group">
          <label>Image Context:</label>
          <select id="imageContext">
            <option value="people" selected>People in photo</option>
            <option value="medical">Medical-related image</option>
            <option value="document">Document/text image</option>
            <option value="artwork">Artwork/creative content</option>
          </select>
        </div>
        <div class="control-group">
          <label>User Question:</label>
          <textarea id="userQuestion" rows="2" placeholder="Enter a user question about the image">Who is this person and what's their medical condition?</textarea>
        </div>
      </div>

      <button onclick="simulateConstitutionalCorrection()" class="primary">üé≠ Simulate Constitutional Self-Correction</button>
      <div id="constitutionalSimulation"></div>
    </div>
  </div>

  <div class="step">
    <h3>‚öôÔ∏è Implementation: Building Constitutional VLMs</h3>
    
    <div class="code-block">
      <div class="code-header">üõ°Ô∏è Constitutional VLM Implementation</div>
      <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>class ConstitutionalVLM(nn.Module):
    """Vision-Language Model with Constitutional AI training"""
    
    def __init__(self, base_vlm, constitution_config):
        super().__init__()
        self.base_vlm = base_vlm
        self.constitution = constitution_config
        
        # Constitutional components
        self.visual_safety_classifier = VisualSafetyClassifier()
        self.constitutional_judge = ConstitutionalJudge(constitution_config)
        self.self_critique_module = SelfCritiqueModule()
        
    def forward_with_constitution(self, image, prompt):
        """Generate response with constitutional self-correction"""
        
        # Step 1: Initial safety check
        safety_score = self.visual_safety_classifier(image)
        if safety_score < self.constitution.safety_threshold:
            return "I can't analyze this image due to safety guidelines."
        
        # Step 2: Generate initial response
        initial_response = self.base_vlm.generate(image, prompt)
        
        # Step 3: Constitutional self-critique
        critique = self.self_critique_module(
            image, prompt, initial_response, self.constitution
        )
        
        # Step 4: Decide if revision needed
        if critique.needs_revision:
            # Generate constitutionally-improved response
            revised_response = self.generate_revision(
                image, prompt, initial_response, critique
            )
            
            # Verify improvement
            improvement_score = self.constitutional_judge(
                image, prompt, revised_response
            )
            
            if improvement_score > critique.initial_score:
                return revised_response
        
        return initial_response
    
    def generate_revision(self, image, prompt, initial_response, critique):
        """Generate improved response based on constitutional critique"""
        
        revision_prompt = f"""
        Constitutional Principles: {self.constitution.principles}
        
        Original question: {prompt}
        My initial response: {initial_response}
        
        Constitutional critique: {critique.feedback}
        
        Please provide an improved response that:
        1. Addresses the critique points
        2. Better follows constitutional principles  
        3. Maintains helpfulness while improving safety/accuracy
        
        Improved response:
        """
        
        return self.base_vlm.generate(image, revision_prompt)

class VisualSafetyClassifier(nn.Module):
    """Classify visual content for safety"""
    
    def __init__(self):
        super().__init__()
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # Safety classification head
        self.safety_classifier = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 4)  # safe, concerning, unsafe, unknown
        )
    
    def forward(self, image):
        # Extract visual features
        with torch.no_grad():
            image_features = self.clip_model.get_image_features(image)
        
        # Classify safety level
        safety_logits = self.safety_classifier(image_features)
        safety_probs = torch.softmax(safety_logits, dim=-1)
        
        return safety_probs[:, 0]  # Return "safe" probability

class SelfCritiqueModule(nn.Module):
    """Generate constitutional critiques of VLM responses"""
    
    def __init__(self):
        super().__init__()
        
    def forward(self, image, prompt, response, constitution):
        """Generate comprehensive constitutional critique"""
        
        critique_prompts = {
            'visual_accuracy': f"""
            Constitutional Principle: "Describe only what you can see"
            
            Image: [shown]
            My response: {response}
            
            Visual accuracy critique:
            - What did I describe that isn't actually visible?
            - What important details did I miss?
            - Where did I make unsupported inferences?
            - Rate accuracy: 1-10
            """,
            
            'safety': f"""
            Constitutional Principles: {constitution.safety_principles}
            
            User question: {prompt}
            Image: [shown]
            My response: {response}
            
            Safety critique:
            - Could my response cause harm if followed?
            - Did I identify someone inappropriately?
            - Did I provide dangerous advice?
            - Rate safety: 1-10
            """,
            
            'helpfulness': f"""
            Constitutional Principle: "Be maximally helpful and educational"
            
            Question: {prompt}  
            My response: {response}
            
            Helpfulness critique:
            - Did I fully address the user's question?
            - Could I provide more educational value?
            - Is my response appropriately detailed?
            - Rate helpfulness: 1-10
            """
        }
        
        critiques = {}
        for aspect, prompt_text in critique_prompts.items():
            if aspect == 'helpfulness':
                # Text-only critique for helpfulness
                critiques[aspect] = self.generate_critique(None, prompt_text)
            else:
                # Include image for visual critiques
                critiques[aspect] = self.generate_critique(image, prompt_text)
        
        return self.synthesize_critiques(critiques)

# Constitutional training configuration
CLAUDE_VISUAL_CONSTITUTION = {
    'safety_principles': [
        "Protect individual privacy - don't identify people",
        "Avoid medical diagnoses or health advice from images", 
        "Be culturally sensitive and avoid stereotyping",
        "Don't assist with potentially illegal activities"
    ],
    'accuracy_principles': [
        "Describe only what is clearly visible",
        "Express uncertainty about unclear visual elements",
        "Distinguish observation from inference",
        "Acknowledge limitations of visual analysis"
    ],
    'helpfulness_principles': [
        "Provide educational context when appropriate",
        "Explain visual concepts clearly",
        "Offer practical insights about what you observe",
        "Help users understand complex visual information"
    ]
}

# Usage example
constitutional_vlm = ConstitutionalVLM(
    base_vlm=your_vlm_model,
    constitution_config=CLAUDE_VISUAL_CONSTITUTION
)

# Generate constitutionally-aligned response
response = constitutional_vlm.forward_with_constitution(image, user_prompt)
</pre>
    </div>
  </div>

  <div class="step">
    <h3>üìä Constitutional AI Benefits for VLMs</h3>
    
    <div class="metric-grid">
      <div class="metric-card">
        <div class="metric-value">‚Üì78%</div>
        <div class="metric-label">Harmful Outputs</div>
      </div>
      <div class="metric-card">
        <div class="metric-value">‚Üì65%</div>
        <div class="metric-label">Visual Hallucinations</div>
      </div>
      <div class="metric-card">
        <div class="metric-value">‚Üë45%</div>
        <div class="metric-label">Educational Value</div>
      </div>
      <div class="metric-card">
        <div class="metric-value">‚Üë89%</div>
        <div class="metric-label">User Trust</div>
      </div>
    </div>

    <div class="success">
      <strong>‚úÖ Constitutional AI Advantages for VLMs:</strong><br><br>
      
      <strong>üéØ Improved Safety:</strong><br>
      ‚Ä¢ Proactive identification of problematic visual content<br>
      ‚Ä¢ Self-correction of inappropriate responses<br>
      ‚Ä¢ Privacy protection through constitutional principles<br><br>
      
      <strong>üìà Better Performance:</strong><br>
      ‚Ä¢ Reduced hallucinations through self-critique<br>
      ‚Ä¢ More accurate visual descriptions<br>
      ‚Ä¢ Higher educational value in responses<br><br>
      
      <strong>üîß Practical Benefits:</strong><br>
      ‚Ä¢ Fewer human reviewers needed for safety<br>
      ‚Ä¢ More consistent behavior across diverse visual content<br>
      ‚Ä¢ Easier to audit and understand model decisions<br>
      ‚Ä¢ Adaptable principles for different use cases
    </div>
  </div>

  <div class="step">
    <h3>üîÆ Future Directions: Advanced Constitutional VLMs</h3>
    
    <div class="warning">
      <strong>üöÄ Emerging Developments:</strong><br><br>
      
      <strong>üß† Multi-Step Constitutional Reasoning:</strong><br>
      ‚Ä¢ VLMs that reason through multiple constitutional principles<br>
      ‚Ä¢ Chain-of-thought constitutional evaluation<br>
      ‚Ä¢ Dynamic principle weighting based on context<br><br>
      
      <strong>üîÑ Adaptive Constitutions:</strong><br>
      ‚Ä¢ Principles that evolve based on cultural context<br>
      ‚Ä¢ Domain-specific constitutional training<br>
      ‚Ä¢ User-customizable ethical guidelines<br><br>
      
      <strong>üåê Multimodal Constitutional Training:</strong><br>
      ‚Ä¢ Joint constitutional training across vision, language, and action<br>
      ‚Ä¢ Constitutional principles for robotics and embodied AI<br>
      ‚Ä¢ Cross-modal ethical reasoning capabilities
    </div>

    <div class="breakthrough-highlight">
      üåü The Future: Constitutional AI will enable VLMs to be reliable partners in high-stakes applications like medical diagnosis, legal analysis, and educational content creation!
    </div>
  </div>
</div>
  <div class="container">
    <h2>üìä Performance Benchmarks & Capabilities</h2>
    <div class="step">
      <h3>üèÜ How Do Modern VLMs Stack Up?</h3>
      
      <div class="interactive-demo">
        <div class="demo-title">üìà VLM Benchmark Comparison</div>

        <div class="controls">
          <div class="control-group">
            <label>Benchmark Category:</label>
            <select id="benchmarkCategory">
              <option value="general" selected>General Vision-Language</option>
              <option value="reasoning">Visual Reasoning</option>
              <option value="ocr">OCR & Document Understanding</option>
              <option value="math">Mathematical Reasoning</option>
              <option value="creative">Creative Tasks</option>
            </select>
          </div>
          <div class="control-group">
            <label>Evaluation Metric:</label>
            <select id="evaluationMetric">
              <option value="accuracy" selected>Accuracy (%)</option>
              <option value="speed">Speed (tokens/sec)</option>
              <option value="cost">Cost ($/1K images)</option>
            </select>
          </div>
        </div>

        <button onclick="updateBenchmarks()">üìä Update Benchmark Comparison</button>
        <div id="benchmarkResults"></div>
      </div>
    </div>

    <div class="step">
      <h3>üí™ Capability Deep Dive</h3>
      <div class="capability-matrix">
        <div class="capability-header">Capability</div>
        <div class="capability-header">GPT-4V</div>
        <div class="capability-header">Gemini Pro</div>
        <div class="capability-header">Claude 3</div>
        <div class="capability-header">Notes</div>
        
        <div class="capability-cell"><strong>Document Analysis</strong></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-good"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span></div>
        <div class="capability-cell">All excel at text extraction and layout understanding</div>
        
        <div class="capability-cell"><strong>Mathematical Reasoning</strong></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-average"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-good"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-good"></span></div>
        <div class="capability-cell">Gemini shows strong math problem solving from images</div>
        
        <div class="capability-cell"><strong>Creative Analysis</strong></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-good"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-average"></span><span class="performance-indicator perf-average"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span></div>
        <div class="capability-cell">GPT-4V and Claude excel at creative interpretation</div>
        
        <div class="capability-cell"><strong>Code Generation</strong></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-average"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-good"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-average"></span><span class="performance-indicator perf-average"></span></div>
        <div class="capability-cell">All can generate code from UI mockups and diagrams</div>
        
        <div class="capability-cell"><strong>Safety & Helpfulness</strong></div>
        <div class="capability-cell"><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-average"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-good"></span><span class="performance-indicator perf-average"></span><span class="performance-indicator perf-average"></span></div>
        <div class="capability-cell"><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span><span class="performance-indicator perf-excellent"></span></div>
        <div class="capability-cell">Claude's Constitutional AI training shows in safety</div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üíª Implementation & API Usage</h2>
    <div class="step">
      <h3>üöÄ Practical VLM Integration</h3>

      <div class="tabs">
        <div class="tab active" onclick="switchTab('openai', this)">OpenAI GPT-4V</div>
        <div class="tab" onclick="switchTab('google', this)">Google Gemini</div>
        <div class="tab" onclick="switchTab('anthropic', this)">Anthropic Claude</div>
        <div class="tab" onclick="switchTab('comparison', this)">API Comparison</div>
      </div>

      <div id="openai" class="tab-content active">
        <div class="code-block">
          <div class="code-header">ü§ñ GPT-4V API Usage (OpenAI)</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import openai
import base64
import requests
from PIL import Image
import io

client = openai.OpenAI(api_key="your-api-key")

def encode_image(image_path):
    """Encode image to base64 for GPT-4V API"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def analyze_image_gpt4v(image_path, prompt="Describe this image in detail"):
    """
    Analyze image using GPT-4V
    """
    base64_image = encode_image(image_path)
    
    response = client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                            "detail": "high"  # "low" | "high" | "auto"
                        }
                    }
                ]
            }
        ],
        max_tokens=1000,
        temperature=0.0  # Deterministic for analysis tasks
    )
    
    return response.choices[0].message.content

# Advanced usage: Multiple images with conversation
def multi_image_conversation(image_paths, conversation_history):
    """
    Handle multi-image conversation with GPT-4V
    """
    messages = []
    
    # Add conversation history
    for msg in conversation_history:
        messages.append(msg)
    
    # Add multiple images to current message
    content = [{"type": "text", "text": "Compare these images:"}]
    
    for image_path in image_paths:
        base64_image = encode_image(image_path)
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_image}",
                "detail": "high"
            }
        })
    
    messages.append({"role": "user", "content": content})
    
    response = client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=messages,
        max_tokens=1500
    )
    
    return response.choices[0].message.content

# Specialized analysis functions
def extract_text_from_image(image_path):
    """OCR and document understanding"""
    return analyze_image_gpt4v(
        image_path, 
        "Extract all text from this image and format it properly. "
        "Maintain the original structure and layout."
    )

def analyze_chart_or_graph(image_path):
    """Data visualization analysis"""
    return analyze_image_gpt4v(
        image_path,
        "Analyze this chart or graph. Describe the data trends, "
        "key insights, and any notable patterns. Provide specific "
        "numbers where visible."
    )

def generate_code_from_ui(image_path, framework="React"):
    """UI mockup to code generation"""
    return analyze_image_gpt4v(
        image_path,
        f"Generate {framework} code for this UI mockup. Include "
        f"proper styling and component structure. Make it responsive "
        f"and production-ready."
    )

# Example usage
if __name__ == "__main__":
    # Basic image analysis
    description = analyze_image_gpt4v("photo.jpg")
    print("Image description:", description)
    
    # OCR example
    extracted_text = extract_text_from_image("document.jpg")
    print("Extracted text:", extracted_text)
    
    # Multi-image comparison
    comparison = multi_image_conversation(
        ["before.jpg", "after.jpg"],
        [{"role": "system", "content": "You are an expert analyst."}]
    )
    print("Comparison:", comparison)
</pre>
        </div>
      </div>

      <div id="google" class="tab-content">
        <div class="code-block">
          <div class="code-header">üíé Gemini Vision API Usage (Google)</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import google.generativeai as genai
from PIL import Image
import io

# Configure API
genai.configure(api_key="your-google-api-key")

def analyze_image_gemini(image_path, prompt="Describe this image"):
    """
    Analyze image using Gemini Pro Vision
    """
    # Load and prepare image
    image = Image.open(image_path)
    
    # Initialize model
    model = genai.GenerativeModel('gemini-pro-vision')
    
    # Generate response
    response = model.generate_content([prompt, image])
    return response.text

def analyze_video_gemini(video_path, prompt="Describe this video"):
    """
    Analyze video using Gemini (unique capability)
    """
    # Upload video file
    video_file = genai.upload_file(path=video_path)
    
    # Wait for processing
    while video_file.state.name == "PROCESSING":
        time.sleep(10)
        video_file = genai.get_file(video_file.name)
    
    model = genai.GenerativeModel('gemini-1.5-pro')
    response = model.generate_content([prompt, video_file])
    
    return response.text

def mathematical_reasoning_from_image(image_path):
    """
    Solve math problems from images (Gemini's strength)
    """
    prompt = """
    Look at this mathematical problem in the image. 
    Solve it step by step, showing your work clearly.
    If there are graphs or diagrams, interpret them as part of the solution.
    """
    
    return analyze_image_gemini(image_path, prompt)

def long_document_analysis(image_paths):
    """
    Analyze multi-page documents (up to 2M tokens context)
    """
    model = genai.GenerativeModel('gemini-1.5-pro')
    
    # Prepare images
    images = [Image.open(path) for path in image_paths]
    
    prompt = """
    Analyze this multi-page document. Provide:
    1. Summary of main topics
    2. Key data points and statistics  
    3. Important conclusions or recommendations
    4. Any action items or next steps mentioned
    """
    
    content = [prompt] + images
    response = model.generate_content(content)
    
    return response.text

def code_generation_from_mockup(image_path, framework="HTML/CSS"):
    """
    Generate code from UI mockups
    """
    prompt = f"""
    Generate clean, production-ready {framework} code for this UI mockup.
    
    Requirements:
    - Responsive design
    - Modern styling
    - Accessible markup
    - Clean, commented code
    - Include hover states and interactions where appropriate
    """
    
    return analyze_image_gemini(image_path, prompt)

# Advanced: Streaming responses for long analysis
def stream_analysis(image_path, prompt):
    """
    Stream response for real-time feedback
    """
    image = Image.open(image_path)
    model = genai.GenerativeModel('gemini-pro-vision')
    
    response = model.generate_content([prompt, image], stream=True)
    
    full_response = ""
    for chunk in response:
        if chunk.text:
            print(chunk.text, end='')
            full_response += chunk.text
    
    return full_response

# Safety settings for content filtering
def safe_image_analysis(image_path, prompt):
    """
    Analysis with safety controls
    """
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
    ]
    
    image = Image.open(image_path)
    model = genai.GenerativeModel('gemini-pro-vision')
    
    response = model.generate_content(
        [prompt, image],
        safety_settings=safety_settings
    )
    
    return response.text

# Example usage
if __name__ == "__main__":
    # Basic analysis
    result = analyze_image_gemini("chart.jpg", "Explain this business chart")
    print("Chart analysis:", result)
    
    # Math problem solving
    solution = mathematical_reasoning_from_image("math_problem.jpg")
    print("Solution:", solution)
    
    # Video analysis (unique to Gemini)
    video_summary = analyze_video_gemini("presentation.mp4", 
                                         "Summarize the key points from this presentation")
    print("Video summary:", video_summary)
</pre>
        </div>
      </div>

      <div id="anthropic" class="tab-content">
        <div class="code-block">
          <div class="code-header">ü§ñ Claude Vision API Usage (Anthropic)</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import anthropic
import base64
from typing import List, Dict

client = anthropic.Anthropic(api_key="your-anthropic-api-key")

def encode_image_claude(image_path):
    """Encode image for Claude API"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def analyze_image_claude(image_path, prompt="Please describe this image"):
    """
    Analyze image using Claude 3 (Opus/Sonnet/Haiku)
    """
    base64_image = encode_image_claude(image_path)
    
    # Determine image type
    image_type = "image/jpeg"
    if image_path.lower().endswith('.png'):
        image_type = "image/png"
    elif image_path.lower().endswith('.gif'):
        image_type = "image/gif"
    elif image_path.lower().endswith('.webp'):
        image_type = "image/webp"
    
    response = client.messages.create(
        model="claude-3-opus-20240229",  # or "claude-3-sonnet-20240229"
        max_tokens=1500,
        temperature=0,
        messages=[
            {
                "role": "user", 
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": image_type,
                            "data": base64_image
                        }
                    },
                    {
                        "type": "text",
                        "text": prompt
                    }
                ]
            }
        ]
    )
    
    return response.content[0].text

def educational_image_explanation(image_path, subject="general"):
    """
    Generate educational explanations (Claude's strength)
    """
    prompt = f"""
    Please provide a detailed educational explanation of this image related to {subject}.
    
    Structure your response as:
    1. What you observe
    2. Key concepts or principles demonstrated
    3. Educational significance or applications
    4. Questions this might help students explore
    
    Make it suitable for learning and teaching.
    """
    
    return analyze_image_claude(image_path, prompt)

def structured_data_extraction(image_path, format_type="JSON"):
    """
    Extract structured data from images (forms, tables, etc.)
    """
    prompt = f"""
    Extract all structured information from this image and format it as {format_type}.
    
    For tables: preserve row/column structure
    For forms: capture field names and values
    For documents: maintain hierarchical organization
    
    Be precise and comprehensive.
    """
    
    return analyze_image_claude(image_path, prompt)

def creative_visual_analysis(image_path):
    """
    Creative and artistic analysis (Claude's strength)
    """
    prompt = """
    Provide a thoughtful creative analysis of this image. Consider:
    
    - Artistic elements (composition, color, lighting, mood)
    - Emotional impact and atmosphere
    - Symbolism or deeper meaning
    - Cultural or historical context if relevant
    - Technical aspects of the photography/artwork
    
    Write in an engaging, insightful style that would be valuable 
    for art students or enthusiasts.
    """
    
    return analyze_image_claude(image_path, prompt)

def multi_step_reasoning(image_path, reasoning_type="logical"):
    """
    Complex reasoning tasks from visual input
    """
    prompt = f"""
    Analyze this image using step-by-step {reasoning_type} reasoning.
    
    Please:
    1. Observe and describe what you see
    2. Identify relevant information for analysis
    3. Apply logical reasoning step by step
    4. Draw conclusions based on your analysis
    5. Explain your reasoning process
    
    Be thorough and show your work clearly.
    """
    
    return analyze_image_claude(image_path, prompt)

def safety_focused_analysis(image_path):
    """
    Analyze images with safety considerations (Claude's constitutional training)
    """
    prompt = """
    Please analyze this image with attention to:
    
    1. Content appropriateness and safety considerations
    2. Factual accuracy of any information shown
    3. Potential misinterpretations or biases
    4. Ethical implications if relevant
    
    Provide a balanced, thoughtful analysis that considers multiple perspectives.
    """
    
    return analyze_image_claude(image_path, prompt)

def conversation_with_images(image_paths: List[str], conversation_prompt: str):
    """
    Multi-image conversation with Claude
    """
    content = []
    
    # Add all images
    for image_path in image_paths:
        base64_image = encode_image_claude(image_path)
        image_type = "image/jpeg"
        if image_path.lower().endswith('.png'):
            image_type = "image/png"
        
        content.append({
            "type": "image",
            "source": {
                "type": "base64", 
                "media_type": image_type,
                "data": base64_image
            }
        })
    
    # Add conversation prompt
    content.append({
        "type": "text",
        "text": conversation_prompt
    })
    
    response = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=2000,
        messages=[{"role": "user", "content": content}]
    )
    
    return response.content[0].text

def document_qa_system(image_path, questions: List[str]):
    """
    Question-answering system for document images
    """
    base64_image = encode_image_claude(image_path)
    
    questions_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
    
    prompt = f"""
    Please analyze this document image and answer the following questions:
    
    {questions_text}
    
    For each question, provide:
    - A direct answer based on the document
    - The specific location/section where you found the information
    - Your confidence level in the answer
    
    If information isn't available in the document, please state that clearly.
    """
    
    response = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=2000,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": "image/jpeg",
                            "data": base64_image
                        }
                    },
                    {
                        "type": "text", 
                        "text": prompt
                    }
                ]
            }
        ]
    )
    
    return response.content[0].text

# Example usage
if __name__ == "__main__":
    # Educational explanation
    explanation = educational_image_explanation("diagram.jpg", "biology")
    print("Educational explanation:", explanation)
    
    # Creative analysis
    creative_analysis = creative_visual_analysis("artwork.jpg")
    print("Creative analysis:", creative_analysis)
    
    # Multi-image conversation
    comparison = conversation_with_images(
        ["chart1.jpg", "chart2.jpg"],
        "Compare these two charts and highlight the key differences in trends."
    )
    print("Chart comparison:", comparison)
    
    # Document Q&A
    answers = document_qa_system("invoice.jpg", [
        "What is the total amount?",
        "When is the due date?", 
        "What company issued this invoice?"
    ])
    print("Document Q&A:", answers)
</pre>
        </div>
      </div>

      <div id="comparison" class="tab-content">
        <div class="benchmark-table">
          <table>
            <tr>
              <th>Feature</th>
              <th>GPT-4V</th>
              <th>Gemini Pro</th>
              <th>Claude 3 Opus</th>
            </tr>
            <tr>
              <td><strong>Max Image Size</strong></td>
              <td>20MB, 4096√ó4096</td>
              <td>4MB, multiple formats</td>
              <td>5MB, up to 8000√ó8000</td>
            </tr>
            <tr>
              <td><strong>Supported Formats</strong></td>
              <td>PNG, JPEG, WEBP, GIF</td>
              <td>PNG, JPEG, WEBP, HEIC</td>
              <td>PNG, JPEG, GIF, WEBP</td>
            </tr>
            <tr>
              <td><strong>Batch Processing</strong></td>
              <td>Multiple images per request</td>
              <td>Multiple images + video</td>
              <td>Multiple images per request</td>
            </tr>
            <tr>
              <td><strong>Context Length</strong></td>
              <td>128K tokens</td>
              <td>2M tokens (Gemini 1.5)</td>
              <td>200K tokens</td>
            </tr>
            <tr>
              <td><strong>Pricing (per image)</strong></td>
              <td>$0.01 (detail=high)</td>
              <td>$0.0025</td>
              <td>$0.0048</td>
            </tr>
            <tr>
              <td><strong>Special Features</strong></td>
              <td>DALL-E integration</td>
              <td>Video understanding</td>
              <td>Constitutional AI safety</td>
            </tr>
          </table>
        </div>

        <div class="success">
          <strong>üéØ API Selection Guide:</strong><br><br>
          
          <strong>Choose GPT-4V for:</strong><br>
          ‚Ä¢ Creative and artistic analysis<br>
          ‚Ä¢ Code generation from mockups<br>
          ‚Ä¢ Integration with DALL-E workflows<br>
          ‚Ä¢ Mature ecosystem and tooling<br><br>
          
          <strong>Choose Gemini for:</strong><br>
          ‚Ä¢ Video analysis (unique capability)<br>
          ‚Ä¢ Mathematical problem solving<br>
          ‚Ä¢ Long document processing (2M tokens)<br>
          ‚Ä¢ Cost-effective high-volume processing<br><br>
          
          <strong>Choose Claude for:</strong><br>
          ‚Ä¢ Educational content and explanations<br>
          ‚Ä¢ Safety-critical applications<br>
          ‚Ä¢ Detailed reasoning and analysis<br>
          ‚Ä¢ Constitutional AI alignment benefits
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üöÄ Advanced Applications & Use Cases</h2>
    <div class="step">
      <h3>üè≠ Production VLM Applications</h3>
      
      <div class="interactive-demo">
        <div class="demo-title">üíº VLM Application Showcase</div>

        <div class="controls">
          <div class="control-group">
            <label>Industry Vertical:</label>
            <select id="industryVertical">
              <option value="healthcare" selected>Healthcare</option>
              <option value="education">Education</option>
              <option value="finance">Finance & Banking</option>
              <option value="retail">Retail & E-commerce</option>
              <option value="manufacturing">Manufacturing</option>
              <option value="media">Media & Entertainment</option>
            </select>
          </div>
          <div class="control-group">
            <label>Use Case Type:</label>
            <select id="useCaseType">
              <option value="analysis" selected>Document/Image Analysis</option>
              <option value="automation">Process Automation</option>
              <option value="qa">Quality Assurance</option>
              <option value="creative">Creative Enhancement</option>
            </select>
          </div>
        </div>

        <button onclick="showcaseApplications()">üéØ Show Applications</button>
        <div id="applicationShowcase"></div>
      </div>
    </div>

    <div class="step">
      <h3>üéØ Real-World Success Stories</h3>
      
      <div class="timeline">
        <div class="timeline-item">
          <div class="timeline-date">2023 Q4</div>
          <div class="timeline-content">
            <h4>üè• Medical Imaging Revolution</h4>
            <p>GPT-4V deployed for radiology report generation, reducing reporting time by 60% while maintaining accuracy. Integration with PACS systems enables real-time image analysis.</p>
          </div>
        </div>
        
        <div class="timeline-item">
          <div class="timeline-date">2024 Q1</div>
          <div class="timeline-content">
            <h4>üìö Educational Content Creation</h4>
            <p>Claude 3 powers automated textbook digitization, converting scanned pages to interactive digital content with 98% accuracy in mathematical notation recognition.</p>
          </div>
        </div>
        
        <div class="timeline-item">
          <div class="timeline-date">2024 Q2</div>
          <div class="timeline-content">
            <h4>üè¶ Financial Document Processing</h4>
            <p>Gemini Pro Vision automates loan application processing, extracting and validating information from complex financial documents, reducing processing time from days to minutes.</p>
          </div>
        </div>
        
        <div class="timeline-item">
          <div class="timeline-date">2024 Q3</div>
          <div class="timeline-content">
            <h4>üõí Retail Visual Search</h4>
            <p>GPT-4V enables "shop the look" functionality, allowing customers to upload photos and find similar products with 95% relevance accuracy across 10M+ product catalogs.</p>
          </div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>üîÆ Future Directions & Emerging Capabilities</h3>
      <div class="breakthrough-highlight">
        üåü Next-Generation VLMs: Multi-step reasoning, video understanding, 3D scene comprehension, and real-time visual interaction!
      </div>

      <div class="model-comparison">
        <div class="model-card">
          <div class="model-name">üé• Video-Native VLMs</div>
          <div class="model-specs">
            ‚Ä¢ Temporal reasoning across video frames<br>
            ‚Ä¢ Action recognition and prediction<br>
            ‚Ä¢ Long-form video summarization<br>
            ‚Ä¢ Real-time video Q&A
          </div>
        </div>

        <div class="model-card">
          <div class="model-name">üèóÔ∏è 3D Scene Understanding</div>
          <div class="model-specs">
            ‚Ä¢ Depth estimation and 3D reconstruction<br>
            ‚Ä¢ Spatial reasoning in 3D environments<br>
            ‚Ä¢ Augmented reality integration<br>
            ‚Ä¢ Robotics and navigation applications
          </div>
        </div>

        <div class="model-card">
          <div class="model-name">‚ö° Real-Time Interaction</div>
          <div class="model-specs">
            ‚Ä¢ Live camera feed processing<br>
            ‚Ä¢ Interactive visual assistance<br>
            ‚Ä¢ Real-time object manipulation<br>
            ‚Ä¢ Streaming video analysis
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üî¨ Training & Fine-tuning VLMs</h2>
    <div class="step">
      <h3>üéì Instruction Tuning for Vision-Language Tasks</h3>
      <p>Training modern VLMs requires sophisticated instruction tuning that teaches models to follow visual instructions and provide helpful, accurate responses about images.</p>

      <div class="math-formula">
        <strong>VLM Training Pipeline:</strong><br><br>
        
        <strong>1. Multimodal Pre-training:</strong><br>
        ‚Ñí<sub>pretrain</sub> = ‚Ñí<sub>LM</sub>(text) + Œª<sub>vision</sub>‚Ñí<sub>vision</sub>(image, text)<br>
        <em>Joint training on text-only and vision-language data</em><br><br>
        
        <strong>2. Instruction Tuning:</strong><br>
        ‚Ñí<sub>instruction</sub> = -‚àë log P(response | instruction, image)<br>
        <em>Learn to follow visual instructions and provide helpful responses</em><br><br>
        
        <strong>3. RLHF (Reinforcement Learning from Human Feedback):</strong><br>
        ‚Ñí<sub>RLHF</sub> = ùîº<sub>œÄ</sub>[r(image, instruction, response)] - Œ≤ KL(œÄ, œÄ<sub>ref</sub>)<br>
        <em>Optimize for human preferences in visual tasks</em>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üéØ VLM Training Simulator</div>

        <div class="controls">
          <div class="control-group">
            <label>Training Stage:</label>
            <select id="trainingStage">
              <option value="pretrain" selected>Pre-training</option>
              <option value="instruction">Instruction Tuning</option>
              <option value="rlhf">RLHF</option>
              <option value="fine-tune">Domain Fine-tuning</option>
            </select>
          </div>
          <div class="control-group">
            <label>Dataset Type:</label>
            <select id="datasetType">
              <option value="general" selected>General Vision-Language</option>
              <option value="medical">Medical Images</option>
              <option value="documents">Document Understanding</option>
              <option value="creative">Creative/Artistic</option>
            </select>
          </div>
          <div class="control-group">
            <label>Training Scale:</label>
            <select id="trainingScale">
              <option value="small">Small (1M examples)</option>
              <option value="medium" selected>Medium (10M examples)</option>
              <option value="large">Large (100M+ examples)</option>
            </select>
          </div>
        </div>

        <button onclick="simulateVLMTraining()">üéì Simulate Training</button>
        <div id="trainingSimulation"></div>
      </div>
    </div>

    <div class="step">
      <h3>üíª Custom VLM Fine-tuning Code</h3>
      <div class="code-block">
        <div class="code-header">üéì Custom VLM Fine-tuning (PyTorch + Transformers)</div>
        <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import torch
import torch.nn as nn
from transformers import (
    LlamaForCausalLM, 
    LlamaTokenizer,
    CLIPVisionModel,
    CLIPImageProcessor,
    Trainer,
    TrainingArguments
)
from torch.utils.data import Dataset
from PIL import Image
import json

class VisionLanguageModel(nn.Module):
    """
    Custom Vision-Language Model combining CLIP vision encoder with LLaMA
    """
    def __init__(self, llm_model_name="meta-llama/Llama-2-7b-hf"):
        super().__init__()
        
        # Vision encoder (CLIP ViT)
        self.vision_encoder = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
        self.vision_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-base-patch32")
        
        # Language model
        self.language_model = LlamaForCausalLM.from_pretrained(llm_model_name)
        self.tokenizer = LlamaTokenizer.from_pretrained(llm_model_name)
        
        # Vision-language projection
        vision_dim = self.vision_encoder.config.hidden_size  # 768 for CLIP ViT-Base
        llm_dim = self.language_model.config.hidden_size     # 4096 for LLaMA-7B
        
        self.vision_projection = nn.Sequential(
            nn.Linear(vision_dim, llm_dim),
            nn.GELU(),
            nn.Linear(llm_dim, llm_dim)
        )
        
        # Special tokens
        self.tokenizer.add_special_tokens({
            'pad_token': '[PAD]',
            'additional_special_tokens': ['<image>', '</image>']
        })
        self.language_model.resize_token_embeddings(len(self.tokenizer))
        
        # Freeze vision encoder initially
        for param in self.vision_encoder.parameters():
            param.requires_grad = False
    
    def encode_image(self, images):
        """Encode images to visual tokens"""
        with torch.no_grad():
            vision_outputs = self.vision_encoder(images)
            image_embeddings = vision_outputs.last_hidden_state
        
        # Project to LLM dimension
        projected_embeddings = self.vision_projection(image_embeddings)
        return projected_embeddings
    
    def forward(self, input_ids, attention_mask, images=None, labels=None):
        """Forward pass with optional images"""
        batch_size, seq_len = input_ids.shape
        
        # Get text embeddings
        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)
        
        if images is not None:
            # Encode images
            image_embeds = self.encode_image(images)  # [batch, num_patches, llm_dim]
            
            # Find <image> token positions
            image_token_id = self.tokenizer.convert_tokens_to_ids('<image>')
            image_positions = (input_ids == image_token_id)
            
            # Replace <image> tokens with actual image embeddings
            for batch_idx in range(batch_size):
                img_positions = torch.where(image_positions[batch_idx])[0]
                if len(img_positions) > 0:
                    # Replace first image token position with image patches
                    pos = img_positions[0]
                    # Insert image patches at the position
                    inputs_embeds[batch_idx, pos:pos+1] = image_embeds[batch_idx, :1]
        
        # Forward through language model
        outputs = self.language_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            labels=labels
        )
        
        return outputs

class VisionLanguageDataset(Dataset):
    """Dataset for vision-language instruction tuning"""
    
    def __init__(self, data_path, tokenizer, vision_processor, max_length=2048):
        with open(data_path, 'r') as f:
            self.data = json.load(f)
        
        self.tokenizer = tokenizer
        self.vision_processor = vision_processor
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # Load and process image
        image = Image.open(item['image_path']).convert('RGB')
        image_tensor = self.vision_processor(image, return_tensors='pt')['pixel_values'][0]
        
        # Create instruction-response format
        instruction = item['instruction']
        response = item['response']
        
        # Format: <image> {instruction} {response}
        text = f"<image> {instruction} {response}"
        
        # Tokenize
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        # Create labels (mask instruction part, only train on response)
        instruction_text = f"<image> {instruction} "
        instruction_length = len(self.tokenizer.encode(instruction_text, add_special_tokens=False))
        
        labels = encoding['input_ids'].clone()
        labels[0, :instruction_length] = -100  # Ignore instruction tokens in loss
        
        return {
            'input_ids': encoding['input_ids'][0],
            'attention_mask': encoding['attention_mask'][0],
            'labels': labels[0],
            'images': image_tensor
        }

def train_vlm(model, train_dataset, eval_dataset=None):
    """Train the Vision-Language Model"""
    
    training_args = TrainingArguments(
        output_dir="./vlm-finetuned",
        per_device_train_batch_size=4,  # Adjust based on GPU memory
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=8,  # Effective batch size: 4*8=32
        num_train_epochs=3,
        learning_rate=2e-5,
        warmup_steps=500,
        weight_decay=0.01,
        logging_steps=50,
        save_steps=1000,
        eval_steps=1000,
        evaluation_strategy="steps" if eval_dataset else "no",
        save_total_limit=3,
        load_best_model_at_end=True if eval_dataset else False,
        metric_for_best_model="eval_loss" if eval_dataset else None,
        fp16=True,  # Mixed precision training
        dataloader_num_workers=4,
        remove_unused_columns=False,
        report_to="wandb",  # Optional: for experiment tracking
    )
    
    # Custom data collator
    def data_collator(features):
        batch = {}
        batch['input_ids'] = torch.stack([f['input_ids'] for f in features])
        batch['attention_mask'] = torch.stack([f['attention_mask'] for f in features])
        batch['labels'] = torch.stack([f['labels'] for f in features])
        batch['images'] = torch.stack([f['images'] for f in features])
        return batch
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    # Train the model
    trainer.train()
    
    # Save the final model
    trainer.save_model()
    
    return trainer

def inference_example(model, tokenizer, vision_processor, image_path, instruction):
    """Run inference with the trained model"""
    model.eval()
    
    # Load and process image
    image = Image.open(image_path).convert('RGB')
    image_tensor = vision_processor(image, return_tensors='pt')['pixel_values']
    
    # Prepare input
    text = f"<image> {instruction} "
    input_ids = tokenizer.encode(text, return_tensors='pt')
    
    with torch.no_grad():
        # Generate response
        outputs = model.language_model.generate(
            input_ids=input_ids,
            images=image_tensor,
            max_length=512,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
        
        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(text.strip(), "").strip()
    
    return response

# Example usage
if __name__ == "__main__":
    # Initialize model
    model = VisionLanguageModel("meta-llama/Llama-2-7b-hf")
    
    # Load datasets
    train_dataset = VisionLanguageDataset(
        "train_data.json", 
        model.tokenizer, 
        model.vision_processor
    )
    
    eval_dataset = VisionLanguageDataset(
        "eval_data.json",
        model.tokenizer,
        model.vision_processor
    )
    
    # Train the model
    trainer = train_vlm(model, train_dataset, eval_dataset)
    
    # Example inference
    response = inference_example(
        model, 
        model.tokenizer, 
        model.vision_processor,
        "test_image.jpg", 
        "What do you see in this image?"
    )
    
    print(f"Model response: {response}")
</pre>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üéØ Key Takeaways & Future Outlook</h2>
    <div class="step">
      <h3>üèÜ What We've Learned</h3>
      <div class="success">
        <strong>‚úÖ Core VLM Insights Mastered:</strong><br><br>
        
        <strong>üèóÔ∏è Architectural Evolution:</strong><br>
        ‚Ä¢ From CLIP's separate encoders to integrated token streams<br>
        ‚Ä¢ Visual tokens flow through language model transformers<br>
        ‚Ä¢ Cross-modal attention enables sophisticated reasoning<br><br>
        
        <strong>üßÆ Mathematical Foundations:</strong><br>
        ‚Ä¢ Visual patch tokenization and positional encoding<br>
        ‚Ä¢ Fusion strategies: early vs late vs cross-attention<br>
        ‚Ä¢ Instruction tuning and RLHF for visual tasks<br><br>
        
        <strong>üíª Production Implementation:</strong><br>
        ‚Ä¢ API usage patterns for GPT-4V, Gemini, and Claude<br>
        ‚Ä¢ Performance benchmarks and capability comparisons<br>
        ‚Ä¢ Custom training and fine-tuning strategies<br><br>
        
        <strong>üöÄ Real-World Applications:</strong><br>
        ‚Ä¢ Document analysis and OCR at scale<br>
        ‚Ä¢ Medical imaging and healthcare automation<br>
        ‚Ä¢ Educational content creation and creative analysis
      </div>
    </div>

    <div class="step">
      <h3>üîÆ The Future of Vision-Language AI</h3>
      <div class="breakthrough-highlight">
        üåü Next Frontier: Embodied AI agents that can see, reason, and act in the physical world using vision-language understanding!
      </div>

      <div class="warning">
        <strong>üöß Current Limitations & Active Research:</strong><br>
        ‚Ä¢ <strong>Spatial reasoning:</strong> Still struggles with complex 3D relationships<br>
        ‚Ä¢ <strong>Fine-grained details:</strong> May miss subtle visual elements<br>
        ‚Ä¢ <strong>Hallucination:</strong> Can generate plausible but incorrect descriptions<br>
        ‚Ä¢ <strong>Computational cost:</strong> High inference costs for complex visual tasks<br>
        ‚Ä¢ <strong>Context limitations:</strong> Long visual sequences remain challenging
      </div>

      <div class="info">
        <strong>üî¨ Emerging Research Directions:</strong><br>
        ‚Ä¢ <strong>Video-native architectures:</strong> Models trained end-to-end on video data<br>
        ‚Ä¢ <strong>3D scene understanding:</strong> Depth estimation and spatial reasoning<br>
        ‚Ä¢ <strong>Multimodal agents:</strong> Systems that can perceive, reason, and act<br>
        ‚Ä¢ <strong>Efficient architectures:</strong> Reducing computational costs while maintaining quality<br>
        ‚Ä¢ <strong>Grounding in robotics:</strong> Connecting vision-language to physical actions
      </div>
    </div>

    <div class="step">
      <h3>üìö Recommended Next Steps</h3>
      <div class="metric-grid">
        <div class="metric-card">
          <div class="metric-value">ü§ñ</div>
          <div class="metric-label">Explore VLA Models</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">üß†</div>
          <div class="metric-label">Study V-JEPA</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">üé®</div>
          <div class="metric-label">Generative Vision</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">‚ö°</div>
          <div class="metric-label">Optimization Techniques</div>
        </div>
      </div>

      <div class="success">
        <strong>üéì Your Vision-Language Journey:</strong><br>
        You now understand how CLIP's breakthrough evolved into the conversational AI systems millions use daily. From joint embeddings to unified token streams, from contrastive learning to instruction tuning - you've mastered the mathematical foundations and practical implementation of modern multimodal AI!
      </div>
    </div>
  </div>

  <script>
    let selectedModel = 'gpt4v';
    let selectedFusion = 'early';

    // Initialize on page load
    document.addEventListener('DOMContentLoaded', () => {
      selectModel('gpt4v', document.querySelector('.model-card'));
      selectFusion('early', document.querySelector('.fusion-card'));
      generateTokenSequence();
      visualizeAttention();
      updateBenchmarks();
      showcaseApplications();
      simulateVLMTraining();
    });

    function selectModel(modelId, element) {
      selectedModel = modelId;
      
      // Update visual selection
      document.querySelectorAll('.model-card').forEach(card => card.classList.remove('selected'));
      element.classList.add('selected');
      
      // Show detailed architecture information
      const details = {
        'gpt4v': {
          architecture: 'GPT-4 Transformer + Custom Vision Encoder',
          innovation: 'Seamless integration of vision into conversational AI',
          strengths: ['Creative visual analysis', 'Code generation', 'Artistic interpretation'],
          technical: 'Uses custom ViT variant with learned visual tokens that integrate into GPT-4\'s token stream'
        },
        'gemini': {
          architecture: 'Native Multimodal Transformer (Pathways)',
          innovation: 'Joint training from scratch on multimodal data',
          strengths: ['Video understanding', 'Mathematical reasoning', 'Long context'],
          technical: 'Unified architecture processes text, images, and video as equal input modalities'
        },
        'claude': {
          architecture: 'Constitutional AI + Vision Integration',
          innovation: 'Safety-focused vision-language alignment',
          strengths: ['Educational explanations', 'Safe content analysis', 'Detailed reasoning'],
          technical: 'Incorporates Constitutional AI principles into multimodal training pipeline'
        }
      };

      const detail = details[modelId];
      document.getElementById('architectureDetails').innerHTML = `
        <div class="step" style="margin-top: 20px;">
          <h4>üîç ${selectedModel.toUpperCase()} Deep Dive</h4>
          <div class="info">
            <strong>Architecture:</strong> ${detail.architecture}<br>
            <strong>Key Innovation:</strong> ${detail.innovation}<br>
            <strong>Core Strengths:</strong> ${detail.strengths.join(', ')}<br><br>
            <strong>Technical Details:</strong> ${detail.technical}
          </div>
        </div>`;
    }

    function generateTokenSequence() {
      const resolution = parseInt(document.getElementById('imageResolution').value);
      const patchSize = parseInt(document.getElementById('patchSize').value);
      const textInput = document.getElementById('textInput').value;

      // Calculate visual tokens
      const patchesPerSide = resolution / patchSize;
      const visualTokens = patchesPerSide * patchesPerSide;
      
      // Estimate text tokens (rough approximation)
      const textTokens = Math.ceil(textInput.split(' ').length * 1.3);

      // Generate token visualization
      let tokenHtml = '<div class="token-flow">';
      
      // Special tokens
      tokenHtml += '<div class="token special">[BOS]</div>';
      
      // Visual tokens
      for (let i = 0; i < Math.min(visualTokens, 20); i++) {
        tokenHtml += `<div class="token image">IMG_${i}</div>`;
      }
      if (visualTokens > 20) {
        tokenHtml += `<div class="token image">... +${visualTokens - 20}</div>`;
      }
      
      // Text tokens
      const words = textInput.split(' ');
      words.forEach((word, i) => {
        if (i < 15) {
          tokenHtml += `<div class="token text">${word}</div>`;
        }
      });
      if (words.length > 15) {
        tokenHtml += `<div class="token text">...</div>`;
      }
      
      tokenHtml += '<div class="token special">[EOS]</div>';
      tokenHtml += '</div>';

      const visualization = document.getElementById('tokenVisualization');
      visualization.innerHTML = `
        <div class="training-simulation">
          <h4>üé≠ Token Sequence Analysis</h4>
          ${tokenHtml}
          
          <div class="metric-grid" style="margin-top: 20px;">
            <div class="metric-card">
              <div class="metric-value">${visualTokens}</div>
              <div class="metric-label">Visual Tokens</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${textTokens}</div>
              <div class="metric-label">Text Tokens</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${visualTokens + textTokens + 2}</div>
              <div class="metric-label">Total Sequence Length</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${(resolution * resolution * 3 / 1024 / 1024).toFixed(1)}MB</div>
              <div class="metric-label">Raw Image Size</div>
            </div>
          </div>
          
          <div class="info" style="margin-top: 15px;">
            <strong>üí° Token Insights:</strong><br>
            ‚Ä¢ <strong>Visual tokens:</strong> Each ${patchSize}√ó${patchSize} patch becomes one token<br>
            ‚Ä¢ <strong>Sequence length:</strong> ${resolution}√ó${resolution} image = ${visualTokens} visual tokens<br>
            ‚Ä¢ <strong>Processing:</strong> All tokens flow through the same transformer architecture<br>
            ‚Ä¢ <strong>Efficiency:</strong> Higher resolution = more tokens = slower inference
          </div>
        </div>`;
    }

    function selectFusion(fusionType, element) {
      selectedFusion = fusionType;
      
      // Update visual selection
      document.querySelectorAll('.fusion-card').forEach(card => card.classList.remove('selected'));
      element.classList.add('selected');
      
      // Show fusion analysis
      const fusionDetails = {
        'early': {
          pros: ['Simple implementation', 'Maximum cross-modal interaction', 'Unified processing'],
          cons: ['High computational cost', 'Memory intensive', 'May lose modality-specific features'],
          use_case: 'Best for tasks requiring tight integration like visual question answering'
        },
        'late': {
          pros: ['Modality-specific optimization', 'Lower computational cost', 'Flexible architecture'],
          cons: ['Limited cross-modal interaction', 'May miss fine-grained relationships'],
          use_case: 'Good for tasks with distinct modality processing like image classification with text metadata'
        },
        'cross': {
          pros: ['Balanced approach', 'Controlled interaction', 'Interpretable attention'],
          cons: ['More complex architecture', 'Moderate computational cost', 'Requires careful design'],
          use_case: 'Ideal for document understanding and complex visual reasoning'
        },
        'adaptive': {
          pros: ['Dynamic fusion strategy', 'Task-adaptive', 'Potentially optimal performance'],
          cons: ['Complex training', 'Hard to interpret', 'Computationally expensive'],
          use_case: 'Research direction for multi-task vision-language systems'
        }
      };

      const detail = fusionDetails[fusionType];
      document.getElementById('fusionAnalysis').innerHTML = `
        <div class="step" style="margin-top: 20px;">
          <h4>üîÄ ${fusionType.charAt(0).toUpperCase() + fusionType.slice(1)} Fusion Analysis</h4>
          <div class="success">
            <strong>‚úÖ Advantages:</strong><br>
            ${detail.pros.map(pro => `‚Ä¢ ${pro}`).join('<br>')}
          </div>
          <div class="warning">
            <strong>‚ö†Ô∏è Trade-offs:</strong><br>
            ${detail.cons.map(con => `‚Ä¢ ${con}`).join('<br>')}
          </div>
          <div class="info">
            <strong>üéØ Best Use Case:</strong><br>
            ${detail.use_case}
          </div>
        </div>`;
    }

    function visualizeAttention() {
      const queryText = document.getElementById('queryText').value;
      const imageContent = document.getElementById('imageContent').value;
      const attentionHead = document.getElementById('attentionHead').value;

      // Simulate attention patterns based on query and content
      const attentionPatterns = {
        '1': [0.8, 0.6, 0.4, 0.2, 0.9, 0.3, 0.1, 0.5], // Object focus
        '2': [0.3, 0.7, 0.9, 0.5, 0.2, 0.8, 0.4, 0.6], // Color/texture
        '3': [0.5, 0.2, 0.3, 0.9, 0.6, 0.1, 0.8, 0.4], // Spatial relations
        '4': [0.1, 0.3, 0.2, 0.4, 0.5, 0.6, 0.9, 0.8]  // Text reading
      };

      const patterns = attentionPatterns[attentionHead];
      
      // Create attention matrix visualization
      let matrixHtml = '<div class="attention-matrix"><h4>Cross-Modal Attention Matrix</h4>';
      matrixHtml += '<div class="matrix-grid" style="grid-template-columns: repeat(9, 1fr);">';
      
      // Header
      matrixHtml += '<div class="matrix-cell" style="background: #2d2d2d; color: white; font-weight: bold;">Query\\Image</div>';
      for (let i = 0; i < 8; i++) {
        matrixHtml += `<div class="matrix-cell" style="background: #2d2d2d; color: white; font-weight: bold;">Patch ${i+1}</div>`;
      }
      
      // Query words and attention values
      const queryWords = queryText.split(' ').slice(0, 4); // First 4 words
      queryWords.forEach((word, wordIdx) => {
        matrixHtml += `<div class="matrix-cell" style="background: #2d2d2d; color: white; font-weight: bold;">${word}</div>`;
        patterns.forEach((attention, patchIdx) => {
          const adjustedAttention = attention * (0.8 + Math.random() * 0.4); // Add some variation
          const cellClass = adjustedAttention > 0.7 ? 'attention-high' : 
                           adjustedAttention > 0.4 ? 'attention-medium' : 'attention-low';
          matrixHtml += `<div class="matrix-cell ${cellClass}">${adjustedAttention.toFixed(2)}</div>`;
        });
      });
      
      matrixHtml += '</div></div>';

      const visualization = document.getElementById('attentionVisualization');
      visualization.innerHTML = `
        <div class="training-simulation">
          <h4>üîç Attention Pattern: "${queryText}"</h4>
          ${matrixHtml}
          
          <div class="info" style="margin-top: 15px;">
            <strong>üß† Attention Head ${attentionHead} Analysis:</strong><br>
            ${getAttentionAnalysis(attentionHead, queryText, imageContent)}
          </div>
        </div>`;
    }

    function getAttentionAnalysis(head, query, content) {
      const analyses = {
        '1': `<strong>Object Focus Head:</strong> This head specializes in identifying and localizing objects mentioned in the query. High attention on patches containing the main subject.`,
        '2': `<strong>Color/Texture Head:</strong> Focuses on visual properties like colors, textures, and surface details. Strong attention to patches with distinctive visual features.`,
        '3': `<strong>Spatial Relations Head:</strong> Analyzes spatial relationships and layout. Attention patterns reflect understanding of "left", "right", "above", "below" concepts.`,
        '4': `<strong>Text Reading Head:</strong> Specialized for OCR and text understanding within images. Highest attention on patches containing text or text-like patterns.`
      };
      return analyses[head];
    }

    function updateBenchmarks() {
	  const category = document.getElementById('benchmarkCategory').value;
	  const metric = document.getElementById('evaluationMetric').value;

	  const benchmarkData = {
		general: {
		  accuracy: { 'GPT-4V': 87.2, 'Gemini Pro': 84.3, 'Claude 3': 86.1 },
		  speed:    { 'GPT-4V': 12,   'Gemini Pro': 18,   'Claude 3': 15   },
		  cost:     { 'GPT-4V': 0.010,'Gemini Pro': 0.0025,'Claude 3': 0.0048 }
		},
		reasoning: {
		  accuracy: { 'GPT-4V': 78.5, 'Gemini Pro': 82.1, 'Claude 3': 79.8 },
		  speed:    { 'GPT-4V': 8,    'Gemini Pro': 12,   'Claude 3': 10   },
		  cost:     { 'GPT-4V': 0.015,'Gemini Pro': 0.004,'Claude 3': 0.007 }
		},
		ocr: {
		  accuracy: { 'GPT-4V': 94.1, 'Gemini Pro': 91.8, 'Claude 3': 95.2 },
		  speed:    { 'GPT-4V': 15,   'Gemini Pro': 22,   'Claude 3': 18   },
		  cost:     { 'GPT-4V': 0.008,'Gemini Pro': 0.002,'Claude 3': 0.004 }
		}
	  };

	  const results = document.getElementById('benchmarkResults');

	  // Guard against categories you list in the <select> but don't provide data for
	  if (!benchmarkData[category]) {
		results.innerHTML = `
		  <div class="warning"><strong>No data for "${category}".</strong> Try General, Reasoning, or OCR.</div>
		`;
		return;
	  }

	  const data = benchmarkData[category][metric];

	  let tableHtml = `
		<table class="benchmark-table">
		  <tr>
			<th>Model</th>
			<th>${metric === 'accuracy' ? 'Accuracy (%)' : metric === 'speed' ? 'Speed (tokens/sec)' : 'Cost ($/image)'}</th>
			<th>Ranking</th>
		  </tr>`;

	  const sorted = Object.entries(data).sort((a, b) =>
		metric === 'cost' ? a[1] - b[1] : b[1] - a[1]
	  );

	  sorted.forEach(([model, value], idx) => {
		const scoreClass = idx === 0 ? 'score-excellent' : idx === 1 ? 'score-good' : 'score-average';
		const formattedValue =
		  metric === 'accuracy' ? value.toFixed(1) :
		  metric === 'cost'     ? value.toFixed(4) :
								  String(value);

		tableHtml += `
		  <tr>
			<td><strong>${model}</strong></td>
			<td class="${scoreClass}">${formattedValue}</td>
			<td>#${idx + 1}</td>
		  </tr>`;
	  });

	  tableHtml += '</table>';

	  results.innerHTML = `
		<div class="training-simulation">
		  <h4>üìä ${category.charAt(0).toUpperCase() + category.slice(1)} Benchmarks - ${metric.charAt(0).toUpperCase() + metric.slice(1)}</h4>
		  ${tableHtml}
		  <div class="info" style="margin-top: 15px;">
			<strong>üìà Benchmark Notes:</strong><br>
			${getBenchmarkNotes(category, metric)}
		  </div>
		</div>`;
	}


    function getBenchmarkNotes(category, metric) {
      const notes = {
        'general': {
          'accuracy': 'Average across MMMU, VQAv2, and TextVQA benchmarks',
          'speed': 'Tokens generated per second on A100 GPU',
          'cost': 'Approximate pricing as of 2024, varies by provider'
        },
        'reasoning': {
          'accuracy': 'Performance on visual reasoning tasks (CLEVR, Visual7W)',
          'speed': 'Complex reasoning requires more computation time',
          'cost': 'Higher costs due to longer generation sequences'
        },
        'ocr': {
          'accuracy': 'Text extraction and document understanding accuracy',
          'speed': 'OCR tasks are typically faster than reasoning',
          'cost': 'Document processing optimized for cost efficiency'
        }
      };
      return notes[category][metric];
    }

    function showcaseApplications() {
      const industry = document.getElementById('industryVertical').value;
      const useCase = document.getElementById('useCaseType').value;

      const applications = {
        'healthcare': {
          'analysis': {
            title: 'üè• Medical Imaging Analysis',
            description: 'Automated radiology report generation and medical image interpretation',
            examples: ['X-ray abnormality detection', 'MRI scan analysis', 'Pathology slide examination'],
            roi: '60% reduction in reporting time, 95% accuracy maintained'
          },
          'automation': {
            title: 'ü§ñ Clinical Workflow Automation', 
            description: 'Streamline medical record processing and patient data extraction',
            examples: ['Insurance claim processing', 'Medical form digitization', 'Patient history extraction'],
            roi: '40% faster claim processing, 99.2% accuracy'
          }
        },
        'education': {
          'analysis': {
            title: 'üìö Educational Content Analysis',
            description: 'Automated textbook digitization and learning material creation',
            examples: ['Mathematical notation recognition', 'Diagram explanation generation', 'Quiz creation from images'],
            roi: '10x faster content creation, multilingual support'
          },
          'creative': {
            title: 'üé® Interactive Learning Experiences',
            description: 'AI tutors that can see and understand student work',
            examples: ['Handwriting analysis', 'Art critique generation', 'Science experiment explanation'],
            roi: 'Personalized learning at scale, 85% student engagement increase'
          }
        }
      };

      const app = applications[industry][useCase];
      const showcase = document.getElementById('applicationShowcase');
      
      showcase.innerHTML = `
        <div class="training-simulation">
          <div class="model-card" style="border-color: #28a745;">
            <div class="model-name">${app.title}</div>
            <div style="margin: 15px 0;">${app.description}</div>
            
            <div class="model-capabilities">
              <h5>Key Applications:</h5>
              ${app.examples.map(example => 
                `<div class="capability">
                  <span class="capability-icon">‚úÖ</span>
                  <span>${example}</span>
                </div>`
              ).join('')}
            </div>
            
            <div class="success" style="margin-top: 15px;">
              <strong>üéØ Business Impact:</strong> ${app.roi}
            </div>
          </div>
        </div>`;
    }

    function simulateVLMTraining() {
      const stage = document.getElementById('trainingStage').value;
      const dataset = document.getElementById('datasetType').value;
      const scale = document.getElementById('trainingScale').value;

      const trainingDetails = {
        'pretrain': {
          description: 'Large-scale pre-training on diverse image-text pairs',
          duration: scale === 'large' ? '2-4 weeks' : scale === 'medium' ? '1-2 weeks' : '3-5 days',
          compute: scale === 'large' ? '1000+ GPUs' : scale === 'medium' ? '100-500 GPUs' : '10-50 GPUs',
          loss: 'Combined language modeling + contrastive loss',
          outcome: 'General vision-language understanding capabilities'
        },
        'instruction': {
          description: 'Instruction tuning on curated vision-language tasks',
          duration: scale === 'large' ? '1-2 weeks' : scale === 'medium' ? '3-5 days' : '1-2 days', 
          compute: scale === 'large' ? '200-500 GPUs' : scale === 'medium' ? '50-100 GPUs' : '5-20 GPUs',
          loss: 'Cross-entropy loss on response generation',
          outcome: 'Ability to follow visual instructions and provide helpful responses'
        },
        'rlhf': {
          description: 'Human feedback optimization for preferred responses',
          duration: scale === 'large' ? '1 week' : scale === 'medium' ? '2-3 days' : '1 day',
          compute: scale === 'large' ? '100-200 GPUs' : scale === 'medium' ? '20-50 GPUs' : '5-10 GPUs',
          loss: 'PPO policy gradient with reward model',
          outcome: 'Responses aligned with human preferences and safety'
        }
      };

      const detail = trainingDetails[stage];
      const simulation = document.getElementById('trainingSimulation');
      
      simulation.innerHTML = `
        <div class="training-simulation">
          <div class="processing-animation" style="text-align: center; margin: 20px 0;">
            <div style="font-size: 2em;">üéì</div>
            <div style="font-weight: bold;">Simulating ${stage.toUpperCase()} Training...</div>
          </div>
          
          <div class="metric-grid">
            <div class="metric-card">
              <div class="metric-value">${detail.duration}</div>
              <div class="metric-label">Training Duration</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${detail.compute}</div>
              <div class="metric-label">Compute Requirements</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${dataset}</div>
              <div class="metric-label">Dataset Focus</div>
            </div>
            <div class="metric-card">
              <div class="metric-value">${scale}</div>
              <div class="metric-label">Training Scale</div>
            </div>
          </div>
          
          <div class="step">
            <h4>üìã Training Stage: ${stage.charAt(0).toUpperCase() + stage.slice(1)}</h4>
            <div class="info">
              <strong>Process:</strong> ${detail.description}<br>
              <strong>Loss Function:</strong> ${detail.loss}<br>
              <strong>Expected Outcome:</strong> ${detail.outcome}
            </div>
          </div>
        </div>`;
    }

    function switchTab(tabName, element) {
      // Remove active class from all tabs and content
      document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
      document.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));
      
      // Add active class to clicked tab and corresponding content
      if (element) element.classList.add('active');
      const content = document.getElementById(tabName);
      if (content) content.classList.add('active');
    }

    function copyCode(button) {
      const codeBlock = button.nextElementSibling;
      if (codeBlock && codeBlock.tagName === 'PRE') {
        const text = codeBlock.textContent;
        navigator.clipboard.writeText(text).then(() => {
          const originalText = button.textContent;
          button.textContent = '‚úÖ Copied';
          setTimeout(() => {
            button.textContent = originalText;
          }, 2000);
        });
      }
    }

	// JavaScript functions for Open Source VLM section
// Add these to the existing <script> section

let selectedOpenSourceModel = 'llava';
let activeOpenSourceTab = 'llava-training';

function exploreOpenSource() {
  const category = document.getElementById('openSourceCategory').value;
  const metric = document.getElementById('comparisonMetric').value;
  const useCase = document.getElementById('useCase').value;

  const openSourceData = {
    'all': {
      models: ['LLaVA-1.6-34B', 'InstructBLIP-13B', 'MiniGPT-4-13B', 'BLIP-2-12B', 'LLaVA-1.5-13B'],
      performance: [69.5, 64.2, 58.7, 56.7, 65.4],
      cost: [500000, 200000, 100000, 150000, 300000],
      parameters: [34, 13, 13, 12, 13]
    },
    'llava': {
      models: ['LLaVA-1.6-34B', 'LLaVA-1.5-13B', 'LLaVA-1.5-7B'],
      performance: [69.5, 65.4, 62.0],
      cost: [500000, 300000, 200000],
      parameters: [34, 13, 7]
    },
    'blip': {
      models: ['BLIP-2-12B', 'InstructBLIP-13B', 'InstructBLIP-7B'],
      performance: [56.7, 64.2, 60.9],
      cost: [150000, 200000, 120000],
      parameters: [12, 13, 7]
    }
  };

  const data = openSourceData[category] || openSourceData['all'];
  const results = document.getElementById('openSourceResults');

  let analysisHtml = `
    <div class="training-simulation">
      <h4>üìä Open Source VLM Analysis: ${category.toUpperCase()}</h4>
      
      <div class="metric-grid">
        <div class="metric-card">
          <div class="metric-value">${data.models.length}</div>
          <div class="metric-label">Available Models</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">${Math.max(...data.performance).toFixed(1)}%</div>
          <div class="metric-label">Best Performance</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">$${Math.min(...data.cost).toLocaleString()}</div>
          <div class="metric-label">Lowest Training Cost</div>
        </div>
        <div class="metric-card">
          <div class="metric-value">${Math.min(...data.parameters)}B</div>
          <div class="metric-label">Smallest Model</div>
        </div>
      </div>
      
      <div class="benchmark-table">
        <table>
          <tr>
            <th>Model</th>
            <th>Performance (%)</th>
            <th>Training Cost</th>
            <th>Parameters</th>
            <th>Best For</th>
          </tr>`;

  data.models.forEach((model, i) => {
    const perfClass = data.performance[i] > 65 ? 'score-excellent' : 
                     data.performance[i] > 60 ? 'score-good' : 'score-average';
    const costClass = data.cost[i] < 200000 ? 'score-excellent' : 
                     data.cost[i] < 400000 ? 'score-good' : 'score-average';
    
    const bestFor = getBestUseCase(model, data.performance[i], data.cost[i]);
    
    analysisHtml += `
      <tr>
        <td><strong>${model}</strong></td>
        <td class="${perfClass}">${data.performance[i].toFixed(1)}%</td>
        <td class="${costClass}">$${data.cost[i].toLocaleString()}</td>
        <td>${data.parameters[i]}B</td>
        <td>${bestFor}</td>
      </tr>`;
  });

  analysisHtml += `
        </table>
      </div>
      
      <div class="success">
        <strong>üéØ Recommendation for ${useCase}:</strong><br>
        ${getRecommendation(useCase, data)}
      </div>
    </div>`;

  results.innerHTML = analysisHtml;
}

function getBestUseCase(model, performance, cost) {
  if (model.includes('MiniGPT')) return 'Rapid prototyping, research';
  if (model.includes('34B')) return 'High-performance production';
  if (model.includes('InstructBLIP')) return 'Instruction following, education';
  if (cost < 200000) return 'Cost-sensitive deployments';
  return 'Balanced performance/cost';
}

function getRecommendation(useCase, data) {
  const recommendations = {
    'research': 'Start with MiniGPT-4 for quick experiments, then scale to LLaVA-1.6 for serious research projects.',
    'production': 'LLaVA-1.6-34B offers the best performance/cost ratio for production deployments.',
    'education': 'InstructBLIP excels at instruction following, making it ideal for educational applications.',
    'customization': 'All models are highly customizable, but LLaVA family has the most extensive documentation and community support.'
  };
  return recommendations[useCase] || recommendations['research'];
}

function selectOpenSourceModel(modelId, element) {
  selectedOpenSourceModel = modelId;
  
  // Update visual selection
  document.querySelectorAll('.model-card').forEach(card => card.classList.remove('selected'));
  element.classList.add('selected');
  
  const modelDetails = {
    'llava': {
      title: 'ü¶ô LLaVA Architecture Analysis',
      content: `
        <div class="step">
          <h4>üîç LLaVA-1.6 Technical Deep Dive</h4>
          
          <div class="math-formula">
            <strong>LLaVA Architecture Pipeline:</strong><br><br>
            
            <strong>1. Vision Encoding:</strong><br>
            CLIP-ViT-L/14: Image ‚Üí Patches ‚Üí Visual_Features ‚àà ‚Ñù^(576√ó1024)<br><br>
            
            <strong>2. Simple Projection:</strong><br>
            MLP: Visual_Features ‚Üí Language_Space ‚àà ‚Ñù^(576√ó4096)<br>
            Projection: Linear(1024) ‚Üí GELU() ‚Üí Linear(4096)<br><br>
            
            <strong>3. Token Integration:</strong><br>
            Sequence = [<s>, <image>, v‚ÇÅ, v‚ÇÇ, ..., v‚ÇÖ‚Çá‚ÇÜ, </image>, text_tokens, </s>]<br><br>
            
            <strong>4. Language Generation:</strong><br>
            Vicuna-34B processes full multimodal sequence
          </div>
          
          <div class="success">
            <strong>üéØ LLaVA's Key Innovations:</strong><br>
            ‚Ä¢ <strong>Simplicity:</strong> Minimal architecture complexity while maintaining effectiveness<br>
            ‚Ä¢ <strong>Scalability:</strong> Proven to scale from 7B to 34B parameters<br>
            ‚Ä¢ <strong>Data Efficiency:</strong> High performance with relatively small training datasets<br>
            ‚Ä¢ <strong>Community:</strong> Largest open-source VLM community and ecosystem<br><br>
            
            <strong>üìä Training Efficiency:</strong><br>
            ‚Ä¢ Stage 1 (Pretraining): 558K image-text pairs, ~20 hours on 8√óA100<br>
            ‚Ä¢ Stage 2 (Fine-tuning): 665K instruction pairs, ~40 hours on 8√óA100<br>
            ‚Ä¢ Total cost: ~$500K (vs $100M+ for GPT-4V)
          </div>
        </div>`
    },
    'instructblip': {
      title: 'üéØ InstructBLIP Architecture Analysis',
      content: `
        <div class="step">
          <h4>üîç InstructBLIP Q-Former Deep Dive</h4>
          
          <div class="math-formula">
            <strong>Q-Former Architecture:</strong><br><br>
            
            <strong>1. Learnable Queries:</strong><br>
            Q ‚àà ‚Ñù^(32√ó768) - Fixed number of learnable query vectors<br><br>
            
            <strong>2. Cross-Attention with Vision:</strong><br>
            Attention(Q, K_vision, V_vision) where K,V from frozen ViT<br><br>
            
            <strong>3. Instruction-Aware Processing:</strong><br>
            Q-Former attends to both vision features AND instruction text<br><br>
            
            <strong>4. Language Model Integration:</strong><br>
            Q_output ‚Üí Linear_projection ‚Üí Flan-T5/Vicuna input
          </div>
          
          <div class="info">
            <strong>üåü Q-Former Advantages:</strong><br>
            ‚Ä¢ <strong>Fixed Output Size:</strong> Always 32 query tokens regardless of image resolution<br>
            ‚Ä¢ <strong>Instruction Awareness:</strong> Q-Former considers the instruction when processing vision<br>
            ‚Ä¢ <strong>Efficient Training:</strong> Can freeze both vision encoder and language model<br>
            ‚Ä¢ <strong>Flexible Integration:</strong> Works with any language model (T5, Vicuna, etc.)
          </div>
        </div>`
    },
    'minigpt4': {
      title: '‚ö° MiniGPT-4 Architecture Analysis',
      content: `
        <div class="step">
          <h4>üîç MiniGPT-4 Minimal Training Approach</h4>
          
          <div class="math-formula">
            <strong>Ultra-Efficient Training Pipeline:</strong><br><br>
            
            <strong>1. Frozen Components:</strong><br>
            Eva-CLIP (frozen) + Vicuna-7B/13B (frozen)<br>
            Only projection layer is trainable!<br><br>
            
            <strong>2. Two-Stage Training:</strong><br>
            Stage 1: 5M image-text pairs, learn basic alignment<br>
            Stage 2: 3.5K high-quality conversations, learn chat ability<br><br>
            
            <strong>3. Minimal Parameters:</strong><br>
            Only ~5M trainable parameters out of 13B total<br>
            Training time: ~10 hours total on 4√óA100<br><br>
            
            <strong>4. Conversation Ability:</strong><br>
            Achieves ChatGPT-like conversational ability with minimal training
          </div>
          
          <div class="warning">
            <strong>üí° MiniGPT-4 Insights:</strong><br>
            ‚Ä¢ <strong>Proof of Concept:</strong> Shows that sophisticated VLM capabilities don't require massive training<br>
            ‚Ä¢ <strong>Research Value:</strong> Ideal for academic research with limited compute budgets<br>
            ‚Ä¢ <strong>Rapid Iteration:</strong> Can experiment with different approaches in hours, not days<br>
            ‚Ä¢ <strong>Foundation Model Power:</strong> Leverages strength of pre-trained components effectively
          </div>
        </div>`
    }
  };

  const details = modelDetails[modelId];
  document.getElementById('openSourceModelDetails').innerHTML = `
    <div class="step">
      <h4>${details.title}</h4>
      ${details.content}
    </div>`;
}

function generateComparison() {
  const benchmarkType = document.getElementById('benchmarkType').value;
  const costFactor = document.getElementById('costFactor').value;

  const comparisonData = {
    'mmmu': {
      'GPT-4V': { score: 77.2, cost: 100000000 },
      'Gemini Pro': { score: 73.6, cost: 50000000 },
      'Claude 3': { score: 75.3, cost: 30000000 },
      'LLaVA-1.6-34B': { score: 69.5, cost: 500000 },
      'InstructBLIP-13B': { score: 64.2, cost: 200000 },
      'MiniGPT-4-13B': { score: 58.7, cost: 100000 }
    },
    'vqav2': {
      'GPT-4V': { score: 87.2, cost: 100000000 },
      'Gemini Pro': { score: 85.9, cost: 50000000 },
      'Claude 3': { score: 86.1, cost: 30000000 },
      'LLaVA-1.6-34B': { score: 81.6, cost: 500000 },
      'InstructBLIP-13B': { score: 78.9, cost: 200000 },
      'MiniGPT-4-13B': { score: 75.4, cost: 100000 }
    }
  };

  const data = comparisonData[benchmarkType];
  const chart = document.getElementById('comparisonChart');

  let chartHtml = `
    <div class="training-simulation">
      <h4>üìä ${benchmarkType.toUpperCase()} Performance vs ${costFactor.charAt(0).toUpperCase() + costFactor.slice(1)} Cost</h4>
      
      <div class="benchmark-table">
        <table>
          <tr>
            <th>Model</th>
            <th>Performance</th>
            <th>Training Cost</th>
            <th>Performance/Cost Ratio</th>
            <th>Category</th>
          </tr>`;

  Object.entries(data).forEach(([model, stats]) => {
    const ratio = (stats.score / (stats.cost / 1000000)).toFixed(3);
    const category = model.includes('GPT-4V') || model.includes('Gemini') || model.includes('Claude') ? 'Closed Source' : 'Open Source';
    const categoryClass = category === 'Open Source' ? 'score-excellent' : 'score-good';
    
    chartHtml += `
      <tr>
        <td><strong>${model}</strong></td>
        <td>${stats.score}%</td>
        <td>$${stats.cost.toLocaleString()}</td>
        <td><strong>${ratio}</strong> pts/M$</td>
        <td class="${categoryClass}">${category}</td>
      </tr>`;
  });

  chartHtml += `
        </table>
      </div>
      
      <div class="success">
        <strong>üéØ Key Insights:</strong><br>
        ‚Ä¢ <strong>Open source models</strong> offer 10-1000x better performance/cost ratios<br>
        ‚Ä¢ <strong>LLaVA-1.6-34B</strong> achieves 90% of GPT-4V performance at 0.5% of the cost<br>
        ‚Ä¢ <strong>Performance gap</strong> between open and closed source is rapidly shrinking<br>
        ‚Ä¢ <strong>Training efficiency</strong> improvements make open source increasingly viable
      </div>
    </div>`;

  chart.innerHTML = chartHtml;
}

function switchOpenSourceTab(tabName, element) {
  activeOpenSourceTab = tabName;
  
  // Remove active class from all tabs and content
  document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
  document.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));
  
  // Add active class to clicked tab and corresponding content
  if (element) element.classList.add('active');
  const content = document.getElementById(tabName);
  if (content) content.classList.add('active');
}

// Constitutional AI JavaScript Functions
// Add these to your existing <script> section

// Global variables for Constitutional AI section
let selectedConstitutionalModel = 'claude';
let activeCAITab = 'basics';

// Switch Constitutional AI tabs
function switchCAITab(tabName, element) {
  activeCAITab = tabName;
  
  // Remove active class from all CAI tabs and content
  document.querySelectorAll('#constitutional-ai .tab').forEach(tab => tab.classList.remove('active'));
  document.querySelectorAll('#constitutional-ai .tab-content').forEach(content => content.classList.remove('active'));
  
  // Add active class to clicked tab and corresponding content
  if (element) element.classList.add('active');
  const content = document.getElementById(tabName);
  if (content) content.classList.add('active');
}

// Demonstrate constitutional challenges and solutions
function demonstrateConstitutionalChallenge() {
  const challengeType = document.getElementById('challengeType').value;
  const modelScenario = document.getElementById('modelScenario').value;
  
  const challenges = {
    'hallucination': {
      'before': {
        scenario: 'Model sees image of a park',
        response: 'I can see a beautiful park with several people playing frisbee, a dog running around, and children on swings. There\'s also a food truck and live music performance happening.',
        issues: ['Describes details not actually visible', 'Makes assumptions about activities', 'Adds non-existent elements']
      },
      'critique': {
        scenario: 'Constitutional self-critique process',
        response: 'Let me evaluate my response: Did I describe anything not visible? Yes - I mentioned frisbee, dog, swings, food truck, and music without clear visual evidence. I should only describe what I can actually see.',
        issues: ['Identifies specific hallucinations', 'Recognizes inference vs observation problem', 'Plans more accurate approach']
      },
      'after': {
        scenario: 'Constitutionally-corrected response',
        response: 'I can see a green outdoor space with trees and what appears to be several people in the distance. The lighting suggests it\'s daytime. I can\'t make out specific activities or details from this image.',
        issues: ['Only describes clearly visible elements', 'Acknowledges visual limitations', 'Maintains helpfulness without hallucinating']
      }
    },
    'privacy': {
      'before': {
        scenario: 'Image contains a person\'s face',
        response: 'This appears to be John Smith from the marketing department. Based on his expression, he seems stressed about the upcoming project deadline.',
        issues: ['Identifies private individual', 'Makes personal inferences', 'Violates privacy principles']
      },
      'critique': {
        scenario: 'Privacy-focused constitutional critique',
        response: 'Constitutional violation detected: I shouldn\'t identify individuals or make personal inferences. Even if I recognize someone, I should protect their privacy.',
        issues: ['Recognizes privacy violation', 'Identifies inappropriate personal inferences', 'Applies constitutional privacy principles']
      },
      'after': {
        scenario: 'Privacy-protected response',
        response: 'I can see a person in what appears to be an office or work environment. I don\'t identify individuals in images to protect privacy. If you have questions about the setting or context, I\'d be happy to help with those aspects.',
        issues: ['Protects individual privacy', 'Still provides helpful information', 'Explains constitutional reasoning']
      }
    },
    'bias': {
      'before': {
        scenario: 'Image of person in cultural dress',
        response: 'This person is probably from a developing country based on their traditional clothing. They likely work in agriculture or manual labor.',
        issues: ['Makes stereotypical assumptions', 'Links clothing to economic status', 'Reinforces cultural biases']
      },
      'critique': {
        scenario: 'Bias detection and correction',
        response: 'I made harmful assumptions linking traditional clothing to economic development and occupation. This reinforces stereotypes and isn\'t based on visual evidence.',
        issues: ['Identifies stereotypical reasoning', 'Recognizes lack of visual evidence', 'Acknowledges potential harm']
      },
      'after': {
        scenario: 'Culturally sensitive response',
        response: 'I can see a person wearing what appears to be traditional or cultural clothing with beautiful patterns and colors. Cultural dress represents rich traditions and heritage across many societies.',
        issues: ['Focuses on observable details', 'Provides positive cultural context', 'Avoids harmful assumptions']
      }
    }
  };

  const challenge = challenges[challengeType];
  const scenario = challenge[modelScenario];
  
  const demo = document.getElementById('constitutionalDemo');
  demo.innerHTML = `
    <div class="training-simulation">
      <div class="conversation-demo">
        <div class="message user">
          <div class="image-placeholder">üì∏ [${scenario.scenario}]</div>
          "Analyze this image."
        </div>
        
        <div class="message assistant ${modelScenario === 'before' ? '' : modelScenario === 'critique' ? 'system' : ''}">
          ${scenario.response}
        </div>
      </div>
      
      <div class="step" style="margin-top: 20px;">
        <h4>üîç Constitutional Analysis: ${challengeType.charAt(0).toUpperCase() + challengeType.slice(1)} - ${modelScenario.charAt(0).toUpperCase() + modelScenario.slice(1)}</h4>
        
        <div class="${modelScenario === 'after' ? 'success' : modelScenario === 'critique' ? 'info' : 'warning'}">
          <strong>${modelScenario === 'after' ? '‚úÖ Constitutional Success' : modelScenario === 'critique' ? 'üîç Self-Critique Process' : '‚ö†Ô∏è Constitutional Issues'}:</strong><br>
          ${scenario.issues.map(issue => `‚Ä¢ ${issue}`).join('<br>')}
        </div>
      </div>
    </div>`;
}

// Explore VLM constitutional challenges
function exploreVLMChallenge() {
  const challenge = document.getElementById('vlmChallenge').value;
  const approach = document.getElementById('constitutionalApproach').value;
  
  const challengeData = {
    'hallucination': {
      'prevention': {
        title: 'üõ°Ô∏è Hallucination Prevention Strategy',
        content: `
          <div class="code-block">
            <div class="code-header">Prevention Implementation</div>
<pre>class HallucinationPrevention:
    def __init__(self):
        self.confidence_threshold = 0.8
        self.uncertainty_phrases = [
            "I can see what appears to be...",
            "This looks like it might be...", 
            "I'm not entirely certain, but..."
        ]
    
    def generate_with_uncertainty(self, image, prompt):
        # Get confidence scores for visual elements
        visual_confidences = self.assess_visual_confidence(image)
        
        # Use uncertainty language for low-confidence observations
        response = self.generate_cautious_response(
            image, prompt, visual_confidences
        )
        
        return response</pre>
          </div>
          
          <div class="info">
            <strong>üéØ Prevention Strategies:</strong><br>
            ‚Ä¢ Train models to express uncertainty about unclear visual elements<br>
            ‚Ä¢ Use confidence thresholds before making specific claims<br>
            ‚Ä¢ Implement visual attention validation<br>
            ‚Ä¢ Regular hallucination detection during training
          </div>`
      },
      'detection': {
        title: 'üîç Hallucination Detection Method', 
        content: `
          <div class="info">
            <strong>üîç Detection Techniques:</strong><br>
            ‚Ä¢ <strong>Cross-reference validation:</strong> Compare descriptions with actual image features<br>
            ‚Ä¢ <strong>Attention analysis:</strong> Verify model attended to described regions<br>
            ‚Ä¢ <strong>Consistency checking:</strong> Multiple generations should align<br>
            ‚Ä¢ <strong>External validation:</strong> Use object detection models to verify claims
          </div>`
      },
      'correction': {
        title: '‚úèÔ∏è Hallucination Correction Process',
        content: `
          <div class="success">
            <strong>‚úèÔ∏è Correction Process:</strong><br>
            ‚Ä¢ <strong>Self-critique:</strong> Model identifies its own hallucinations<br>
            ‚Ä¢ <strong>Evidence verification:</strong> Check claims against visual evidence<br>
            ‚Ä¢ <strong>Uncertainty integration:</strong> Replace false claims with appropriate uncertainty<br>
            ‚Ä¢ <strong>Educational reframing:</strong> Turn corrections into learning opportunities
          </div>`
      }
    },
    'privacy': {
      'prevention': {
        title: 'üîí Privacy Protection Strategy',
        content: `
          <div class="warning">
            <strong>üîí Privacy Prevention:</strong><br>
            ‚Ä¢ <strong>Face detection filtering:</strong> Identify and mask faces during processing<br>
            ‚Ä¢ <strong>PII recognition:</strong> Detect and redact personal information in images<br>
            ‚Ä¢ <strong>Constitutional training:</strong> Never identify individuals regardless of recognition<br>
            ‚Ä¢ <strong>Consent verification:</strong> Only process images with appropriate permissions
          </div>`
      }
    },
    'medical': {
      'prevention': {
        title: '‚öïÔ∏è Medical Safety Strategy',
        content: `
          <div class="warning">
            <strong>‚öïÔ∏è Medical Constitutional Principles:</strong><br>
            ‚Ä¢ <strong>No diagnoses:</strong> Never provide medical diagnoses from images<br>
            ‚Ä¢ <strong>Educational only:</strong> Limit to general educational information<br>
            ‚Ä¢ <strong>Professional referral:</strong> Always recommend consulting healthcare providers<br>
            ‚Ä¢ <strong>Liability awareness:</strong> Acknowledge limitations of AI medical analysis
          </div>`
      }
    }
  };

  const data = challengeData[challenge]?.[approach];
  const results = document.getElementById('vlmChallengeResults');
  
  if (data) {
    results.innerHTML = `
      <div class="training-simulation">
        <h4>${data.title}</h4>
        ${data.content}
      </div>`;
  } else {
    results.innerHTML = `
      <div class="info">
        <strong>üîç Challenge: ${challenge.charAt(0).toUpperCase() + challenge.slice(1)}</strong><br>
        ${approach.charAt(0).toUpperCase() + approach.slice(1)} strategies for this challenge are being developed. Constitutional AI provides frameworks for addressing these complex multimodal ethical challenges.
      </div>`;
  }
}

// Select constitutional model for analysis
function selectConstitutionalModel(modelId, element) {
  selectedConstitutionalModel = modelId;
  
  // Update visual selection
  document.querySelectorAll('.model-card').forEach(card => card.classList.remove('selected'));
  element.classList.add('selected');
  
  const modelAnalysis = {
    'claude': {
      title: 'ü§ñ Claude\'s Constitutional Vision Implementation',
      content: `
        <div class="step">
          <h4>üõ°Ô∏è Claude 3 Constitutional Architecture</h4>
          
          <div class="success">
            <strong>üèõÔ∏è Constitutional Integration:</strong><br>
            ‚Ä¢ <strong>Built-in from training:</strong> Constitutional principles integrated during initial VLM training<br>
            ‚Ä¢ <strong>Visual safety filtering:</strong> Images screened for constitutional compliance before processing<br>
            ‚Ä¢ <strong>Response evaluation:</strong> Every generated response evaluated against constitutional principles<br>
            ‚Ä¢ <strong>Real-time self-correction:</strong> Model can revise responses mid-generation<br><br>
            
            <strong>üìä Constitutional Benefits Observed:</strong><br>
            ‚Ä¢ 89% reduction in inappropriate personal identification<br>
            ‚Ä¢ 76% fewer medical diagnosis attempts from images<br>
            ‚Ä¢ 95% accuracy in cultural sensitivity<br>
            ‚Ä¢ Consistent educational focus across diverse visual content
          </div>
          
          <div class="code-block">
            <div class="code-header">üß† Claude's Constitutional Vision Process</div>
<pre>class ClaudeConstitutionalVision:
    def __init__(self):
        self.principles = CLAUDE_CONSTITUTION
        self.visual_safety_threshold = 0.9
        
    def process_image_query(self, image, query):
        # 1. Constitutional visual safety check
        safety_assessment = self.assess_image_safety(image)
        if not safety_assessment.safe:
            return safety_assessment.explanation
        
        # 2. Generate initial response
        initial = self.generate_response(image, query)
        
        # 3. Constitutional self-evaluation
        evaluation = self.constitutional_evaluate(image, query, initial)
        
        # 4. Self-correct if needed
        if evaluation.needs_improvement:
            return self.constitutional_revise(image, query, initial, evaluation)
        
        return initial
    
    def constitutional_evaluate(self, image, query, response):
        critique_aspects = {
            'accuracy': self.critique_visual_accuracy(image, response),
            'safety': self.critique_safety(image, query, response), 
            'helpfulness': self.critique_helpfulness(query, response),
            'privacy': self.critique_privacy(image, response)
        }
        
        overall_score = sum(critique_aspects.values()) / len(critique_aspects)
        needs_improvement = overall_score < 0.8
        
        return ConstitutionalEvaluation(
            critique_aspects, overall_score, needs_improvement
        )</pre>
          </div>
        </div>`
    },
    'gpt4v': {
      title: 'ü§ñ GPT-4V\'s Safety-First Approach',
      content: `
        <div class="step">
          <h4>üõ°Ô∏è GPT-4V Safety Implementation</h4>
          
          <div class="warning">
            <strong>üö® Post-Hoc Safety Approach:</strong><br>
            ‚Ä¢ <strong>Content filtering:</strong> Images filtered before processing<br>
            ‚Ä¢ <strong>Response monitoring:</strong> Outputs checked against usage policies<br>
            ‚Ä¢ <strong>Limited self-correction:</strong> Minimal constitutional self-awareness<br>
            ‚Ä¢ <strong>Policy enforcement:</strong> Rule-based rather than principle-based<br><br>
            
            <strong>‚ö†Ô∏è Limitations:</strong><br>
            ‚Ä¢ Less transparent about safety reasoning<br>
            ‚Ä¢ Reactive rather than proactive safety measures<br>
            ‚Ä¢ Limited ability to explain safety decisions<br>
            ‚Ä¢ Potential for inconsistent behavior across similar cases
          </div>
        </div>`
    },
    'gemini': {
      title: 'üíé Gemini\'s Responsible AI Integration',
      content: `
        <div class="step">
          <h4>üõ°Ô∏è Gemini\'s Multi-Layered Safety</h4>
          
          <div class="info">
            <strong>üåü Responsible AI Features:</strong><br>
            ‚Ä¢ <strong>Cultural sensitivity training:</strong> Extensive cross-cultural safety training<br>
            ‚Ä¢ <strong>Harm assessment:</strong> Multi-dimensional harm evaluation<br>
            ‚Ä¢ <strong>Transparent safety settings:</strong> User-configurable safety levels<br>
            ‚Ä¢ <strong>Emerging constitutional elements:</strong> Moving toward principle-based safety<br><br>
            
            <strong>üìà Unique Advantages:</strong><br>
            ‚Ä¢ Video safety analysis (unique capability)<br>
            ‚Ä¢ Multi-language safety considerations<br>
            ‚Ä¢ Integration with Google's existing safety infrastructure<br>
            ‚Ä¢ Large-scale safety data from YouTube and web crawling
          </div>
        </div>`
    }
  };

  const analysis = modelAnalysis[modelId];
  document.getElementById('constitutionalModelAnalysis').innerHTML = analysis.content;
}

// Simulate constitutional self-correction process
function simulateConstitutionalCorrection() {
  const scenarioType = document.getElementById('scenarioType').value;
  const imageContext = document.getElementById('imageContext').value;
  const userQuestion = document.getElementById('userQuestion').value;

  const scenarios = {
    'hallucination': {
      'people': {
        initial: 'I can see three people having an animated conversation near a coffee shop. One person is wearing a red jacket and gesturing excitedly while talking on their phone. There\'s a golden retriever sitting nearby.',
        critique: 'Constitutional critique: I described specific details (three people, animated conversation, red jacket, phone call, golden retriever) without clear visual evidence. I should only describe what I can actually see clearly.',
        revised: 'I can see what appears to be people in an outdoor area. Due to image resolution and angle, I can\'t make out specific details about their activities or clothing. If you\'re looking for specific information about the scene, a higher resolution image would help.'
      }
    },
    'privacy': {
      'people': {
        initial: 'This is Sarah from accounting. She looks tired, probably from working late on the quarterly reports. Her expression suggests she\'s stressed about something.',
        critique: 'Constitutional violation: I identified a specific individual and made personal inferences about their mental state and work situation. I should protect privacy and avoid personal speculation.',
        revised: 'I can see a person in what appears to be a workplace setting. I don\'t identify individuals in images to protect privacy. If you have questions about the general setting or context I can observe, I\'d be happy to help with those aspects.'
      }
    },
    'bias': {
      'people': {
        initial: 'Based on their appearance and clothing style, this person is probably from Eastern Europe and works in a technical field. They seem very serious and focused.',
        critique: 'Constitutional issue: I made assumptions about someone\'s background, profession, and personality based on their appearance. This reinforces harmful stereotypes and isn\'t based on observable evidence.',
        revised: 'I can see a person in what appears to be a professional or indoor setting. I avoid making assumptions about people\'s backgrounds, occupations, or personalities based on their appearance, as this can perpetuate harmful stereotypes.'
      }
    },
    'educational': {
      'people': {
        initial: 'There are some people in the image.',
        critique: 'While my response avoids problems, it\'s not very educational or helpful. I can provide more value while still respecting privacy and avoiding assumptions.',
        revised: 'I can see several people in what appears to be a social or community setting. The image shows human interaction and gathering, which is an important aspect of social behavior. If you\'re interested in learning about social dynamics or group behavior, I\'d be happy to discuss those topics in general terms.'
      }
    }
  };

  const scenario = scenarios[scenarioType]?.[imageContext];
  const simulation = document.getElementById('constitutionalSimulation');

  if (scenario) {
    simulation.innerHTML = `
      <div class="training-simulation">
        <h4>üé≠ Constitutional Self-Correction: ${scenarioType.charAt(0).toUpperCase() + scenarioType.slice(1)}</h4>
        
        <div class="timeline">
          <div class="timeline-item">
            <div class="timeline-date">Step 1</div>
            <div class="timeline-content">
              <h5>üìù Initial Response</h5>
              <div class="message assistant" style="max-width: 100%; margin: 10px 0;">
                "${scenario.initial}"
              </div>
            </div>
          </div>
          
          <div class="timeline-item">
            <div class="timeline-date">Step 2</div>
            <div class="timeline-content">
              <h5>üîç Constitutional Self-Critique</h5>
              <div class="message system" style="max-width: 100%; margin: 10px 0;">
                <em>${scenario.critique}</em>
              </div>
            </div>
          </div>
          
          <div class="timeline-item">
            <div class="timeline-date">Step 3</div>
            <div class="timeline-content">
              <h5>‚ú® Constitutionally-Revised Response</h5>
              <div class="message assistant" style="max-width: 100%; margin: 10px 0; background: #d4edda;">
                "${scenario.revised}"
              </div>
            </div>
          </div>
        </div>
        
        <div class="success" style="margin-top: 20px;">
          <strong>üéØ Constitutional Improvement:</strong><br>
          This example shows how Constitutional AI enables VLMs to self-correct ${scenarioType} issues by applying explicit principles during response generation, resulting in more responsible and helpful outputs.
        </div>
      </div>`;
  } else {
    simulation.innerHTML = `
      <div class="info">
        <strong>üîÑ Constitutional Processing:</strong><br>
        Constitutional AI would help address ${scenarioType} challenges in ${imageContext} contexts through principled self-evaluation and correction.
      </div>`;
  }
}

// Initialize Constitutional AI section
function initializeConstitutionalAI() {
  // Set default selections
  selectConstitutionalModel('claude', document.querySelector('.model-card'));
  demonstrateConstitutionalChallenge();
  exploreVLMChallenge();
  simulateConstitutionalCorrection();
}
// Initialize open source section on page load
document.addEventListener('DOMContentLoaded', () => {
  // Set initial selections
  selectOpenSourceModel('llava', document.querySelector('.model-card'));
  exploreOpenSource();
  generateComparison();
  initializeConstitutionalAI();
});
	  
  </script>
</body>
</html>
