<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Training VLAs: Data, Models & Pipelines</title>
  <style>
    body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:1200px;margin:0 auto;padding:20px;background:linear-gradient(135deg,#1a1a1a 0%,#2d2d2d 100%);color:#e0e0e0;line-height:1.6}
    .container{background:#fff;color:#2d2d2d;border-radius:20px;padding:30px;margin:20px 0;border:1px solid #e0e0e0;box-shadow:0 4px 20px rgba(0,0,0,.1)}
    .nav-bar{background:#2d2d2d;color:#fff;padding:15px 30px;border-radius:15px;margin:20px 0;display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:15px}
    .nav-home,.nav-prev,.nav-next{background:#fff;color:#2d2d2d;padding:8px 16px;border-radius:6px;text-decoration:none;font-weight:bold;transition:all .3s}
    .nav-next{background:#28a745;color:#fff}
    .nav-home:hover,.nav-prev:hover,.nav-next:hover{transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    .nav-title{font-size:1.2em;font-weight:bold;flex:1;min-width:300px}
    .step{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;margin:15px 0;border-radius:15px;border-left:4px solid #2d2d2d}
    .interactive-demo{background:#f8f9fa;border:2px solid #e9ecef;border-radius:15px;padding:25px;margin:20px 0}
    .demo-title{font-size:1.3em;font-weight:bold;margin-bottom:20px;text-align:center;color:#2d2d2d}
    .controls{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .control-group{display:flex;flex-direction:column;gap:5px}
    .control-group label{font-weight:bold;font-size:14px;color:#2d2d2d}
    button{background:#2d2d2d;border:none;color:#fff;padding:12px 24px;border-radius:8px;cursor:pointer;font-weight:bold;transition:all .3s;margin:5px}
    button:hover{background:#1a1a1a;transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}
    button.primary{background:#28a745}
    button.primary:hover{background:#218838}
    input,select,textarea{padding:8px 12px;border:1px solid #dadce0;border-radius:6px;background:#fff;color:#2d2d2d}
    textarea{min-height:70px;resize:vertical}
    .math-formula{background:#f8f9fa;border:1px solid #e9ecef;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;text-align:center;font-size:16px;box-shadow:0 2px 8px rgba(0,0,0,.1)}
    .code-block{background:#2d2d2d;color:#e0e0e0;padding:20px;border-radius:8px;margin:15px 0;font-family:'Courier New',monospace;font-size:14px;overflow:auto;position:relative}
    .code-header{background:#1a1a1a;color:#28a745;padding:8px 15px;margin:-20px -20px 15px -20px;border-radius:8px 8px 0 0;font-weight:bold;font-size:12px}
    .copy-button{position:absolute;top:10px;right:10px;background:#28a745;color:#fff;border:none;padding:4px 8px;border-radius:4px;cursor:pointer;font-size:10px}
    .data-flow{display:flex;justify-content:space-between;align-items:center;margin:20px 0;padding:20px;background:#fff;border-radius:12px;border:2px solid #e9ecef;flex-wrap:wrap;gap:15px}
    .data-component{background:#f8f9fa;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center;min-width:120px;flex:1}
    .data-component.collection{border-color:#dc3545}
    .data-component.processing{border-color:#007bff}
    .data-component.training{border-color:#fd7e14}
    .data-component.evaluation{border-color:#28a745}
    .data-arrow{font-size:24px;color:#28a745;font-weight:bold}
    .dataset-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:20px;margin:20px 0}
    .dataset-card{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;transition:all .3s;cursor:pointer}
    .dataset-card:hover{border-color:#28a745;transform:translateY(-3px);box-shadow:0 6px 20px rgba(0,0,0,.1)}
    .dataset-card.selected{border-color:#28a745;background:#d4edda}
    .dataset-name{font-size:1.3em;font-weight:bold;margin-bottom:10px;color:#2d2d2d}
    .dataset-specs{background:#f8f9fa;padding:12px;border-radius:6px;margin:10px 0;font-size:12px;font-family:'Courier New',monospace}
    .dataset-stats{margin:15px 0}
    .stat{display:flex;align-items:center;margin:5px 0;font-size:14px}
    .stat-icon{margin-right:8px;font-size:16px}
    .metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(150px,1fr));gap:15px;margin:20px 0}
    .metric-card{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center}
    .metric-value{font-size:1.5em;font-weight:bold;color:#2d2d2d;margin-bottom:5px}
    .metric-label{font-size:12px;color:#666}
    .training-pipeline{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0}
    .pipeline-stage{background:#f8f9fa;border:1px solid #e9ecef;border-radius:8px;padding:15px;margin:10px 0;position:relative}
    .stage-number{background:#28a745;color:#fff;width:30px;height:30px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-weight:bold;position:absolute;left:-15px;top:50%;transform:translateY(-50%)}
    .pipeline-content{margin-left:30px}
    .model-comparison{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:20px;margin:20px 0}
    .model-card{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;transition:all .3s;cursor:pointer}
    .model-card:hover{border-color:#28a745;transform:translateY(-3px);box-shadow:0 6px 20px rgba(0,0,0,.1)}
    .model-card.selected{border-color:#28a745;background:#d4edda}
    .model-name{font-size:1.3em;font-weight:bold;margin-bottom:10px;color:#2d2d2d}
    .model-specs{background:#f8f9fa;padding:12px;border-radius:6px;margin:10px 0;font-size:12px;font-family:'Courier New',monospace}
    .model-capabilities{margin:15px 0}
    .capability{display:flex;align-items:center;margin:5px 0;font-size:14px}
    .capability-icon{margin-right:8px;font-size:16px}
    .tabs{display:flex;margin-bottom:20px;border-bottom:2px solid #e9ecef;flex-wrap:wrap}
    .tab{padding:12px 20px;cursor:pointer;border-bottom:3px solid transparent;font-weight:bold;transition:all .3s;margin-bottom:-2px}
    .tab:hover{background:#f8f9fa}
    .tab.active{border-bottom-color:#28a745;background:#d4edda;color:#155724}
    .tab-content{display:none}
    .tab-content.active{display:block}
    .info{background:#d1ecf1;border-left:4px solid #17a2b8;color:#0c5460;padding:15px;border-radius:8px;margin:15px 0}
    .success{background:#d4edda;border-left:4px solid #28a745;color:#155724;padding:15px;border-radius:8px;margin:15px 0}
    .warning{background:#fff3cd;border-left:4px solid #ffc107;color:#856404;padding:15px;border-radius:8px;margin:15px 0}
    .danger{background:#f8d7da;border-left:4px solid #dc3545;color:#721c24;padding:15px;border-radius:8px;margin:15px 0}
    .breakthrough-highlight{background:linear-gradient(135deg,#28a745,#20c997);color:#fff;padding:20px;border-radius:12px;text-align:center;margin:20px 0;font-size:18px;font-weight:bold;box-shadow:0 4px 15px rgba(40,167,69,.3)}
    .training-simulator{background:linear-gradient(135deg,#e3f2fd,#f3e5f5);border:3px solid #28a745;border-radius:15px;padding:25px;margin:20px 0}
    .sim-controls{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:20px;margin:20px 0}
    .sim-output{background:#fff;border:2px solid #28a745;border-radius:10px;padding:20px;margin:15px 0}
    .cost-breakdown{background:#fff;border:1px solid #e9ecef;border-radius:8px;margin:10px 0;overflow:hidden}
    .cost-header{background:#2d2d2d;color:#fff;padding:10px 15px;font-weight:bold}
    .cost-item{display:flex;justify-content:space-between;padding:8px 15px;border-bottom:1px solid #e9ecef}
    .cost-total{background:#28a745;color:#fff;padding:10px 15px;font-weight:bold}
    .progress-bar{background:#e9ecef;border-radius:10px;height:8px;margin:10px 0;overflow:hidden}
    .progress-fill{background:#28a745;height:100%;transition:width 1s ease}
    .benchmark-table{width:100%;border-collapse:collapse;margin:15px 0;background:#fff;border:1px solid #e9ecef;border-radius:8px;overflow:hidden}
    .benchmark-table th{background:#2d2d2d;color:#fff;padding:12px;text-align:center;font-weight:bold}
    .benchmark-table td{padding:12px;text-align:center;border-bottom:1px solid #e9ecef}
    .score-excellent{background:#d4edda;color:#155724;font-weight:bold}
    .score-good{background:#d1ecf1;color:#0c5460;font-weight:bold}
    .score-average{background:#fff3cd;color:#856404;font-weight:bold}
    .score-poor{background:#f8d7da;color:#721c24;font-weight:bold}
    .data-quality-indicator{display:inline-block;width:20px;height:20px;border-radius:50%;margin:0 5px;position:relative}
    .quality-excellent{background:#28a745}
    .quality-good{background:#17a2b8}
    .quality-average{background:#ffc107}
    .quality-poor{background:#dc3545}
    .synthetic-demo{background:#fff;border:2px dashed #28a745;border-radius:10px;padding:20px;margin:15px 0;text-align:center}
    .robot-icon{font-size:3em;margin:15px 0;animation:robotTrain 3s ease-in-out infinite}
    @keyframes robotTrain{0%{transform:scale(1) rotate(0deg)}50%{transform:scale(1.1) rotate(5deg)}100%{transform:scale(1) rotate(0deg)}}
    .pipeline-visualization{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0;position:relative}
    .pipeline-step{background:#f8f9fa;border:2px solid #e9ecef;border-radius:8px;padding:15px;margin:10px;text-align:center;position:relative}
    .pipeline-step.active{border-color:#28a745;background:#d4edda}
    .pipeline-arrow{position:absolute;right:-20px;top:50%;transform:translateY(-50%);font-size:24px;color:#28a745;font-weight:bold}
    .evaluation-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px;margin:20px 0}
    .eval-metric{background:#fff;border:2px solid #e9ecef;border-radius:8px;padding:15px;text-align:center}
    .eval-score{font-size:2em;font-weight:bold;margin:10px 0}
    .eval-label{font-size:12px;color:#666}
    .data-source-explorer{background:#fff;border:2px solid #e9ecef;border-radius:12px;padding:20px;margin:15px 0}
    .source-tabs{display:flex;gap:10px;margin-bottom:20px;flex-wrap:wrap}
    .source-tab{background:#e9ecef;color:#2d2d2d;padding:8px 16px;border-radius:6px;cursor:pointer;transition:all .3s;font-size:12px}
    .source-tab.active{background:#28a745;color:#fff}
    .source-content{background:#f8f9fa;padding:15px;border-radius:8px;min-height:100px}
  </style>
</head>
<body>
  <div class="nav-bar">
    <div class="nav-title">üõ†Ô∏è Training VLAs: Data, Models & Pipelines</div>
    <a href="index.html" class="nav-home">üè† Home</a>
    <a href="vision-language-action.html" class="nav-prev">‚Üê VLA Fundamentals</a>
    <a href="deploying-vlas.html" class="nav-next">Next: Deploying VLAs ‚Üí</a>
  </div>

  <div class="container">
    <h1>üõ†Ô∏è From Zero to Robot: The Complete VLA Training Pipeline</h1>
    <p>Building Vision-Language-Action models requires mastering a complex pipeline: from curating multi-robot datasets to optimizing training infrastructure. This tutorial takes you through the entire journey of training production-ready VLA models, covering data collection, model architectures, training strategies, and evaluation methodologies.</p>
    
    <div class="breakthrough-highlight">
      üéØ The Goal: Train a VLA model that can control multiple robot types with just natural language instructions
    </div>
  </div>

  <div class="container">
    <h2>üìä Section 1: The Training Data Ecosystem</h2>
    
    <div class="step">
      <h3>üåê Open X-Embodiment: The Foundation Dataset</h3>
      <p>The <strong>Open X-Embodiment Dataset</strong> represents the largest collection of robot demonstration data, enabling cross-embodiment learning at unprecedented scale. Understanding this ecosystem is crucial for training effective VLA models.</p>

      <div class="data-flow">
        <div class="data-component collection">
          <h4>üìö Data Sources</h4>
          <div>60+ Datasets</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Academia: Berkeley, Stanford<br>‚Ä¢ Industry: Google, Meta<br>‚Ä¢ Community: HuggingFace LeRobot
          </div>
        </div>
        <div class="data-arrow">‚Üí</div>
        <div class="data-component processing">
          <h4>üîÑ Standardization</h4>
          <div>Unified Format</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Action spaces normalized<br>‚Ä¢ Vision preprocessing<br>‚Ä¢ Task annotations
          </div>
        </div>
        <div class="data-arrow">‚Üí</div>
        <div class="data-component training">
          <h4>üéØ Training Split</h4>
          <div>Multi-Embodiment</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ 15+ robot types<br>‚Ä¢ 1M+ demonstrations<br>‚Ä¢ 100+ task categories
          </div>
        </div>
        <div class="data-arrow">‚Üí</div>
        <div class="data-component evaluation">
          <h4>‚úÖ Evaluation</h4>
          <div>Cross-Robot Transfer</div>
          <div style="font-size:12px;margin-top:5px">
            ‚Ä¢ Unseen robots<br>‚Ä¢ Novel tasks<br>‚Ä¢ Real-world validation
          </div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üîç Dataset Explorer</div>
        <p><strong>Explore the training data ecosystem for VLA models:</strong></p>

        <div class="data-source-explorer">
          <div class="source-tabs">
            <div class="source-tab active" onclick="switchDataSource('oxe', this)">üåê Open X-Embodiment</div>
            <div class="source-tab" onclick="switchDataSource('aloha', this)">ü¶æ ALOHA</div>
            <div class="source-tab" onclick="switchDataSource('bridge', this)">üåâ Bridge Data</div>
            <div class="source-tab" onclick="switchDataSource('rt1', this)">ü§ñ RT-1 Dataset</div>
            <div class="source-tab" onclick="switchDataSource('synthetic', this)">üé≠ Synthetic Data</div>
          </div>
          <div id="dataSourceContent" class="source-content"></div>
        </div>

        <div class="controls">
          <div class="control-group">
            <label>Dataset Filter:</label>
            <select id="datasetFilter">
              <option value="all" selected>All Datasets</option>
              <option value="manipulation">Manipulation Only</option>
              <option value="navigation">Navigation Only</option>
              <option value="mobile">Mobile Robots</option>
              <option value="humanoid">Humanoid Robots</option>
            </select>
          </div>
          <div class="control-group">
            <label>Quality Threshold:</label>
            <select id="qualityThreshold">
              <option value="low">Low (All Data)</option>
              <option value="medium" selected>Medium (Filtered)</option>
              <option value="high">High (Curated Only)</option>
            </select>
          </div>
        </div>

        <button onclick="analyzeDatasets()" class="primary">üìä Analyze Available Data</button>
        <div id="datasetAnalysis"></div>
      </div>
    </div>

    <div class="step">
      <h3>üé≠ Synthetic Data Generation: Scaling Beyond Reality</h3>
      <p>Real robot data is expensive and time-consuming to collect. <strong>Synthetic data generation</strong> enables scaling training datasets by orders of magnitude while maintaining diversity and quality control.</p>

      <div class="math-formula">
        <strong>Synthetic Data Generation Pipeline:</strong><br><br>
        <strong>1. Physics Simulation:</strong><br>
        MuJoCo/Isaac Gym ‚Üí High-fidelity robot dynamics + realistic scenes<br><br>
        <strong>2. Domain Randomization:</strong><br>
        P(success | Œ∏<sub>real</sub>) ‚âà ‚à´ P(success | Œ∏<sub>sim</sub>, Œµ) P(Œµ) dŒµ<br>
        Œµ ~ {lighting, textures, physics params, camera angles}<br><br>
        <strong>3. Task Distribution:</strong><br>
        T<sub>synthetic</sub> = {pick_place, assembly, cooking, ...} √ó 10<sup>6</sup> variations<br><br>
        <strong>4. Quality Control:</strong><br>
        Filter(demonstrations) ‚Üí Success rate &gt; 90% ‚Üí Human verification sample
      </div>

      <div class="synthetic-demo">
        <div class="demo-title">üé≠ Synthetic Data Generator</div>
        <div class="robot-icon">ü§ñ</div>
        <div class="controls">
          <div class="control-group">
            <label>Simulation Environment:</label>
            <select id="simEnvironment">
              <option value="mujoco" selected>MuJoCo (Physics)</option>
              <option value="isaac">Isaac Gym (GPU Parallel)</option>
              <option value="pybullet">PyBullet (Open Source)</option>
              <option value="unreal">Unreal Engine (Photorealistic)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Task Complexity:</label>
            <select id="taskComplexity">
              <option value="basic" selected>Basic (Pick & Place)</option>
              <option value="medium">Medium (Assembly Tasks)</option>
              <option value="complex">Complex (Cooking Recipes)</option>
              <option value="extreme">Extreme (Multi-Robot)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Domain Randomization:</label>
            <select id="domainRandom">
              <option value="minimal">Minimal (Controlled)</option>
              <option value="moderate" selected>Moderate (Realistic)</option>
              <option value="aggressive">Aggressive (Maximum Diversity)</option>
            </select>
          </div>
        </div>
        <button onclick="generateSyntheticData()" class="primary">üé¨ Generate Training Data</button>
        <div id="syntheticResults"></div>
      </div>
    </div>

    <div class="step">
      <h3>üîç Data Quality Control & Curation</h3>
      <div class="warning">
        <strong>‚ö†Ô∏è The Data Quality Problem:</strong><br>
        Raw robot demonstrations often contain failures, suboptimal behavior, and inconsistent labeling. <strong>Data curation</strong> is critical for VLA performance - garbage in, garbage out applies especially to robotics where failure modes can be dangerous.<br><br>
        <strong>Common Issues:</strong><br>
        ‚Ä¢ Failed demonstrations (20-40% of collected data)<br>
        ‚Ä¢ Inconsistent action labels or coordinate frames<br>
        ‚Ä¢ Poor camera angles or lighting conditions<br>
        ‚Ä¢ Annotation errors in task descriptions<br>
        ‚Ä¢ Drift in robot calibration across sessions
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üîç Data Quality Analyzer</div>
        <div class="controls">
          <div class="control-group">
            <label>Quality Metric:</label>
            <select id="qualityMetric">
              <option value="success" selected>Task Success Rate</option>
              <option value="smoothness">Trajectory Smoothness</option>
              <option value="efficiency">Action Efficiency</option>
              <option value="safety">Safety Compliance</option>
            </select>
          </div>
          <div class="control-group">
            <label>Robot Platform:</label>
            <select id="qualityRobot">
              <option value="franka" selected>Franka Panda</option>
              <option value="ur5">Universal Robot UR5</option>
              <option value="aloha">ALOHA Bimanual</option>
              <option value="mobile">Mobile Manipulator</option>
            </select>
          </div>
        </div>
        <button onclick="analyzeDataQuality()" class="primary">üîç Analyze Data Quality</button>
        <div id="qualityAnalysisResults"></div>
      </div>

      <div class="code-block">
        <div class="code-header">üõ†Ô∏è Data Quality Control Pipeline</div>
        <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import numpy as np
import torch
from scipy import signal
from sklearn.metrics import pairwise_distances

class RobotDataQualityController:
    """
    Comprehensive data quality control for robot training datasets
    Filters out failed demonstrations and ensures high-quality training data
    """
    
    def __init__(self, success_threshold=0.9, smoothness_threshold=0.1):
        self.success_threshold = success_threshold
        self.smoothness_threshold = smoothness_threshold
        
    def check_task_success(self, demonstration):
        """
        Determine if a robot demonstration successfully completed the task
        """
        trajectory = demonstration['actions']
        final_state = demonstration['final_observation']
        task_goal = demonstration['task_description']
        
        # Check trajectory completion
        trajectory_complete = len(trajectory) > 10  # Minimum meaningful length
        
        # Check if gripper properly grasped object (for manipulation tasks)
        if 'gripper_position' in demonstration:
            gripper_positions = demonstration['gripper_position']
            grasp_detected = np.any(gripper_positions < 0.02)  # Gripper closed
        else:
            grasp_detected = True  # Skip if no gripper data
        
        # Check end-effector reached target (if available)
        if 'target_position' in demonstration and 'end_effector_pos' in demonstration:
            final_ee_pos = demonstration['end_effector_pos'][-1]
            target_pos = demonstration['target_position']
            distance_to_target = np.linalg.norm(final_ee_pos - target_pos)
            target_reached = distance_to_target < 0.05  # 5cm threshold
        else:
            target_reached = True  # Skip if no target data
        
        # Combined success criteria
        task_success = trajectory_complete and grasp_detected and target_reached
        return task_success
    
    def calculate_trajectory_smoothness(self, actions):
        """
        Calculate smoothness of robot trajectory using jerk analysis
        Jerky motions indicate poor control or failed demonstrations
        """
        if len(actions) < 3:
            return 0.0
        
        # Calculate acceleration (second derivative of position)
        velocities = np.diff(actions, axis=0)
        accelerations = np.diff(velocities, axis=0)
        jerks = np.diff(accelerations, axis=0)
        
        # RMS jerk as smoothness metric (lower is smoother)
        rms_jerk = np.sqrt(np.mean(jerks**2))
        
        # Convert to smoothness score (0-1, higher is better)
        smoothness = np.exp(-rms_jerk * 10)  # Exponential decay
        return smoothness
    
    def detect_anomalies(self, demonstration):
        """
        Detect various types of anomalies in robot demonstrations
        """
        issues = []
        
        # Check for extreme joint angles
        actions = demonstration['actions']
        if np.any(np.abs(actions) > 3.0):  # Beyond ¬±3 radians
            issues.append("extreme_joint_angles")
        
        # Check for sudden discontinuities
        if len(actions) > 1:
            action_diffs = np.diff(actions, axis=0)
            max_diff = np.max(np.abs(action_diffs))
            if max_diff > 0.5:  # Large sudden movement
                issues.append("sudden_discontinuity")
        
        # Check trajectory length
        if len(actions) < 5:
            issues.append("too_short")
        elif len(actions) > 1000:
            issues.append("too_long")
        
        # Check for stuck robot (no movement)
        action_variance = np.var(actions, axis=0)
        if np.all(action_variance < 1e-6):
            issues.append("no_movement")
        
        return issues
    
    def filter_demonstrations(self, dataset, verbose=True):
        """
        Apply comprehensive quality filtering to robot dataset
        """
        filtered_data = []
        quality_stats = {
            'total': len(dataset),
            'task_failures': 0,
            'poor_smoothness': 0,
            'anomalies': 0,
            'passed': 0
        }
        
        for i, demo in enumerate(dataset):
            # Check task success
            task_success = self.check_task_success(demo)
            if not task_success:
                quality_stats['task_failures'] += 1
                continue
            
            # Check trajectory smoothness
            smoothness = self.calculate_trajectory_smoothness(demo['actions'])
            if smoothness < self.smoothness_threshold:
                quality_stats['poor_smoothness'] += 1
                continue
            
            # Check for anomalies
            anomalies = self.detect_anomalies(demo)
            if anomalies:
                quality_stats['anomalies'] += 1
                if verbose and i < 5:  # Show first few anomalies
                    print(f"Demo {i} anomalies: {anomalies}")
                continue
            
            # Demonstration passed all checks
            demo['quality_score'] = smoothness
            filtered_data.append(demo)
            quality_stats['passed'] += 1
        
        if verbose:
            print("\nüìä Data Quality Analysis:")
            print(f"Total demonstrations: {quality_stats['total']:,}")
            print(f"Task failures: {quality_stats['task_failures']:,} ({100*quality_stats['task_failures']/quality_stats['total']:.1f}%)")
            print(f"Poor smoothness: {quality_stats['poor_smoothness']:,} ({100*quality_stats['poor_smoothness']/quality_stats['total']:.1f}%)")
            print(f"Anomalies detected: {quality_stats['anomalies']:,} ({100*quality_stats['anomalies']/quality_stats['total']:.1f}%)")
            print(f"‚úÖ Passed quality control: {quality_stats['passed']:,} ({100*quality_stats['passed']/quality_stats['total']:.1f}%)")
        
        return filtered_data, quality_stats

# Example usage with simulated data
def demonstrate_quality_control():
    """Demonstrate the data quality control pipeline"""
    
    # Simulate a robot dataset with various quality issues
    np.random.seed(42)
    simulated_dataset = []
    
    for i in range(1000):
        # Generate random demonstration
        trajectory_length = np.random.randint(20, 200)
        
        # Simulate different quality levels
        if i < 700:  # 70% good data
            actions = np.cumsum(np.random.normal(0, 0.02, (trajectory_length, 7)), axis=0)
            success = True
        elif i < 850:  # 15% task failures
            actions = np.random.normal(0, 0.1, (trajectory_length, 7))  # Random actions
            success = False
        elif i < 950:  # 10% jerky motions
            actions = np.random.normal(0, 0.3, (trajectory_length, 7))  # High noise
            success = True
        else:  # 5% extreme anomalies
            actions = np.random.uniform(-5, 5, (trajectory_length, 7))  # Extreme values
            success = True
        
        demo = {
            'actions': actions,
            'task_description': f'Task {i}',
            'success': success,
            'gripper_position': np.random.uniform(0, 0.08, trajectory_length),
            'end_effector_pos': np.cumsum(np.random.normal(0, 0.01, (trajectory_length, 3)), axis=0),
            'target_position': np.array([0.5, 0.2, 0.3])
        }
        simulated_dataset.append(demo)
    
    # Apply quality control
    quality_controller = RobotDataQualityController()
    filtered_dataset, stats = quality_controller.filter_demonstrations(simulated_dataset)
    
    return filtered_dataset, stats

# Run demonstration
filtered_data, quality_stats = demonstrate_quality_control()

print(f"\nüéØ Quality Control Results:")
print(f"Original dataset: {quality_stats['total']:,} demonstrations")
print(f"High-quality dataset: {len(filtered_data):,} demonstrations")
print(f"Data retention rate: {len(filtered_data)/quality_stats['total']:.1%}")
print("\n‚úÖ Ready for VLA training with curated, high-quality demonstrations!")</pre>
      </div>

      <div class="success">
        <strong>‚úÖ Quality Control Benefits:</strong><br>
        ‚Ä¢ <strong>Higher success rates:</strong> Filtered data improves model performance by 15-25%<br>
        ‚Ä¢ <strong>Faster convergence:</strong> Clean data reduces training time by 30-50%<br>
        ‚Ä¢ <strong>Better generalization:</strong> Diverse, high-quality demos improve transfer learning<br>
        ‚Ä¢ <strong>Safety assurance:</strong> Removes dangerous or erratic behavior patterns<br><br>
        <strong>üéØ Industry Standard:</strong> Production VLA training always includes rigorous data curation
      </div>
    </div>

    <div class="step">
      <h3>üìà Data Collection Strategies</h3>
      <div class="dataset-grid">
        <div class="dataset-card" onclick="selectDataStrategy('teleoperation', this)">
          <div class="dataset-name">üéÆ Human Teleoperation</div>
          <div class="dataset-specs">
            <strong>Method:</strong> Remote control by human operators<br>
            <strong>Quality:</strong> High (human expertise)<br>
            <strong>Cost:</strong> $50-100/hour per demonstration<br>
            <strong>Scale:</strong> Limited (1K-10K demos/month)<br>
            <strong>Diversity:</strong> High task coverage
          </div>
          <div class="dataset-stats">
            <div class="stat"><span class="stat-icon">üéØ</span><span>Success Rate: 85-95%</span></div>
            <div class="stat"><span class="stat-icon">‚è±Ô∏è</span><span>Collection Speed: 10-50 demos/day</span></div>
            <div class="stat"><span class="stat-icon">üí∞</span><span>Cost per Demo: $50-200</span></div>
          </div>
        </div>

        <div class="dataset-card" onclick="selectDataStrategy('autonomous', this)">
          <div class="dataset-name">ü§ñ Autonomous Collection</div>
          <div class="dataset-specs">
            <strong>Method:</strong> Robot explores and learns autonomously<br>
            <strong>Quality:</strong> Variable (70-90% success)<br>
            <strong>Cost:</strong> $0.10-1/hour (compute + robot time)<br>
            <strong>Scale:</strong> Massive (100K+ demos/month)<br>
            <strong>Diversity:</strong> Requires careful task distribution
          </div>
          <div class="dataset-stats">
            <div class="stat"><span class="stat-icon">üéØ</span><span>Success Rate: 70-90%</span></div>
            <div class="stat"><span class="stat-icon">‚ö°</span><span>Collection Speed: 1K+ demos/day</span></div>
            <div class="stat"><span class="stat-icon">üí∞</span><span>Cost per Demo: $0.10-1</span></div>
          </div>
        </div>

        <div class="dataset-card" onclick="selectDataStrategy('simulation', this)">
          <div class="dataset-name">üé≠ Simulation + Domain Transfer</div>
          <div class="dataset-specs">
            <strong>Method:</strong> Train in simulation, transfer to real<br>
            <strong>Quality:</strong> High in simulation, requires validation<br>
            <strong>Cost:</strong> $0.01-0.10/demo (pure compute)<br>
            <strong>Scale:</strong> Unlimited (millions of demos)<br>
            <strong>Diversity:</strong> Perfect control over task distribution
          </div>
          <div class="dataset-stats">
            <div class="stat"><span class="stat-icon">üéØ</span><span>Sim Success: 95-99%</span></div>
            <div class="stat"><span class="stat-icon">üîÑ</span><span>Real Transfer: 60-85%</span></div>
            <div class="stat"><span class="stat-icon">üí∞</span><span>Cost per Demo: $0.01-0.10</span></div>
          </div>
        </div>
      </div>
      <div id="dataStrategyAnalysis"></div>
    </div>
  </div>

  <div class="container">
    <h2>üß† Section 2: VLA Model Architectures & Training</h2>

    <div class="step">
      <h3>üèóÔ∏è Model Architecture Choices</h3>
      <p>Successful VLA training requires careful architecture selection based on target deployment, available compute, and performance requirements. Each architecture represents different trade-offs between capability, efficiency, and training cost.</p>

      <div class="model-comparison">
        <div class="model-card" onclick="selectVLAArchitecture('openvla', this)">
          <div class="model-name">ü¶ô OpenVLA-Style (7B)</div>
          <div class="model-specs">
            <strong>Backbone:</strong> LLaMA-7B + DINOv2/SigLIP vision<br>
            <strong>Action Head:</strong> Vector Quantization (8K vocab)<br>
            <strong>Training Cost:</strong> $100K-500K<br>
            <strong>Inference:</strong> A100 (30-50ms latency)<br>
            <strong>Performance:</strong> Competitive cross-embodiment
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">üéØ</span><span>Strong instruction following</span></div>
            <div class="capability"><span class="capability-icon">üîÑ</span><span>Cross-embodiment learning</span></div>
            <div class="capability"><span class="capability-icon">üí∞</span><span>Moderate training cost</span></div>
            <div class="capability"><span class="capability-icon">üöÄ</span><span>Research friendly</span></div>
          </div>
        </div>

        <div class="model-card" onclick="selectVLAArchitecture('smolvla', this)">
          <div class="model-name">‚ö° SmolVLA-Style (450M-600M)</div>
          <div class="model-specs">
            <strong>Backbone:</strong> Qwen-0.5B + MobileViT vision<br>
            <strong>Action Head:</strong> FAST DCT tokenization<br>
            <strong>Training Cost:</strong> $10K-50K<br>
            <strong>Inference:</strong> RTX 4090 / Jetson Orin<br>
            <strong>Performance:</strong> Efficient for specific domains
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">‚ö°</span><span>Fast inference (10-20ms)</span></div>
            <div class="capability"><span class="capability-icon">üí∞</span><span>Low training cost</span></div>
            <div class="capability"><span class="capability-icon">üì±</span><span>Edge deployment ready</span></div>
            <div class="capability"><span class="capability-icon">üéØ</span><span>Good for specific robots</span></div>
          </div>
        </div>

        <div class="model-card" onclick="selectVLAArchitecture('groot', this)">
          <div class="model-name">üöÄ GR00T-Style (20B+)</div>
          <div class="model-specs">
            <strong>Backbone:</strong> Large multimodal transformer<br>
            <strong>Action Head:</strong> Flow matching + diffusion<br>
            <strong>Training Cost:</strong> $1M-5M<br>
            <strong>Inference:</strong> H100 cluster / Jetson Thor<br>
            <strong>Performance:</strong> State-of-the-art humanoid control
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">üèÜ</span><span>Cutting-edge performance</span></div>
            <div class="capability"><span class="capability-icon">ü§ñ</span><span>Humanoid specialization</span></div>
            <div class="capability"><span class="capability-icon">üî¨</span><span>Research frontier</span></div>
            <div class="capability"><span class="capability-icon">üí∏</span><span>High resource requirements</span></div>
          </div>
        </div>
      </div>
      <div id="architectureAnalysis"></div>
    </div>

    <div class="step">
      <h3>üéì Training Pipeline Deep Dive</h3>
      <div class="training-pipeline">
        <div class="pipeline-stage">
          <div class="stage-number">1</div>
          <div class="pipeline-content">
            <h4>üîß Infrastructure Setup</h4>
            <p>Multi-GPU training environment, distributed data loading, mixed precision optimization</p>
          </div>
        </div>
        <div class="pipeline-stage">
          <div class="stage-number">2</div>
          <div class="pipeline-content">
            <h4>üìö Data Preprocessing</h4>
            <p>Vision normalization, action tokenization, sequence padding, cross-embodiment alignment</p>
          </div>
        </div>
        <div class="pipeline-stage">
          <div class="stage-number">3</div>
          <div class="pipeline-content">
            <h4>üß† Model Initialization</h4>
            <p>Pre-trained backbone loading, vision encoder fusion, action head initialization</p>
          </div>
        </div>
        <div class="pipeline-stage">
          <div class="stage-number">4</div>
          <div class="pipeline-content">
            <h4>üéØ Training Loop</h4>
            <p>Gradient accumulation, learning rate scheduling, checkpoint saving, validation monitoring</p>
          </div>
        </div>
        <div class="pipeline-stage">
          <div class="stage-number">5</div>
          <div class="pipeline-content">
            <h4>üìä Evaluation & Validation</h4>
            <p>Cross-embodiment testing, real robot validation, safety verification</p>
          </div>
        </div>
      </div>

      <div class="code-block">
        <div class="code-header">üöÄ OpenVLA Training Pipeline Implementation</div>
        <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from transformers import LlamaForCausalLM, AutoImageProcessor
import wandb
import json
from pathlib import Path

class VLATrainingPipeline:
    """
    Complete training pipeline for Vision-Language-Action models
    Supports multi-GPU training, mixed precision, and cross-embodiment learning
    """
    
    def __init__(self, config):
        self.config = config
        self.setup_distributed()
        self.setup_model()
        self.setup_data()
        self.setup_training()
        
    def setup_distributed(self):
        """Initialize distributed training if available"""
        if torch.cuda.device_count() > 1:
            dist.init_process_group(backend='nccl')
            self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
            torch.cuda.set_device(self.local_rank)
        else:
            self.local_rank = 0
        
        print(f"üöÄ Training on {torch.cuda.device_count()} GPUs")
    
    def setup_model(self):
        """Initialize VLA model with pre-trained components"""
        
        # Load pre-trained language model backbone
        self.model = VLAModel(
            llm_name=self.config.llm_backbone,
            vision_encoders=self.config.vision_encoders,
            action_vocab_size=self.config.action_vocab_size,
            max_sequence_length=self.config.max_seq_len
        )
        
        # Move to GPU and wrap with DDP if distributed
        self.model = self.model.cuda(self.local_rank)
        if torch.cuda.device_count() > 1:
            self.model = DDP(self.model, device_ids=[self.local_rank])
        
        # Enable gradient checkpointing for memory efficiency
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
        
        print(f"üìä Model Parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"üéØ Trainable Parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")
    
    def setup_data(self):
        """Setup data loaders with multi-embodiment support"""
        
        # Load and filter datasets
        datasets = {}
        for dataset_name in self.config.datasets:
            dataset = load_robot_dataset(dataset_name)
            
            # Apply quality control
            quality_controller = RobotDataQualityController()
            filtered_data, stats = quality_controller.filter_demonstrations(
                dataset, verbose=(self.local_rank == 0)
            )
            datasets[dataset_name] = filtered_data
            
            if self.local_rank == 0:
                print(f"üìä {dataset_name}: {len(filtered_data):,} high-quality demos")
        
        # Combine datasets with proper weighting
        self.train_dataset = MultiRobotDataset(
            datasets=datasets,
            tokenizer=self.model.tokenizer,
            action_tokenizer=self.model.action_tokenizer,
            max_sequence_length=self.config.max_seq_len,
            data_weights=self.config.dataset_weights
        )
        
        # Create distributed data loader
        train_sampler = torch.utils.data.distributed.DistributedSampler(
            self.train_dataset, num_replicas=torch.cuda.device_count(), rank=self.local_rank
        ) if torch.cuda.device_count() > 1 else None
        
        self.train_loader = torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.config.batch_size,
            sampler=train_sampler,
            num_workers=self.config.num_workers,
            pin_memory=True,
            drop_last=True
        )
        
        print(f"üéØ Training Dataset Size: {len(self.train_dataset):,} examples")
    
    def setup_training(self):
        """Setup optimizer, scheduler, and training utilities"""
        
        # Optimizer with different learning rates for different components
        param_groups = [
            {
                'params': [p for n, p in self.model.named_parameters() 
                          if 'vision_encoder' in n and p.requires_grad],
                'lr': self.config.vision_lr,
                'name': 'vision_encoder'
            },
            {
                'params': [p for n, p in self.model.named_parameters() 
                          if 'llm' in n and p.requires_grad],
                'lr': self.config.llm_lr,
                'name': 'llm_backbone'
            },
            {
                'params': [p for n, p in self.model.named_parameters() 
                          if 'action' in n and p.requires_grad],
                'lr': self.config.action_lr,
                'name': 'action_head'
            }
        ]
        
        self.optimizer = torch.optim.AdamW(
            param_groups,
            weight_decay=self.config.weight_decay,
            betas=(0.9, 0.95)
        )
        
        # Learning rate scheduler
        total_steps = len(self.train_loader) * self.config.num_epochs
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=total_steps, eta_min=1e-6
        )
        
        # Mixed precision training
        self.scaler = torch.cuda.amp.GradScaler() if self.config.mixed_precision else None
        
        # Initialize logging
        if self.local_rank == 0:
            wandb.init(project="vla-training", config=self.config.__dict__)
    
    def train_epoch(self, epoch):
        """Train for one epoch"""
        self.model.train()
        total_loss = 0
        total_samples = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # Move batch to GPU
            batch = {k: v.cuda(self.local_rank) if torch.is_tensor(v) else v 
                    for k, v in batch.items()}
            
            # Forward pass with mixed precision
            with torch.cuda.amp.autocast(enabled=self.config.mixed_precision):
                outputs = self.model(
                    images=batch['images'],
                    text_input=batch['text'],
                    action_sequences=batch['actions'],
                    robot_type=batch['robot_type']
                )
                
                # Calculate loss (next-token prediction)
                loss = outputs.loss
                
                # Add auxiliary losses if configured
                if hasattr(outputs, 'vq_loss'):
                    loss += 0.25 * outputs.vq_loss  # VQ commitment loss
                
                # Scale loss for gradient accumulation
                loss = loss / self.config.gradient_accumulation_steps
            
            # Backward pass
            if self.scaler:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # Gradient accumulation
            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                # Gradient clipping
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # Optimizer step
                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                self.scheduler.step()
                self.optimizer.zero_grad()
            
            # Logging
            total_loss += loss.item() * self.config.gradient_accumulation_steps
            total_samples += batch['images'].size(0)
            
            # Log progress
            if batch_idx % 100 == 0 and self.local_rank == 0:
                current_lr = self.scheduler.get_last_lr()[0]
                print(f"Epoch {epoch}, Batch {batch_idx}, "
                      f"Loss: {loss.item():.4f}, LR: {current_lr:.2e}")
                
                wandb.log({
                    'train/loss': loss.item(),
                    'train/learning_rate': current_lr,
                    'train/epoch': epoch
                })
        
        avg_loss = total_loss / len(self.train_loader)
        return avg_loss
    
    def validate_model(self, validation_tasks):
        """Validate model on cross-embodiment tasks"""
        self.model.eval()
        validation_results = {}
        
        with torch.no_grad():
            for task_name, task_data in validation_tasks.items():
                task_success = 0
                task_samples = 0
                
                for batch in task_data:
                    batch = {k: v.cuda(self.local_rank) if torch.is_tensor(v) else v 
                            for k, v in batch.items()}
                    
                    # Generate actions for validation
                    generated_actions = self.model.generate_actions(
                        images=batch['images'],
                        instructions=batch['instructions'],
                        robot_type=batch['robot_type'],
                        max_actions=100
                    )
                    
                    # Compare with ground truth (simplified)
                    gt_actions = batch['ground_truth_actions']
                    action_similarity = self.calculate_action_similarity(
                        generated_actions, gt_actions
                    )
                    
                    task_success += (action_similarity > 0.8).sum().item()
                    task_samples += len(generated_actions)
                
                validation_results[task_name] = task_success / task_samples
        
        return validation_results
    
    def train(self):
        """Main training loop"""
        best_validation_score = 0
        
        for epoch in range(self.config.num_epochs):
            # Train epoch
            train_loss = self.train_epoch(epoch)
            
            # Validation every N epochs
            if epoch % self.config.validation_interval == 0:
                validation_results = self.validate_model(self.validation_tasks)
                avg_validation_score = np.mean(list(validation_results.values()))
                
                if self.local_rank == 0:
                    print(f"\nüìä Epoch {epoch} Results:")
                    print(f"Training Loss: {train_loss:.4f}")
                    print(f"Validation Score: {avg_validation_score:.3f}")
                    
                    # Log to wandb
                    wandb.log({
                        'val/average_score': avg_validation_score,
                        'train/epoch_loss': train_loss,
                        **{f'val/{k}': v for k, v in validation_results.items()}
                    })
                    
                    # Save best model
                    if avg_validation_score > best_validation_score:
                        best_validation_score = avg_validation_score
                        self.save_checkpoint(epoch, is_best=True)
            
            # Save regular checkpoint
            if epoch % self.config.save_interval == 0 and self.local_rank == 0:
                self.save_checkpoint(epoch)
        
        print(f"üéâ Training completed! Best validation score: {best_validation_score:.3f}")
    
    def save_checkpoint(self, epoch, is_best=False):
        """Save model checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': self.config.__dict__
        }
        
        checkpoint_path = Path(self.config.checkpoint_dir) / f"vla_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            best_path = Path(self.config.checkpoint_dir) / "vla_best.pth"
            torch.save(checkpoint, best_path)
            print(f"üíæ Saved best model at epoch {epoch}")

# Example training configuration
class TrainingConfig:
    """Training configuration for OpenVLA-style model"""
    
    # Model architecture
    llm_backbone = "meta-llama/Llama-2-7b-hf"
    vision_encoders = ["facebook/dinov2-base", "google/siglip-base-patch16-224"]
    action_vocab_size = 8192
    max_seq_len = 2048
    
    # Training data
    datasets = ["open_x_embodiment", "bridge_data", "aloha_mobile"]
    dataset_weights = [0.6, 0.3, 0.1]  # Weight different datasets
    
    # Training hyperparameters
    num_epochs = 50
    batch_size = 8  # Per GPU
    gradient_accumulation_steps = 16  # Effective batch size = 8 * 16 * num_gpus
    
    # Learning rates (different for different components)
    vision_lr = 1e-5   # Lower for pre-trained vision
    llm_lr = 1e-5      # Lower for pre-trained LLM
    action_lr = 1e-4   # Higher for new action head
    weight_decay = 0.05
    
    # Optimization
    mixed_precision = True
    gradient_checkpointing = True
    
    # Logging and saving
    validation_interval = 5
    save_interval = 10
    checkpoint_dir = "./checkpoints"

# Example usage
if __name__ == "__main__":
    config = TrainingConfig()
    trainer = VLATrainingPipeline(config)
    
    print("üéØ Starting VLA training...")
    print(f"üìä Configuration:")
    print(f"  ‚Ä¢ Model: {config.llm_backbone}")
    print(f"  ‚Ä¢ Datasets: {', '.join(config.datasets)}")
    print(f"  ‚Ä¢ Effective batch size: {config.batch_size * config.gradient_accumulation_steps * torch.cuda.device_count()}")
    print(f"  ‚Ä¢ Training epochs: {config.num_epochs}")
    
    # Start training
    trainer.train()
    print("‚úÖ VLA training completed successfully!")</pre>
      </div>
    </div>

    <div class="step">
      <h3>‚öñÔ∏è Training Strategy Comparison</h3>
      <div class="tabs">
        <div class="tab active" onclick="switchTrainingTab('joint', this)">üîÑ Joint Training</div>
        <div class="tab" onclick="switchTrainingTab('staged', this)">üìà Staged Training</div>
        <div class="tab" onclick="switchTrainingTab('adapter', this)">üîß Adapter Training</div>
        <div class="tab" onclick="switchTrainingTab('continual', this)">‚ôªÔ∏è Continual Learning</div>
      </div>

      <div id="joint" class="tab-content active">
        <div class="success">
          <strong>üîÑ Joint Training Approach:</strong><br>
          Train vision, language, and action components simultaneously from scratch or fine-tune all components together.
        </div>

        <div class="math-formula">
          <strong>Joint Training Mathematics:</strong><br><br>
          <strong>Multi-Modal Loss Function:</strong><br>
          ‚Ñí<sub>joint</sub> = ‚Ñí<sub>language</sub> + Œª<sub>vision</sub> √ó ‚Ñí<sub>vision</sub> + Œª<sub>action</sub> √ó ‚Ñí<sub>action</sub><br><br>
          <strong>Where:</strong><br>
          ‚Ñí<sub>language</sub> = CrossEntropy(text_logits, text_targets)<br>
          ‚Ñí<sub>vision</sub> = MSE(vision_features, target_features) &nbsp; [if applicable]<br>
          ‚Ñí<sub>action</sub> = CrossEntropy(action_logits, action_tokens)<br><br>
          <strong>Weight Balancing:</strong> Œª<sub>vision</sub> ‚àà [0.1, 1.0], Œª<sub>action</sub> ‚àà [1.0, 10.0]
        </div>

        <div class="metric-grid">
          <div class="metric-card"><div class="metric-value">Fast</div><div class="metric-label">Convergence Speed</div></div>
          <div class="metric-card"><div class="metric-value">High</div><div class="metric-label">Data Efficiency</div></div>
          <div class="metric-card"><div class="metric-value">Complex</div><div class="metric-label">Tuning Difficulty</div></div>
          <div class="metric-card"><div class="metric-value">$$$</div><div class="metric-label">Compute Cost</div></div>
        </div>

        <div class="info">
          <strong>üéØ Best for:</strong> When you have large, high-quality datasets and want maximum performance<br>
          <strong>‚úÖ Advantages:</strong> Optimal cross-modal alignment, end-to-end optimization<br>
          <strong>‚ö†Ô∏è Challenges:</strong> Requires careful learning rate tuning, higher compute requirements
        </div>
      </div>

      <div id="staged" class="tab-content">
        <div class="info">
          <strong>üìà Staged Training Approach:</strong><br>
          Train components sequentially: vision encoder ‚Üí language alignment ‚Üí action head ‚Üí joint fine-tuning.
        </div>

        <div class="training-pipeline">
          <div class="pipeline-stage">
            <div class="stage-number">1</div>
            <div class="pipeline-content">
              <h4>üëÅÔ∏è Vision Pre-training</h4>
              <p>Train vision encoder on large image datasets (ImageNet, CLIP data) for robust visual representations</p>
            </div>
          </div>
          <div class="pipeline-stage">
            <div class="stage-number">2</div>
            <div class="pipeline-content">
              <h4>üîó Vision-Language Alignment</h4>
              <p>Train vision-text alignment using CLIP-style contrastive learning or VLM datasets</p>
            </div>
          </div>
          <div class="pipeline-stage">
            <div class="stage-number">3</div>
            <div class="pipeline-content">
              <h4>üéØ Action Head Training</h4>
              <p>Train action prediction head on robot demonstrations while freezing other components</p>
            </div>
          </div>
          <div class="pipeline-stage">
            <div class="stage-number">4</div>
            <div class="pipeline-content">
              <h4>üîÑ Joint Fine-tuning</h4>
              <p>End-to-end fine-tuning of all components with reduced learning rates for stability</p>
            </div>
          </div>
        </div>

        <div class="metric-grid">
          <div class="metric-card"><div class="metric-value">Stable</div><div class="metric-label">Training Stability</div></div>
          <div class="metric-card"><div class="metric-value">Medium</div><div class="metric-label">Data Requirements</div></div>
          <div class="metric-card"><div class="metric-value">Easy</div><div class="metric-label">Hyperparameter Tuning</div></div>
          <div class="metric-card"><div class="metric-value">$</div><div class="metric-label">Compute Cost</div></div>
        </div>
      </div>

      <div id="adapter" class="tab-content">
        <div class="warning">
          <strong>üîß Adapter Training Approach:</strong><br>
          Freeze pre-trained components, train only lightweight adapter layers for robot-specific control.
        </div>

        <div class="code-block">
          <div class="code-header">üîß Efficient Adapter Training</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>class EfficientVLAAdapter(nn.Module):
    """
    Lightweight adapter for robot-specific VLA training
    Enables training new robot capabilities with minimal compute
    """
    def __init__(self, base_vla_model, robot_config, adapter_rank=64):
        super().__init__()
        
        # Freeze base VLA model
        self.base_vla = base_vla_model
        for param in self.base_vla.parameters():
            param.requires_grad = False
        
        # Robot-specific adapter layers
        hidden_dim = self.base_vla.config.hidden_size
        self.robot_adapter = nn.ModuleDict({
            # Low-rank adaptation for action prediction
            'action_lora_A': nn.Linear(hidden_dim, adapter_rank, bias=False),
            'action_lora_B': nn.Linear(adapter_rank, robot_config.action_dim, bias=False),
            
            # Robot-specific normalization
            'action_norm': nn.LayerNorm(robot_config.action_dim),
            
            # Optional: robot-specific vision adaptation
            'vision_adapter': nn.Linear(hidden_dim, hidden_dim, bias=False) if robot_config.vision_adapter else None
        })
        
        # Initialize LoRA with small weights
        nn.init.normal_(self.robot_adapter.action_lora_A.weight, std=0.02)
        nn.init.zeros_(self.robot_adapter.action_lora_B.weight)
    
    def forward(self, images, instructions, robot_type):
        # Get base model representations
        base_outputs = self.base_vla(images, instructions, robot_type)
        hidden_states = base_outputs.last_hidden_state
        
        # Apply robot-specific adaptation
        if self.robot_adapter.vision_adapter:
            adapted_hidden = hidden_states + self.robot_adapter.vision_adapter(hidden_states)
        else:
            adapted_hidden = hidden_states
        
        # Low-rank action prediction
        action_features = self.robot_adapter.action_lora_A(adapted_hidden)
        robot_actions = self.robot_adapter.action_lora_B(action_features)
        robot_actions = self.robot_adapter.action_norm(robot_actions)
        
        return robot_actions

# Training efficiency comparison
def compare_training_efficiency():
    """Compare different training approaches"""
    
    approaches = {
        'full_finetuning': {
            'trainable_params': 7_000_000_000,  # 7B parameters
            'training_time': '7 days',
            'gpu_hours': 1000,
            'cost': '$5000',
            'data_required': '500K demos'
        },
        'adapter_training': {
            'trainable_params': 50_000_000,    # 50M adapter parameters
            'training_time': '6 hours', 
            'gpu_hours': 50,
            'cost': '$250',
            'data_required': '10K demos'
        },
        'lora_finetuning': {
            'trainable_params': 100_000_000,   # 100M LoRA parameters
            'training_time': '12 hours',
            'gpu_hours': 100,
            'cost': '$500',
            'data_required': '50K demos'
        }
    }
    
    for approach, stats in approaches.items():
        efficiency_ratio = 7_000_000_000 / stats['trainable_params']
        print(f"\n{approach.upper()}:")
        print(f"  Trainable params: {stats['trainable_params']:,}")
        print(f"  Efficiency gain: {efficiency_ratio:.1f}x fewer parameters")
        print(f"  Training time: {stats['training_time']}")
        print(f"  Estimated cost: {stats['cost']}")

compare_training_efficiency()</pre>
        </div>

        <div class="metric-grid">
          <div class="metric-card"><div class="metric-value">Lightning</div><div class="metric-label">Training Speed</div></div>
          <div class="metric-card"><div class="metric-value">Low</div><div class="metric-label">Data Requirements</div></div>
          <div class="metric-card"><div class="metric-value">Simple</div><div class="metric-label">Setup Complexity</div></div>
          <div class="metric-card"><div class="metric-value">$</div><div class="metric-label">Compute Cost</div></div>
        </div>
      </div>

      <div id="continual" class="tab-content">
        <div class="info">
          <strong>‚ôªÔ∏è Continual Learning Approach:</strong><br>
          Continuously update VLA models as new robot data becomes available, preventing catastrophic forgetting while learning new capabilities.
        </div>

        <div class="danger">
          <strong>üß† The Catastrophic Forgetting Problem:</strong><br>
          When training VLA models on new robot data, they often "forget" previously learned skills. This is especially problematic in robotics where safety and reliability are critical.<br><br>
          <strong>Solutions:</strong><br>
          ‚Ä¢ <strong>Elastic Weight Consolidation (EWC):</strong> Protect important parameters<br>
          ‚Ä¢ <strong>Experience Replay:</strong> Mix new data with representative old data<br>
          ‚Ä¢ <strong>Progressive Networks:</strong> Add new capacity for new tasks<br>
          ‚Ä¢ <strong>Meta-Learning:</strong> Learn to learn new tasks quickly
        </div>
      </div>
    </div>

    <div class="step">
      <h3>üéØ Model-Specific Training Recipes</h3>
      <div class="interactive-demo">
        <div class="demo-title">üìã Training Recipe Generator</div>
        <div class="controls">
          <div class="control-group">
            <label>Target Model:</label>
            <select id="targetModel">
              <option value="openvla" selected>OpenVLA (7B)</option>
              <option value="smolvla">SmolVLA (600M)</option>
              <option value="pi0">œÄ0-FAST (3B)</option>
              <option value="groot">GR00T-Style (20B)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Hardware Setup:</label>
            <select id="hardwareSetup">
              <option value="single_gpu" selected>Single A100 (40GB)</option>
              <option value="multi_gpu">4x A100 (160GB total)</option>
              <option value="cluster">8x H100 (640GB total)</option>
              <option value="budget">RTX 4090 (24GB)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Dataset Size:</label>
            <select id="datasetSize">
              <option value="small">Small (10K demos)</option>
              <option value="medium" selected>Medium (100K demos)</option>
              <option value="large">Large (1M demos)</option>
              <option value="massive">Massive (10M demos)</option>
            </select>
          </div>
        </div>
        <button onclick="generateTrainingRecipe()" class="primary">üç≥ Generate Training Recipe</button>
        <div id="trainingRecipeOutput"></div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üéõÔ∏è Section 3: Interactive Training Simulator</h2>

    <div class="step">
      <h3>üßÆ VLA Training Cost Calculator</h3>
      <p>Understanding the true cost of VLA training helps with planning and budgeting. This simulator estimates training costs based on model size, dataset, hardware, and training strategy.</p>

      <div class="training-simulator">
        <div class="demo-title">üéÆ VLA Training Simulator</div>
        <div class="sim-controls">
          <div class="control-group">
            <label>Model Architecture:</label>
            <select id="simModelArch">
              <option value="smolvla">SmolVLA (600M params)</option>
              <option value="openvla" selected>OpenVLA (7B params)</option>
              <option value="groot">GR00T-Style (20B params)</option>
              <option value="custom">Custom Size</option>
            </select>
          </div>
          <div class="control-group">
            <label>Dataset Scale:</label>
            <select id="simDataScale">
              <option value="pilot">Pilot (1K demos)</option>
              <option value="small">Small (10K demos)</option>
              <option value="medium" selected>Medium (100K demos)</option>
              <option value="large">Large (1M demos)</option>
            </select>
          </div>
          <div class="control-group">
            <label>Hardware Platform:</label>
            <select id="simHardware">
              <option value="a100">A100 (40GB)</option>
              <option value="h100" selected>H100 (80GB)</option>
              <option value="v100">V100 (32GB)</option>
              <option value="rtx4090">RTX 4090 (24GB)</option>
            </select>
          </div>
          <div class="control-group" id="customParamsGroup" style="display:none;">
            <label>Custom Model Size (billions):</label>
            <input type="number" id="customParams" min="0.1" max="100" step="0.1" value="7">
          </div>
          <div class="control-group">
            <label>Training Strategy:</label>
            <select id="simStrategy">
              <option value="joint" selected>Joint Training</option>
              <option value="staged">Staged Training</option>
              <option value="adapter">Adapter Fine-tuning</option>
            </select>
          </div>
          <div class="control-group">
            <label>Data Quality Level:</label>
            <select id="simDataQuality">
              <option value="raw">Raw (No filtering)</option>
              <option value="filtered" selected>Filtered (Quality control)</option>
              <option value="curated">Curated (Human verified)</option>
            </select>
          </div>
        </div>
        
        <button onclick="runTrainingSimulation()" class="primary">üöÄ Simulate Training</button>
        
        <div class="sim-output">
          <div id="simulationResults"></div>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üìä Section 4: Evaluation Methodologies</h2>

    <div class="step">
      <h3>üéØ VLA Evaluation Framework</h3>
      <p>Evaluating VLA models requires comprehensive testing across multiple dimensions: task success, cross-embodiment transfer, safety, and real-world robustness. Unlike text generation, robot evaluation has physical consequences.</p>

      <div class="math-formula">
        <strong>VLA Evaluation Metrics:</strong><br><br>
        <strong>1. Task Success Rate:</strong><br>
        SR<sub>task</sub> = (Successful Completions) / (Total Attempts)<br><br>
        <strong>2. Cross-Embodiment Transfer:</strong><br>
        CET = Œ£<sub>i</sub> SR<sub>robot_i</sub> / |Robots|<br><br>
        <strong>3. Instruction Following Accuracy:</strong><br>
        IFA = (Actions Match Intent) / (Total Instructions)<br><br>
        <strong>4. Safety Compliance:</strong><br>
        SC = 1 - (Dangerous Actions) / (Total Actions)<br><br>
        <strong>5. Sim-to-Real Transfer:</strong><br>
        S2R = SR<sub>real</sub> / SR<sub>simulation</sub>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üìä VLA Model Evaluation Dashboard</div>
        <div class="controls">
          <div class="control-group">
            <label>Model Under Test:</label>
            <select id="evalModel">
              <option value="openvla" selected>OpenVLA (7B)</option>
              <option value="smolvla">SmolVLA (600M)</option>
              <option value="rt2">RT-2 Baseline</option>
              <option value="custom">Your Custom Model</option>
            </select>
          </div>
          <div class="control-group">
            <label>Evaluation Domain:</label>
            <select id="evalDomain">
              <option value="manipulation" selected>Object Manipulation</option>
              <option value="navigation">Mobile Navigation</option>
              <option value="assembly">Assembly Tasks</option>
              <option value="kitchen">Kitchen Activities</option>
            </select>
          </div>
          <div class="control-group">
            <label>Robot Platform:</label>
            <select id="evalRobot">
              <option value="franka" selected>Franka Panda</option>
              <option value="ur5">UR5 Industrial</option>
              <option value="aloha">ALOHA Bimanual</option>
              <option value="mobile">Mobile Manipulator</option>
            </select>
          </div>
        </div>
        <button onclick="runEvaluation()" class="primary">üîç Run Evaluation</button>
        <div id="evaluationResults"></div>
      </div>
    </div>

    <div class="step">
      <h3>üîÑ Cross-Embodiment Transfer Evaluation</h3>
      <div class="info">
        <strong>üéØ The Holy Grail:</strong> A VLA model that can control robots it has never seen before demonstrates true understanding of embodied intelligence principles.
      </div>

      <div class="evaluation-grid">
        <div class="eval-metric">
          <div class="eval-score" style="color:#28a745">84%</div>
          <div class="eval-label">Same Robot Type</div>
          <div style="font-size:10px;margin-top:5px">Franka ‚Üí Franka</div>
        </div>
        <div class="eval-metric">
          <div class="eval-score" style="color:#17a2b8">71%</div>
          <div class="eval-label">Similar Morphology</div>
          <div style="font-size:10px;margin-top:5px">Franka ‚Üí UR5</div>
        </div>
        <div class="eval-metric">
          <div class="eval-score" style="color:#ffc107">58%</div>
          <div class="eval-label">Different Category</div>
          <div style="font-size:10px;margin-top:5px">Arm ‚Üí Mobile</div>
        </div>
        <div class="eval-metric">
          <div class="eval-score" style="color:#fd7e14">43%</div>
          <div class="eval-label">Novel Embodiment</div>
          <div style="font-size:10px;margin-top:5px">Arm ‚Üí Humanoid</div>
        </div>
      </div>

      <div class="code-block">
        <div class="code-header">üî¨ Cross-Embodiment Evaluation Framework</div>
        <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>class CrossEmbodimentEvaluator:
    """
    Comprehensive evaluation framework for cross-embodiment transfer
    Tests VLA model generalization across different robot morphologies
    """
    
    def __init__(self, test_tasks, robot_configurations):
        self.test_tasks = test_tasks
        self.robot_configs = robot_configurations
        
    def evaluate_zero_shot_transfer(self, vla_model, source_robot, target_robot):
        """
        Evaluate zero-shot transfer between robot embodiments
        
        Args:
            vla_model: Trained VLA model
            source_robot: Robot type used during training
            target_robot: New robot type for evaluation
        """
        results = {
            'task_success_rates': {},
            'action_similarity_scores': {},
            'safety_violations': 0,
            'total_attempts': 0
        }
        
        for task_name, task_episodes in self.test_tasks.items():
            task_successes = 0
            similarity_scores = []
            
            for episode in task_episodes:
                # Generate actions for target robot
                generated_actions = vla_model.generate_actions(
                    images=episode['observations'],
                    instructions=episode['instruction'],
                    robot_type=target_robot,
                    max_sequence_length=200
                )
                
                # Check if robot can physically execute actions
                action_validity = self.check_action_validity(
                    generated_actions, self.robot_configs[target_robot]
                )
                
                if not action_validity:
                    results['safety_violations'] += 1
                    continue
                
                # Simulate execution (or use real robot)
                task_success = self.simulate_task_execution(
                    generated_actions, episode['goal_state'], target_robot
                )
                
                if task_success:
                    task_successes += 1
                
                # Calculate action similarity to optimal trajectory
                if 'expert_actions' in episode:
                    similarity = self.calculate_action_similarity(
                        generated_actions, episode['expert_actions']
                    )
                    similarity_scores.append(similarity)
                
                results['total_attempts'] += 1
            
            # Store task-specific results
            results['task_success_rates'][task_name] = task_successes / len(task_episodes)
            results['action_similarity_scores'][task_name] = np.mean(similarity_scores)
        
        # Calculate overall transfer metrics
        overall_success_rate = np.mean(list(results['task_success_rates'].values()))
        safety_rate = 1 - (results['safety_violations'] / results['total_attempts'])
        
        return {
            'overall_success_rate': overall_success_rate,
            'safety_rate': safety_rate,
            'task_breakdown': results['task_success_rates'],
            'action_quality': results['action_similarity_scores']
        }
    
    def check_action_validity(self, actions, robot_config):
        """Check if generated actions are physically valid for target robot"""
        
        # Check joint limits
        joint_limits = robot_config['joint_limits']
        for i, (action_seq) in enumerate(actions):
            if np.any(action_seq < joint_limits['lower']) or np.any(action_seq > joint_limits['upper']):
                return False
        
        # Check velocity limits
        if len(actions) > 1:
            velocities = np.diff(actions, axis=0)
            max_velocity = robot_config['max_velocity']
            if np.any(np.abs(velocities) > max_velocity):
                return False
        
        # Check workspace limits
        if 'workspace_bounds' in robot_config:
            # This would require forward kinematics - simplified check
            if np.any(np.abs(actions) > 3.0):  # Conservative joint angle limit
                return False
        
        return True
    
    def calculate_action_similarity(self, generated_actions, expert_actions):
        """Calculate similarity between generated and expert action sequences"""
        
        # Align sequences (handle different lengths)
        min_len = min(len(generated_actions), len(expert_actions))
        gen_aligned = generated_actions[:min_len]
        exp_aligned = expert_actions[:min_len]
        
        # Calculate normalized MSE
        mse = np.mean((gen_aligned - exp_aligned) ** 2)
        
        # Convert to similarity score (0-1, higher is better)
        # Use expert action variance for normalization
        expert_variance = np.var(expert_actions)
        similarity = np.exp(-mse / (expert_variance + 1e-8))
        
        return similarity
    
    def generate_evaluation_report(self, evaluation_results):
        """Generate comprehensive evaluation report"""
        
        report = {
            'summary': {
                'overall_score': evaluation_results['overall_success_rate'],
                'safety_score': evaluation_results['safety_rate'],
                'recommendation': self.get_deployment_recommendation(evaluation_results)
            },
            'detailed_results': evaluation_results,
            'comparison_baseline': {
                'random_policy': 0.05,
                'task_specific_RL': 0.60,
                'previous_vla': 0.73
            }
        }
        
        return report

# Example evaluation run
robot_configs = {
    'franka': {
        'joint_limits': {'lower': np.array([-2.8, -1.7, -2.8, -3.0, -2.8, -0.0, -2.8]),
                        'upper': np.array([2.8, 1.7, 2.8, -0.1, 2.8, 3.7, 2.8])},
        'max_velocity': np.array([2.0, 2.0, 2.0, 2.0, 2.5, 2.5, 2.5]),
        'workspace_bounds': {'x': [0.3, 0.8], 'y': [-0.3, 0.3], 'z': [0.0, 0.8]}
    },
    'ur5': {
        'joint_limits': {'lower': np.array([-6.28, -6.28, -3.14, -6.28, -6.28, -6.28]),
                        'upper': np.array([6.28, 6.28, 3.14, 6.28, 6.28, 6.28])},
        'max_velocity': np.array([3.14, 3.14, 3.14, 6.28, 6.28, 6.28])
    }
}

evaluator = CrossEmbodimentEvaluator(test_tasks=[], robot_configurations=robot_configs)
print("üî¨ Cross-embodiment evaluation framework initialized!")</pre>
      </div>
    </div>

    <div class="step">
      <h3>üìà Real-World Validation Pipeline</h3>
      <div class="pipeline-visualization">
        <div class="pipeline-step active">
          <h4>üß™ Simulation Testing</h4>
          <div style="font-size:12px;margin-top:5px">
            MuJoCo/Isaac Gym validation<br>
            Success rate &gt; 90% required
          </div>
          <div class="pipeline-arrow">‚Üí</div>
        </div>
        <div class="pipeline-step">
          <h4>üîí Safety Verification</h4>
          <div style="font-size:12px;margin-top:5px">
            Workspace boundary checks<br>
            Emergency stop testing
          </div>
          <div class="pipeline-arrow">‚Üí</div>
        </div>
        <div class="pipeline-step">
          <h4>ü§ñ Real Robot Testing</h4>
          <div style="font-size:12px;margin-top:5px">
            Limited real-world trials<br>
            Human supervision required
          </div>
          <div class="pipeline-arrow">‚Üí</div>
        </div>
        <div class="pipeline-step">
          <h4>‚úÖ Production Approval</div>
          <div style="font-size:12px;margin-top:5px">
            Performance benchmarks met<br>
            Safety requirements passed
          </div>
        </div>
      </div>

      <div class="interactive-demo">
        <div class="demo-title">üé≠ Evaluation Simulator</div>
        <div class="controls">
          <div class="control-group">
            <label>Evaluation Phase:</label>
            <select id="evalPhase">
              <option value="simulation" selected>Simulation Testing</option>
              <option value="safety">Safety Verification</option>
              <option value="real_robot">Real Robot Testing</option>
              <option value="production">Production Validation</option>
            </select>
          </div>
          <div class="control-group">
            <label>Task Category:</label>
            <select id="evalTaskCategory">
              <option value="pick_place" selected>Pick & Place</option>
              <option value="assembly">Assembly Tasks</option>
              <option value="pouring">Liquid Pouring</option>
              <option value="cleaning">Surface Cleaning</option>
            </select>
          </div>
        </div>
        <button onclick="simulateEvaluation()" class="primary">üß™ Run Evaluation Phase</button>
        <div id="evaluationPhaseResults"></div>
      </div>
    </div>

    <div class="step">
      <h3>‚ö° Performance Benchmarking</h3>
      <div class="interactive-demo">
        <div class="demo-title">üèÜ VLA Model Comparison Benchmark</div>
        <div class="controls">
          <div class="control-group">
            <label>Benchmark Suite:</label>
            <select id="benchmarkSuite">
              <option value="manipulation" selected>RLBench Manipulation</option>
              <option value="navigation">RoboTHOR Navigation</option>
              <option value="metaworld">Meta-World Multi-Task</option>
              <option value="real_robot">Real Robot Evaluation</option>
            </select>
          </div>
          <div class="control-group">
            <label>Comparison Baseline:</label>
            <select id="comparisonBaseline">
              <option value="random" selected>Random Policy</option>
              <option value="bc">Behavior Cloning</option>
              <option value="rl">Reinforcement Learning</option>
              <option value="rt2">RT-2 (Google)</option>
            </select>
          </div>
        </div>
        <button onclick="runBenchmark()" class="primary">üèÅ Run Benchmark</button>
        <div id="benchmarkResults"></div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üöÄ Section 5: Advanced Training Techniques</h2>

    <div class="step">
      <h3>üî¨ Cutting-Edge Training Innovations</h3>
      <div class="tabs">
        <div class="tab active" onclick="switchAdvancedTab('curriculum', this)">üìö Curriculum Learning</div>
        <div class="tab" onclick="switchAdvancedTab('meta', this)">üß† Meta-Learning</div>
        <div class="tab" onclick="switchAdvancedTab('constitutional', this)">‚öñÔ∏è Constitutional Training</div>
        <div class="tab" onclick="switchAdvancedTab('multimodal', this)">üåê Multi-Modal Extensions</div>
      </div>

      <div id="curriculum" class="tab-content active">
        <div class="success">
          <strong>üìö Curriculum Learning for VLA:</strong><br>
          Start with simple tasks and gradually increase complexity. This approach significantly improves learning efficiency and final performance.
        </div>

        <div class="math-formula">
          <strong>Curriculum Learning Mathematics:</strong><br><br>
          <strong>Task Difficulty Progression:</strong><br>
          D(t) = D<sub>min</sub> + (D<sub>max</sub> - D<sub>min</sub>) √ó œÉ(Œ± √ó (t - t<sub>0</sub>))<br><br>
          <strong>Where:</strong><br>
          ‚Ä¢ œÉ(x) = sigmoid function (smooth transition)<br>
          ‚Ä¢ Œ± = curriculum speed parameter<br>
          ‚Ä¢ t<sub>0</sub> = curriculum start time<br><br>
          <strong>Success-Based Pacing:</strong><br>
          Advance to next difficulty when: SR<sub>current</sub> &gt; Œ∏<sub>mastery</sub>
        </div>

        <div class="code-block">
          <div class="code-header">üìö Curriculum Learning Implementation</div>
          <button class="copy-button" onclick="copyCode(this)">üìã Copy</button>
<pre>class VLACurriculumLearning:
    """
    Curriculum learning for VLA training
    Gradually increases task complexity based on model performance
    """
    
    def __init__(self, task_hierarchy, mastery_threshold=0.8):
        self.task_hierarchy = task_hierarchy
        self.mastery_threshold = mastery_threshold
        self.current_level = 0
        self.level_performance = []
        
    def get_current_tasks(self):
        """Get tasks for current curriculum level"""
        return self.task_hierarchy[self.current_level]
    
    def update_curriculum(self, recent_performance):
        """Update curriculum based on recent model performance"""
        
        # Calculate moving average of performance
        window_size = min(100, len(recent_performance))
        if len(recent_performance) >= window_size:
            avg_performance = np.mean(recent_performance[-window_size:])
            
            # Advance curriculum if mastery achieved
            if avg_performance > self.mastery_threshold:
                if self.current_level < len(self.task_hierarchy) - 1:
                    self.current_level += 1
                    print(f"üìà Advancing to curriculum level {self.current_level}")
                    print(f"   Tasks: {[task['name'] for task in self.get_current_tasks()]}")
                    
                    # Reset performance tracking for new level
                    recent_performance.clear()
            
            # Store level performance
            self.level_performance.append({
                'level': self.current_level,
                'performance': avg_performance,
                'tasks': len(self.get_current_tasks())
            })
        
        return self.current_level
    
    def get_curriculum_progress(self):
        """Get detailed curriculum progress information"""
        return {
            'current_level': self.current_level,
            'total_levels': len(self.task_hierarchy),
            'progress_percentage': (self.current_level / len(self.task_hierarchy)) * 100,
            'level_history': self.level_performance
        }

# Define task hierarchy from simple to complex
robot_task_hierarchy = [
    # Level 0: Basic motor control
    [
        {'name': 'joint_control', 'description': 'Move individual joints', 'complexity': 0.1},
        {'name': 'reach_target', 'description': 'Reach 3D positions', 'complexity': 0.2}
    ],
    
    # Level 1: Simple manipulation
    [
        {'name': 'pick_cube', 'description': 'Pick up cube objects', 'complexity': 0.4},
        {'name': 'place_target', 'description': 'Place objects at targets', 'complexity': 0.5}
    ],
    
    # Level 2: Complex manipulation  
    [
        {'name': 'stack_blocks', 'description': 'Stack multiple objects', 'complexity': 0.7},
        {'name': 'pour_liquid', 'description': 'Pour liquids accurately', 'complexity': 0.8}
    ],
    
    # Level 3: Multi-step tasks
    [
        {'name': 'cooking_prep', 'description': 'Prepare ingredients', 'complexity': 0.9},
        {'name': 'assembly_task', 'description': 'Assemble complex objects', 'complexity': 1.0}
    ]
]

# Initialize curriculum learning
curriculum = VLACurriculumLearning(robot_task_hierarchy, mastery_threshold=0.85)

# Simulate curriculum progression
performance_history = []
for training_step in range(1000):
    # Simulate increasing performance with some noise
    base_performance = min(0.95, 0.3 + training_step * 0.001)
    noise = np.random.normal(0, 0.1)
    current_performance = max(0, min(1, base_performance + noise))
    
    performance_history.append(current_performance)
    
    # Update curriculum every 50 steps
    if training_step % 50 == 0:
        curriculum.update_curriculum(performance_history)
        
        if training_step % 200 == 0:
            progress = curriculum.get_curriculum_progress()
            print(f"\nüéì Training Step {training_step}:")
            print(f"   Curriculum Progress: {progress['progress_percentage']:.1f}%")
            print(f"   Current Level: {progress['current_level']}")

print(f"\n‚úÖ Curriculum learning simulation completed!")
print(f"Final curriculum level: {curriculum.current_level}/{len(robot_task_hierarchy)-1}")</pre>
        </div>
      </div>

      <div id="meta" class="tab-content">
        <div class="info">
          <strong>üß† Meta-Learning for VLA:</strong><br>
          Train models to quickly adapt to new robot types with minimal demonstrations. Essential for rapid deployment across diverse embodiments.
        </div>

        <div class="warning">
          <strong>üîÑ The Few-Shot Adaptation Challenge:</strong><br>
          Traditional VLA training requires thousands of demonstrations per robot type. Meta-learning enables adaptation with just 10-100 demonstrations by learning good initialization and update rules.<br><br>
          <strong>Key Techniques:</strong><br>
          ‚Ä¢ <strong>MAML (Model-Agnostic Meta-Learning):</strong> Learn initialization that adapts quickly<br>
          ‚Ä¢ <strong>Prototypical Networks:</strong> Learn to classify robot types and adapt accordingly<br>
          ‚Ä¢ <strong>Gradient-Based Meta-Learning:</strong> Learn how to update parameters effectively
        </div>
      </div>

      <div id="constitutional" class="tab-content">
        <div class="breakthrough-highlight">
          ‚öñÔ∏è Constitutional AI for Physical Systems: Teaching robots to be helpful, harmless, and honest in the real world
        </div>

        <div class="success">
          <strong>‚öñÔ∏è Constitutional AI for VLA Models:</strong><br>
          Extend Constitutional AI principles to physical robot behavior, ensuring safe and aligned behavior in real-world environments.<br><br>
          <strong>Core Principles for Robot Behavior:</strong><br>
          ‚Ä¢ <strong>Physical Safety:</strong> Never perform actions that could harm humans or property<br>
          ‚Ä¢ <strong>Task Alignment:</strong> Always work toward the intended goal, not just literal instruction following<br>
          ‚Ä¢ <strong>Graceful Failure:</strong> When uncertain, ask for clarification or stop safely<br>
          ‚Ä¢ <strong>Transparency:</strong> Communicate intentions and uncertainties to human operators<br><br>
          <strong>Implementation:</strong> Train models to critique and revise their own action plans before execution
        </div>
      </div>

      <div id="multimodal" class="tab-content">
        <div class="info">
          <strong>üåê Multi-Modal VLA Extensions:</strong><br>
          Extend VLA beyond vision and language to include audio, haptic feedback, and proprioceptive sensing for richer robot interaction.
        </div>

        <div class="data-flow">
          <div class="data-component collection">
            <h4>üëÅÔ∏è Vision</h4>
            <div>RGB + Depth</div>
          </div>
          <div class="data-arrow">+</div>
          <div class="data-component processing">
            <h4>üîä Audio</h4>
            <div>Speech + Environmental</div>
          </div>
          <div class="data-arrow">+</div>
          <div class="data-component training">
            <h4>‚úã Haptic</h4>
            <div>Touch + Force Feedback</div>
          </div>
          <div class="data-arrow">+</div>
          <div class="data-component evaluation">
            <h4>üß† Proprioception</h4>
            <div>Joint States + IMU</div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <h2>üéØ Section 6: Key Takeaways - Building Production VLA Models</h2>

    <div class="step">
      <h3>üí° Essential Training Insights</h3>
      <div class="model-comparison">
        <div class="model-card">
          <div class="model-name">üìä Data is Everything</div>
          <div class="tutorial-description">
            High-quality, diverse robot demonstration data is more important than model size. OpenVLA's success comes from careful data curation.
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">üîç</span><span>Rigorous quality control improves performance 25%+</span></div>
            <div class="capability"><span class="capability-icon">üåê</span><span>Cross-embodiment data enables generalization</span></div>
            <div class="capability"><span class="capability-icon">üé≠</span><span>Synthetic data scales training beyond physical limits</span></div>
          </div>
        </div>

        <div class="model-card">
          <div class="model-name">üèóÔ∏è Architecture Efficiency</div>
          <div class="tutorial-description">
            Smart architecture choices (VQ-VAE, FAST tokenization, adapters) matter more than raw parameter count for robotics applications.
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">‚ö°</span><span>Efficient tokenization enables real-time control</span></div>
            <div class="capability"><span class="capability-icon">üîß</span><span>Adapter training reduces costs by 100x</span></div>
            <div class="capability"><span class="capability-icon">üéØ</span><span>Specialized heads outperform generic approaches</span></div>
          </div>
        </div>

        <div class="model-card">
          <div class="model-name">üî¨ Evaluation Rigor</div>
          <div class="tutorial-description">
            Comprehensive evaluation across multiple robots and safety scenarios is critical before real-world deployment.
          </div>
          <div class="model-capabilities">
            <div class="capability"><span class="capability-icon">ü§ñ</span><span>Cross-embodiment testing validates generalization</span></div>
            <div class="capability"><span class="capability-icon">üõ°Ô∏è</span><span>Safety verification prevents dangerous behavior</span></div>
            <div class="capability"><span class="capability-icon">üìà</span><span>Real-world validation confirms sim-to-real transfer</span></div>
          </div>
        </div>
      </div>
    </div>

    <div class="step">
      <h3>üìã Your VLA Training Checklist</h3>
      <div class="success">
        <strong>‚úÖ Pre-Training Checklist:</strong><br>
        <input type="checkbox"> <strong>Data Collection:</strong> Gathered 10K+ high-quality robot demonstrations<br>
        <input type="checkbox"> <strong>Quality Control:</strong> Applied filtering, removed failed demonstrations<br>
        <input type="checkbox"> <strong>Architecture Selection:</strong> Chose appropriate model size for target hardware<br>
        <input type="checkbox"> <strong>Infrastructure:</strong> Setup multi-GPU training environment<br>
        <input type="checkbox"> <strong>Baseline Evaluation:</strong> Established performance benchmarks<br><br>
        
        <strong>üéØ During Training:</strong><br>
        <input type="checkbox"> <strong>Monitoring:</strong> Track loss, learning rates, gradient norms<br>
        <input type="checkbox"> <strong>Validation:</strong> Regular cross-embodiment testing<br>
        <input type="checkbox"> <strong>Safety Checks:</strong> Verify generated actions stay within safe bounds<br>
        <input type="checkbox"> <strong>Checkpointing:</strong> Save model states for recovery<br><br>
        
        <strong>üöÄ Post-Training:</strong><br>
        <input type="checkbox"> <strong>Comprehensive Evaluation:</strong> Test on multiple robots and tasks<br>
        <input type="checkbox"> <strong>Safety Validation:</strong> Verify safe behavior in edge cases<br>
        <input type="checkbox"> <strong>Performance Analysis:</strong> Compare against baselines<br>
        <input type="checkbox"> <strong>Documentation:</strong> Record training process and lessons learned
      </div>
    </div>

    <div class="breakthrough-highlight">
      üéØ Bottom Line: Successful VLA training combines high-quality data, efficient architectures, and rigorous evaluation. The open source community proves that with the right approach, small teams can build world-class robot foundation models.
    </div>

    <div class="success">
      <strong>üéì You've Mastered VLA Training!</strong><br><br>
      You now understand the complete pipeline for training production VLA models, from data curation to evaluation methodologies. You've seen how to implement OpenVLA-style training, optimize for different hardware constraints, and ensure high-quality results through rigorous testing.<br><br>
      <strong>Ready for deployment?</strong> Continue to <a href="deploying-vlas.html">Deploying VLAs: Hardware, Integration & Production</a> to learn how to take your trained models to real robots, or explore <a href="advanced-vla-robotics.html">Advanced VLA & Future Robotics</a> for cutting-edge research directions.
    </div>
  </div>

<script>
   // ---------------------------
   // Utility: copy code blocks
   // ---------------------------
   function copyCode(btn){
     try{
       const pre = btn.parentElement.querySelector('pre');
       const text = pre.innerText;
       navigator.clipboard.writeText(text);
       btn.textContent = '‚úî Copied';
       setTimeout(()=>btn.textContent='üìã Copy', 1500);
     }catch(e){
       console.error(e);
     }
   }

   // ---------------------------
   // Section 1: Data source explorer
   // ---------------------------
   function switchDataSource(source, el) {
     document.querySelectorAll('.source-tab').forEach(t => t.classList.remove('active'));
     el.classList.add('active');
     
     const content = document.getElementById('dataSourceContent');
     const sourceData = {
       oxe: {
         title: 'üåê Open X-Embodiment Dataset',
         description: 'The largest multi-robot dataset with standardized format',
         stats: '1M+ demonstrations ‚Ä¢ 60+ datasets ‚Ä¢ 15+ robots ‚Ä¢ 100+ tasks',
         details: 'Standardized action spaces, consistent observation formats, comprehensive task annotations. Includes data from Berkeley, Stanford, Google, and community contributors.'
       },
       aloha: {
         title: 'ü¶æ ALOHA: Bimanual Mobile Manipulation',
         description: 'High-quality bimanual robot demonstrations for complex tasks',
         stats: '25K demonstrations ‚Ä¢ Mobile platform ‚Ä¢ Bimanual control ‚Ä¢ Kitchen tasks',
         details: 'Focus on complex manipulation requiring two arms. Mobile base enables navigation + manipulation. High success rate demonstrations.'
       },
       bridge: {
         title: 'üåâ Bridge Data: Large-Scale Robot Learning',
         description: 'Diverse manipulation dataset from multiple institutions',
         stats: '100K+ demonstrations ‚Ä¢ 7 robots ‚Ä¢ Diverse objects ‚Ä¢ Real environments', 
         details: 'Cross-institutional data collection. Focus on object manipulation with diverse backgrounds, lighting, and objects.'
       },
       rt1: {
         title: 'ü§ñ RT-1 Dataset: Google Robotics Data',
         description: 'Large-scale dataset from Google\'s robotics research',
         stats: '130K demonstrations ‚Ä¢ Google robots ‚Ä¢ Office environments ‚Ä¢ Instruction following',
         details: 'High-quality demonstrations with natural language instructions. Focus on office and kitchen environments.'
       },
       synthetic: {
         title: 'üé≠ Synthetic Robot Data Generation',
         description: 'Physics-based simulation for unlimited data generation',
         stats: 'Unlimited scale ‚Ä¢ Perfect labels ‚Ä¢ Domain randomization ‚Ä¢ Cost effective',
         details: 'MuJoCo/Isaac Gym simulations with domain randomization. Enables training data generation at scale with perfect ground truth.'
       }
     };
     
     const data = sourceData[source];
     content.innerHTML = `
       <h4>${data.title}</h4>
       <p><strong>${data.description}</strong></p>
       <div style="background:#e9ecef;padding:10px;border-radius:6px;margin:10px 0;font-family:'Courier New',monospace;font-size:12px">
         ${data.stats}
       </div>
       <p style="font-size:13px;color:#666">${data.details}</p>
     `;
   }

   function analyzeDatasets(){
     const filter = document.getElementById('datasetFilter').value;
     const quality = document.getElementById('qualityThreshold').value;
     const box = document.getElementById('datasetAnalysis');
     
     const analysis = {
       all: { datasets: 47, demos: 850000, robots: 15, quality_score: 0.78 },
       manipulation: { datasets: 32, demos: 650000, robots: 12, quality_score: 0.82 },
       navigation: { datasets: 8, demos: 120000, robots: 6, quality_score: 0.75 },
       mobile: { datasets: 12, demos: 200000, robots: 8, quality_score: 0.73 },
       humanoid: { datasets: 5, demos: 80000, robots: 3, quality_score: 0.69 }
     }[filter];
     
     const qualityMultiplier = { low: 1.0, medium: 0.7, high: 0.4 }[quality];
     const filteredDemos = Math.floor(analysis.demos * qualityMultiplier);
     
     box.innerHTML = `
       <div class="metric-grid">
         <div class="metric-card"><div class="metric-value">${analysis.datasets}</div><div class="metric-label">Available Datasets</div></div>
         <div class="metric-card"><div class="metric-value">${filteredDemos.toLocaleString()}</div><div class="metric-label">Quality Demonstrations</div></div>
         <div class="metric-card"><div class="metric-value">${analysis.robots}</div><div class="metric-label">Robot Embodiments</div></div>
         <div class="metric-card"><div class="metric-value">${(analysis.quality_score * 100).toFixed(0)}%</div><div class="metric-label">Average Quality Score</div></div>
       </div>
       <div class="info">
         <strong>Filter:</strong> ${filter.toUpperCase()} ‚Ä¢ <strong>Quality:</strong> ${quality.toUpperCase()}<br>
         <strong>Estimated training cost:</strong> $${Math.floor(filteredDemos * 0.001 * 50).toLocaleString()} - $${Math.floor(filteredDemos * 0.001 * 200).toLocaleString()}
       </div>
     `;
   }

   function generateSyntheticData(){
     const env = document.getElementById('simEnvironment').value;
     const complexity = document.getElementById('taskComplexity').value;
     const randomization = document.getElementById('domainRandom').value;
     const box = document.getElementById('syntheticResults');
     
     const simData = {
       mujoco: { speed: 100, quality: 0.85, cost: 0.05 },
       isaac: { speed: 1000, quality: 0.82, cost: 0.02 },
       pybullet: { speed: 200, quality: 0.75, cost: 0.01 },
       unreal: { speed: 20, quality: 0.95, cost: 0.50 }
     }[env];
     
     const complexityMultiplier = { basic: 1.0, medium: 0.6, complex: 0.3, extreme: 0.1 }[complexity];
     const dailyGeneration = Math.floor(simData.speed * complexityMultiplier * 24); // demos per day
     
     box.innerHTML = `
       <div class="metric-grid">
         <div class="metric-card"><div class="metric-value">${dailyGeneration.toLocaleString()}</div><div class="metric-label">Demos/Day</div></div>
         <div class="metric-card"><div class="metric-value">${(simData.quality * 100).toFixed(0)}%</div><div class="metric-label">Quality Score</div></div>
         <div class="metric-card"><div class="metric-value">$${simData.cost}</div><div class="metric-label">Cost/Demo</div></div>
         <div class="metric-card"><div class="metric-value">${env.toUpperCase()}</div><div class="metric-label">Environment</div></div>
       </div>
       <div class="success">
         <strong>Generation Capacity:</strong> ${(dailyGeneration * 30).toLocaleString()} demos/month<br>
         <strong>Monthly Cost:</strong> $${(dailyGeneration * 30 * simData.cost).toLocaleString()}<br>
         <strong>Recommended for:</strong> ${complexity} tasks with ${randomization} domain randomization
       </div>
     `;
   }

   // ---------------------------
   // Data strategy selection
   // ---------------------------
   function selectDataStrategy(strategy, el){
     document.querySelectorAll('.dataset-card').forEach(c=>c.classList.remove('selected'));
     el.classList.add('selected');
     
     const box = document.getElementById('dataStrategyAnalysis');
     const strategies = {
       teleoperation: {
         best_for: 'High-quality demonstrations for complex tasks',
         pros: ['Human expertise', 'High success rates', 'Complex task coverage'],
         cons: ['Expensive', 'Limited scale', 'Operator fatigue'],
         use_case: 'Initial dataset creation, complex skill demonstration, quality baselines'
       },
       autonomous: {
         best_for: 'Large-scale data collection with existing robot capabilities',
         pros: ['Scalable', 'Low cost per demo', 'Continuous collection'],
         cons: ['Variable quality', 'Requires initial policy', 'Limited exploration'],
         use_case: 'Scaling existing capabilities, data augmentation, continuous improvement'
       },
       simulation: {
         best_for: 'Unlimited data generation with perfect control over task distribution',
         pros: ['Unlimited scale', 'Perfect labels', 'Complete safety', 'Task control'],
         cons: ['Sim-to-real gap', 'Physics approximation', 'Limited realism'],
         use_case: 'Pre-training, curriculum learning, safety testing, rare scenario coverage'
       }
     };
     
     const data = strategies[strategy];
     box.innerHTML = `
       <div class="info">
         <strong>Selected Strategy:</strong> ${strategy.toUpperCase()}<br><br>
         <strong>Best for:</strong> ${data.best_for}<br>
         <strong>Advantages:</strong> ${data.pros.join(', ')}<br>
         <strong>Limitations:</strong> ${data.cons.join(', ')}<br>
         <strong>Primary use case:</strong> ${data.use_case}
       </div>
     `;
   }

   // ---------------------------
   // Section 2: Architecture selection
   // ---------------------------
   function selectVLAArchitecture(arch, el){
     document.querySelectorAll('.model-card').forEach(c=>c.classList.remove('selected'));
     el.classList.add('selected');
     
     const box = document.getElementById('architectureAnalysis');
     const architectures = {
       openvla: {
         summary: 'Balanced approach optimizing for performance and accessibility',
         training_time: '3-7 days on 4x A100',
         data_requirements: '100K-1M demonstrations',
         best_for: 'Research, prototyping, cross-embodiment learning',
         limitations: 'Moderate inference latency, requires GPU for deployment'
       },
       smolvla: {
         summary: 'Efficiency-focused for edge deployment and real-time control',
         training_time: '12-24 hours on 1x A100',
         data_requirements: '10K-100K demonstrations',
         best_for: 'Edge deployment, consumer robotics, battery-powered robots',
         limitations: 'Limited complexity handling, requires domain-specific tuning'
       },
       groot: {
         summary: 'State-of-the-art performance for complex humanoid robotics',
         training_time: '2-4 weeks on 32x H100',
         data_requirements: '1M-10M demonstrations',
         best_for: 'Advanced humanoid robots, research frontiers, unlimited budgets',
         limitations: 'Extremely high compute requirements, complex infrastructure'
       }
     };
     
     const data = architectures[arch];
     box.innerHTML = `
       <div class="success">
         <strong>Architecture:</strong> ${arch.toUpperCase()}<br><br>
         ${data.summary}<br><br>
         <strong>Training Time:</strong> ${data.training_time}<br>
         <strong>Data Requirements:</strong> ${data.data_requirements}<br>
         <strong>Best for:</strong> ${data.best_for}<br>
         <strong>Limitations:</strong> ${data.limitations}
       </div>
     `;
   }

   // ---------------------------
   // Training strategy tabs
   // ---------------------------
   function switchTrainingTab(id, el){
     document.querySelectorAll('.tab').forEach(t=>t.classList.remove('active'));
     document.querySelectorAll('.tab-content').forEach(p=>p.classList.remove('active'));
     el.classList.add('active');
     document.getElementById(id).classList.add('active');
   }

   function switchAdvancedTab(id, el){
     document.querySelectorAll('.tab').forEach(t=>t.classList.remove('active'));
     document.querySelectorAll('.tab-content').forEach(p=>p.classList.remove('active'));
     el.classList.add('active');
     document.getElementById(id).classList.add('active');
   }

   // ---------------------------
   // Training recipe generator
   // ---------------------------
   function generateTrainingRecipe(){
     const model = document.getElementById('targetModel').value;
     const hardware = document.getElementById('hardwareSetup').value;
     const dataset = document.getElementById('datasetSize').value;
     const box = document.getElementById('trainingRecipeOutput');
     
     const recipes = {
       openvla: {
         batch_size: { single_gpu: 4, multi_gpu: 8, cluster: 16, budget: 2 },
         gradient_acc: { single_gpu: 32, multi_gpu: 16, cluster: 8, budget: 64 },
         learning_rate: 1e-5,
         epochs: { small: 100, medium: 50, large: 20, massive: 10 },
         estimated_time: { single_gpu: '7 days', multi_gpu: '2 days', cluster: '12 hours', budget: '2 weeks' },
         cost: { small: '$500', medium: '$2000', large: '$8000', massive: '$25000' }
       },
       smolvla: {
         batch_size: { single_gpu: 16, multi_gpu: 32, cluster: 64, budget: 8 },
         gradient_acc: { single_gpu: 8, multi_gpu: 4, cluster: 2, budget: 16 },
         learning_rate: 3e-5,
         epochs: { small: 80, medium: 40, large: 15, massive: 8 },
         estimated_time: { single_gpu: '18 hours', multi_gpu: '6 hours', cluster: '2 hours', budget: '3 days' },
         cost: { small: '$100', medium: '$400', large: '$1500', massive: '$5000' }
       },
       pi0: {
         batch_size: { single_gpu: 8, multi_gpu: 16, cluster: 32, budget: 4 },
         gradient_acc: { single_gpu: 16, multi_gpu: 8, cluster: 4, budget: 32 },
         learning_rate: 2e-5,
         epochs: { small: 60, medium: 30, large: 12, massive: 6 },
         estimated_time: { single_gpu: '3 days', multi_gpu: '18 hours', cluster: '6 hours', budget: '1 week' },
         cost: { small: '$300', medium: '$1200', large: '$4500', massive: '$15000' }
       },
       groot: {
         batch_size: { single_gpu: 2, multi_gpu: 4, cluster: 8, budget: 1 },
         gradient_acc: { single_gpu: 64, multi_gpu: 32, cluster: 16, budget: 128 },
         learning_rate: 5e-6,
         epochs: { small: 200, medium: 100, large: 40, massive: 20 },
         estimated_time: { single_gpu: '4 weeks', multi_gpu: '1 week', cluster: '2 days', budget: '8 weeks' },
         cost: { small: '$5000', medium: '$20000', large: '$80000', massive: '$300000' }
       }
     };
     
     const recipe = recipes[model];
     
     box.innerHTML = `
       <div class="cost-breakdown">
         <div class="cost-header">${model.toUpperCase()} Training Recipe</div>
         <div class="cost-item"><span>Batch Size per GPU:</span><span>${recipe.batch_size[hardware]}</span></div>
         <div class="cost-item"><span>Gradient Accumulation:</span><span>${recipe.gradient_acc[hardware]} steps</span></div>
         <div class="cost-item"><span>Learning Rate:</span><span>${recipe.learning_rate}</span></div>
         <div class="cost-item"><span>Training Epochs:</span><span>${recipe.epochs[dataset]}</span></div>
         <div class="cost-item"><span>Estimated Time:</span><span>${recipe.estimated_time[hardware]}</span></div>
         <div class="cost-total"><span>Estimated Cost:</span><span>${recipe.cost[dataset]}</span></div>
       </div>
       <div class="info">
         <strong>Effective batch size:</strong> ${recipe.batch_size[hardware] * recipe.gradient_acc[hardware] * (hardware === 'single_gpu' ? 1 : hardware === 'multi_gpu' ? 4 : 8)}<br>
         <strong>Hardware recommendation:</strong> ${hardware === 'budget' ? 'Use gradient checkpointing and smaller batch sizes' : 'Optimal for this configuration'}
       </div>
     `;
   }

   // ---------------------------
   // Section 3: Training simulator  
   // ---------------------------
   function runTrainingSimulation(){
     const arch = document.getElementById('simModelArch').value;
     const scale = document.getElementById('simDataScale').value;
     const hardware = document.getElementById('simHardware').value;
     const strategy = document.getElementById('simStrategy').value;
     const quality = document.getElementById('simDataQuality').value;
     const box = document.getElementById('simulationResults');
     
     // Handle custom model size
     let params;
     if (arch === 'custom') {
       params = parseFloat(document.getElementById('customParams').value) || 7;
     } else {
       params = { smolvla: 0.6, openvla: 7, groot: 20 }[arch];
     }
     
     // Calculate training costs
     const hardwareSpecs = {
       a100: { cost_per_hour: 3.5, memory: 40, compute: 312 },
       h100: { cost_per_hour: 8.0, memory: 80, compute: 1000 },
       v100: { cost_per_hour: 2.5, memory: 32, compute: 125 },
       rtx4090: { cost_per_hour: 1.0, memory: 24, compute: 83 }
     };
     
     const dataScales = {
       pilot: { demos: 1000, data_cost: 50 },
       small: { demos: 10000, data_cost: 500 },
       medium: { demos: 100000, data_cost: 5000 },
       large: { demos: 1000000, data_cost: 50000 }
     };
     
     const qualityFactors = {
       raw: { multiplier: 1.0, processing_cost: 0 },
       filtered: { multiplier: 0.7, processing_cost: 1000 },
       curated: { multiplier: 0.4, processing_cost: 5000 }
     };
     
     const strategyFactors = {
       joint: { time_multiplier: 1.0, complexity: 'High' },
       staged: { time_multiplier: 1.5, complexity: 'Medium' },
       adapter: { time_multiplier: 0.1, complexity: 'Low' }
     };
     
     const hw = hardwareSpecs[hardware];
     const data = dataScales[scale];
     const qual = qualityFactors[quality];
     const strat = strategyFactors[strategy];
     
     // Calculate training time (simplified model)
     const baseTrainingHours = Math.pow(params, 0.8) * Math.pow(data.demos * qual.multiplier / 100000, 0.5) * 20;
     const actualTrainingHours = baseTrainingHours * strat.time_multiplier;
     
     // Calculate costs
     const computeCost = actualTrainingHours * hw.cost_per_hour;
     const dataCost = data.data_cost + qual.processing_cost;
     const totalCost = computeCost + dataCost;
     
     // Memory requirements
     const memoryRequired = params * 4 + 8; // Rough estimate: 4GB per billion params + overhead
     const memoryFit = memoryRequired <= hw.memory;
     
     box.innerHTML = `
       <div class="cost-breakdown">
         <div class="cost-header">üßÆ Training Cost Breakdown</div>
         <div class="cost-item"><span>Model Size:</span><span>${params}B parameters</span></div>
         <div class="cost-item"><span>Training Data:</span><span>${(data.demos * qual.multiplier).toLocaleString()} demos</span></div>
         <div class="cost-item"><span>Training Time:</span><span>${actualTrainingHours.toFixed(1)} hours</span></div>
         <div class="cost-item"><span>Compute Cost:</span><span>$${computeCost.toFixed(0)}</span></div>
         <div class="cost-item"><span>Data Cost:</span><span>$${dataCost.toLocaleString()}</span></div>
         <div class="cost-total"><span>Total Cost:</span><span>$${totalCost.toFixed(0)}</span></div>
       </div>
       
       <div class="metric-grid">
         <div class="metric-card"><div class="metric-value">${memoryRequired.toFixed(0)}GB</div><div class="metric-label">Memory Required</div></div>
         <div class="metric-card"><div class="metric-value">${hw.memory}GB</div><div class="metric-label">Available Memory</div></div>
         <div class="metric-card"><div class="metric-value">${memoryFit ? '‚úÖ' : '‚ùå'}</div><div class="metric-label">Memory Fit</div></div>
         <div class="metric-card"><div class="metric-value">${strat.complexity}</div><div class="metric-label">Setup Complexity</div></div>
       </div>
       
       ${!memoryFit ? '<div class="danger"><strong>‚ö†Ô∏è Memory Warning:</strong> This configuration exceeds available GPU memory. Consider model sharding, gradient checkpointing, or smaller batch sizes.</div>' : ''}
       
       <div class="success">
         <strong>Recommendation:</strong> ${strategy === 'adapter' ? 'Excellent choice for rapid iteration and low-cost deployment' : strategy === 'joint' ? 'Best performance but requires careful hyperparameter tuning' : 'Good balance of stability and performance'}<br>
         <strong>Timeline:</strong> ${actualTrainingHours < 24 ? 'Same day' : actualTrainingHours < 168 ? Math.ceil(actualTrainingHours/24) + ' days' : Math.ceil(actualTrainingHours/168) + ' weeks'} to completion
       </div>
     `;
   }

   // Toggle custom params input
   document.getElementById('simModelArch').addEventListener('change', function(){
     const customGroup = document.getElementById('customParamsGroup');
     customGroup.style.display = this.value === 'custom' ? 'block' : 'none';
   });

   // ---------------------------
   // Data quality analyzer
   // ---------------------------
   function analyzeDataQuality(){
     const metric = document.getElementById('qualityMetric').value;
     const robot = document.getElementById('qualityRobot').value;
     const box = document.getElementById('qualityAnalysisResults');
     
     const qualityData = {
       success: {
         franka: { raw: 0.73, filtered: 0.87, curated: 0.94 },
         ur5: { raw: 0.68, filtered: 0.83, curated: 0.91 },
         aloha: { raw: 0.61, filtered: 0.78, curated: 0.89 },
         mobile: { raw: 0.58, filtered: 0.74, curated: 0.86 }
       },
       smoothness: {
         franka: { raw: 0.65, filtered: 0.82, curated: 0.93 },
         ur5: { raw: 0.62, filtered: 0.79, curated: 0.90 },
         aloha: { raw: 0.57, filtered: 0.74, curated: 0.87 },
         mobile: { raw: 0.52, filtered: 0.69, curated: 0.83 }
       },
       efficiency: {
         franka: { raw: 0.71, filtered: 0.84, curated: 0.92 },
         ur5: { raw: 0.69, filtered: 0.81, curated: 0.89 },
         aloha: { raw: 0.64, filtered: 0.77, curated: 0.86 },
         mobile: { raw: 0.59, filtered: 0.72, curated: 0.82 }
       },
       safety: {
         franka: { raw: 0.88, filtered: 0.95, curated: 0.99 },
         ur5: { raw: 0.85, filtered: 0.93, curated: 0.98 },
         aloha: { raw: 0.82, filtered: 0.91, curated: 0.97 },
         mobile: { raw: 0.79, filtered: 0.89, curated: 0.96 }
       }
     };
     
     const scores = qualityData[metric][robot];
     const getScoreClass = (score) => {
       if (score >= 0.9) return 'quality-excellent';
       if (score >= 0.8) return 'quality-good';
       if (score >= 0.7) return 'quality-average';
       return 'quality-poor';
     };
     
     box.innerHTML = `
       <div class="benchmark-table">
         <thead>
           <tr><th>Data Quality Level</th><th>${metric.charAt(0).toUpperCase() + metric.slice(1)} Score</th><th>Quality Indicator</th></tr>
         </thead>
         <tbody>
           <tr>
             <td>Raw Data</td>
             <td class="${getScoreClass(scores.raw).replace('quality-', 'score-')}">${(scores.raw * 100).toFixed(1)}%</td>
             <td><span class="data-quality-indicator ${getScoreClass(scores.raw)}"></span></td>
           </tr>
           <tr>
             <td>Filtered Data</td>
             <td class="${getScoreClass(scores.filtered).replace('quality-', 'score-')}">${(scores.filtered * 100).toFixed(1)}%</td>
             <td><span class="data-quality-indicator ${getScoreClass(scores.filtered)}"></span></td>
           </tr>
           <tr>
             <td>Curated Data</td>
             <td class="${getScoreClass(scores.curated).replace('quality-', 'score-')}">${(scores.curated * 100).toFixed(1)}%</td>
             <td><span class="data-quality-indicator ${getScoreClass(scores.curated)}"></span></td>
           </tr>
         </tbody>
       </table>
       <div class="info">
         <strong>Robot:</strong> ${robot.toUpperCase()} ‚Ä¢ <strong>Metric:</strong> ${metric.charAt(0).toUpperCase() + metric.slice(1)}<br>
         <strong>Improvement:</strong> +${((scores.curated - scores.raw) * 100).toFixed(1)}% from raw to curated<br>
         <strong>Recommendation:</strong> ${scores.filtered > 0.85 ? 'Filtered data sufficient for most applications' : 'Consider human curation for critical applications'}
       </div>
     `;
   }

   // ---------------------------
   // Section 4: Evaluation functions
   // ---------------------------
   function runEvaluation(){
     const model = document.getElementById('evalModel').value;
     const domain = document.getElementById('evalDomain').value;
     const robot = document.getElementById('evalRobot').value;
     const box = document.getElementById('evaluationResults');
     
     const evalData = {
       openvla: {
         manipulation: { franka: 0.84, ur5: 0.78, aloha: 0.81, mobile: 0.71 },
         navigation: { franka: 0.65, ur5: 0.62, aloha: 0.68, mobile: 0.79 },
         assembly: { franka: 0.77, ur5: 0.73, aloha: 0.85, mobile: 0.66 },
         kitchen: { franka: 0.72, ur5: 0.69, aloha: 0.88, mobile: 0.74 }
       },
       smolvla: {
         manipulation: { franka: 0.78, ur5: 0.73, aloha: 0.76, mobile: 0.68 },
         navigation: { franka: 0.61, ur5: 0.58, aloha: 0.63, mobile: 0.74 },
         assembly: { franka: 0.71, ur5: 0.67, aloha: 0.79, mobile: 0.61 },
         kitchen: { franka: 0.67, ur5: 0.64, aloha: 0.82, mobile: 0.69 }
       },
       rt2: {
         manipulation: { franka: 0.82, ur5: 0.76, aloha: 0.79, mobile: 0.73 },
         navigation: { franka: 0.68, ur5: 0.65, aloha: 0.71, mobile: 0.81 },
         assembly: { franka: 0.75, ur5: 0.71, aloha: 0.83, mobile: 0.68 },
         kitchen: { franka: 0.74, ur5: 0.71, aloha: 0.86, mobile: 0.76 }
       },
       custom: {
         manipulation: { franka: 0.75, ur5: 0.70, aloha: 0.73, mobile: 0.65 },
         navigation: { franka: 0.58, ur5: 0.55, aloha: 0.60, mobile: 0.71 },
         assembly: { franka: 0.68, ur5: 0.64, aloha: 0.76, mobile: 0.58 },
         kitchen: { franka: 0.64, ur5: 0.61, aloha: 0.79, mobile: 0.66 }
       }
     };
     
     const score = evalData[model][domain][robot];
     const maxScore = Math.max(...Object.values(evalData).map(m => m[domain][robot]));
     const isTopPerformer = score === maxScore;
     
     const getScoreClass = (score) => {
       if (score >= 0.85) return 'score-excellent';
       if (score >= 0.75) return 'score-good';
       if (score >= 0.65) return 'score-average';
       return 'score-poor';
     };
     
     // Generate additional metrics
     const safetyScore = Math.max(0.8, score * 0.95 + Math.random() * 0.05);
     const latency = model === 'smolvla' ? '15ms' : model === 'openvla' ? '45ms' : '35ms';
     const memoryUsage = model === 'smolvla' ? '2.1GB' : model === 'openvla' ? '14.2GB' : '8.5GB';
     
     box.innerHTML = `
       <div class="evaluation-grid">
         <div class="eval-metric">
           <div class="eval-score ${getScoreClass(score)}" style="color: ${score >= 0.85 ? '#155724' : score >= 0.75 ? '#0c5460' : score >= 0.65 ? '#856404' : '#721c24'}">${(score * 100).toFixed(1)}%</div>
           <div class="eval-label">Task Success Rate</div>
         </div>
         <div class="eval-metric">
           <div class="eval-score" style="color: ${safetyScore >= 0.95 ? '#155724' : '#0c5460'}">${(safetyScore * 100).toFixed(1)}%</div>
           <div class="eval-label">Safety Compliance</div>
         </div>
         <div class="eval-metric">
           <div class="eval-score" style="color: #0c5460">${latency}</div>
           <div class="eval-label">Inference Latency</div>
         </div>
         <div class="eval-metric">
           <div class="eval-score" style="color: #0c5460">${memoryUsage}</div>
           <div class="eval-label">Memory Usage</div>
         </div>
       </div>
       
       <div class="${isTopPerformer ? 'success' : 'info'}">
         <strong>Model:</strong> ${model.toUpperCase()} ‚Ä¢ <strong>Domain:</strong> ${domain} ‚Ä¢ <strong>Robot:</strong> ${robot}<br>
         <strong>Performance:</strong> ${isTopPerformer ? 'üèÜ Top performer in this category' : 'Competitive performance with room for improvement'}<br>
         <strong>Key Insights:</strong> ${score > 0.8 ? 'Production ready' : score > 0.7 ? 'Suitable for pilot deployments' : 'Requires additional training or task-specific fine-tuning'}<br>
         <strong>Recommendation:</strong> ${safetyScore > 0.95 ? 'Approved for supervised deployment' : 'Additional safety validation required'}
       </div>
     `;
   }

   function simulateEvaluation(){
     const phase = document.getElementById('evalPhase').value;
     const taskCat = document.getElementById('evalTaskCategory').value;
     const box = document.getElementById('evaluationPhaseResults');
     
     const phaseData = {
       simulation: {
         description: 'High-fidelity physics simulation testing',
         requirements: 'Success rate > 90% before real robot testing',
         typical_results: 'Safe environment for extensive testing',
         next_step: 'Safety verification in controlled environment'
       },
       safety: {
         description: 'Comprehensive safety verification protocol',
         requirements: 'Zero dangerous actions in 1000+ trials',
         typical_results: 'Workspace boundary compliance, emergency stop testing',
         next_step: 'Limited real robot trials with human supervision'
       },
       real_robot: {
         description: 'Real robot validation with human oversight',
         requirements: 'Maintain >80% success rate in real environment',
         typical_results: 'Sim-to-real gap typically 10-20% performance drop',
         next_step: 'Production deployment approval process'
       },
       production: {
         description: 'Final validation for unsupervised deployment',
         requirements: 'Consistent performance across diverse conditions',
         typical_results: 'Long-term reliability and edge case handling',
         next_step: 'Full production deployment authorization'
       }
     };
     
     const data = phaseData[phase];
     const baseScore = Math.random() * 0.3 + 0.65; // Random score between 65-95%
     const adjustedScore = phase === 'real_robot' ? baseScore * 0.85 : baseScore; // Sim-to-real drop
     
     box.innerHTML = `
       <div class="pipeline-visualization">
         <div class="pipeline-step ${phase === 'simulation' ? 'active' : ''}">
           <h4>üß™ Simulation</h4>
           <div style="font-size:10px">${phase === 'simulation' ? 'ACTIVE' : 'COMPLETE'}</div>
         </div>
         <div class="pipeline-step ${phase === 'safety' ? 'active' : ''}">
           <h4>üîí Safety</h4>
           <div style="font-size:10px">${phase === 'safety' ? 'ACTIVE' : phase === 'simulation' ? 'PENDING' : 'COMPLETE'}</div>
         </div>
         <div class="pipeline-step ${phase === 'real_robot' ? 'active' : ''}">
           <h4>ü§ñ Real Robot</h4>
           <div style="font-size:10px">${phase === 'real_robot' ? 'ACTIVE' : ['simulation', 'safety'].includes(phase) ? 'PENDING' : 'COMPLETE'}</div>
         </div>
         <div class="pipeline-step ${phase === 'production' ? 'active' : ''}">
           <h4>‚úÖ Production</h4>
           <div style="font-size:10px">${phase === 'production' ? 'ACTIVE' : 'PENDING'}</div>
         </div>
       </div>
       
       <div class="metric-grid">
         <div class="metric-card"><div class="metric-value">${(adjustedScore * 100).toFixed(1)}%</div><div class="metric-label">Success Rate</div></div>
         <div class="metric-card"><div class="metric-value">${Math.floor(Math.random() * 500 + 100)}</div><div class="metric-label">Trials Completed</div></div>
         <div class="metric-card"><div class="metric-value">${phase.toUpperCase()}</div><div class="metric-label">Current Phase</div></div>
         <div class="metric-card"><div class="metric-value">${taskCat.toUpperCase()}</div><div class="metric-label">Task Category</div></div>
       </div>
       
       <div class="info">
         <strong>Phase:</strong> ${data.description}<br>
         <strong>Requirements:</strong> ${data.requirements}<br>
         <strong>Status:</strong> ${data.typical_results}<br>
         <strong>Next Step:</strong> ${data.next_step}
       </div>
     `;
   }

   function runBenchmark(){
     const suite = document.getElementById('benchmarkSuite').value;
     const baseline = document.getElementById('comparisonBaseline').value;
     const box = document.getElementById('benchmarkResults');
     
     const benchmarkData = {
       manipulation: {
         random: 0.05, bc: 0.65, rl: 0.78, rt2: 0.82,
         tasks: ['Pick & Place', 'Stacking', 'Insertion', 'Pouring'],
         metric: 'Task Success Rate'
       },
       navigation: {
         random: 0.10, bc: 0.72, rl: 0.85, rt2: 0.79,
         tasks: ['Point Navigation', 'Object Navigation', 'Instruction Following', 'Obstacle Avoidance'],
         metric: 'Navigation Success Rate'
       },
       metaworld: {
         random: 0.02, bc: 0.48, rl: 0.73, rt2: 0.69,
         tasks: ['50 Manipulation Tasks', 'Multi-Task Learning', 'Few-Shot Adaptation', 'Cross-Task Transfer'],
         metric: 'Average Success Rate'
       },
       real_robot: {
         random: 0.03, bc: 0.52, rl: 0.68, rt2: 0.74,
         tasks: ['Real Environment', 'Lighting Variations', 'Object Diversity', 'Long Horizons'],
         metric: 'Real-World Success Rate'
       }
     };
     
     const data = benchmarkData[suite];
     const baselineScore = data[baseline];
     const vlaScore = baselineScore * (1.1 + Math.random() * 0.2); // VLA typically 10-30% better
     
     box.innerHTML = `
       <table class="benchmark-table">
         <thead>
           <tr><th>Model</th><th>${data.metric}</th><th>Relative Performance</th></tr>
         </thead>
         <tbody>
           <tr>
             <td>${baseline.toUpperCase()} (Baseline)</td>
             <td class="score-average">${(baselineScore * 100).toFixed(1)}%</td>
             <td>-</td>
           </tr>
           <tr>
             <td>VLA Model (Ours)</td>
             <td class="${vlaScore > 0.8 ? 'score-excellent' : vlaScore > 0.7 ? 'score-good' : 'score-average'}">${(vlaScore * 100).toFixed(1)}%</td>
             <td class="score-excellent">+${((vlaScore - baselineScore) * 100).toFixed(1)}%</td>
           </tr>
         </tbody>
       </table>
       
       <div class="success">
         <strong>Benchmark Suite:</strong> ${suite.toUpperCase()}<br>
         <strong>Test Tasks:</strong> ${data.tasks.join(', ')}<br>
         <strong>Performance Improvement:</strong> ${((vlaScore / baselineScore - 1) * 100).toFixed(1)}% over ${baseline.toUpperCase()}<br>
         <strong>Statistical Significance:</strong> ${vlaScore > baselineScore * 1.05 ? '‚úÖ Significant improvement (p < 0.05)' : '‚ö†Ô∏è Marginal improvement, more data needed'}
       </div>
     `;
   }

   // Initialize default content
   switchDataSource('oxe', document.querySelector('.source-tab.active'));
   analyzeDatasets();
   generateSyntheticData();
   analyzeDataQuality();
 </script>
</body>
</html>
