<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Visual Attention Mechanisms Deep Dive</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);
            color: #e0e0e0;
            line-height: 1.6;
        }
        
        .container {
            background: #ffffff;
            color: #2d2d2d;
            border-radius: 20px;
            padding: 30px;
            margin: 20px 0;
            border: 1px solid #e0e0e0;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        }
        
        .nav-bar {
            background: #2d2d2d;
            color: white;
            padding: 15px 30px;
            border-radius: 15px;
            margin: 20px 0;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        
        .nav-home, .nav-prev, .nav-next {
            background: #ffffff;
            color: #2d2d2d;
            padding: 8px 16px;
            border-radius: 6px;
            text-decoration: none;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        .nav-next {
            background: #28a745;
            color: white;
        }
        
        .nav-home:hover, .nav-prev:hover, .nav-next:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        .nav-title {
            font-size: 1.2em;
            font-weight: bold;
            flex: 1;
            min-width: 300px;
        }
        
        .step {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            padding: 20px;
            margin: 15px 0;
            border-radius: 15px;
            border-left: 4px solid #2d2d2d;
        }
        
        .interactive-demo {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
        }
        
        .demo-title {
            font-size: 1.3em;
            font-weight: bold;
            margin-bottom: 20px;
            text-align: center;
            color: #2d2d2d;
        }
        
        .controls {
            display: flex;
            gap: 15px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        
        .control-group {
            display: flex;
            flex-direction: column;
            gap: 5px;
        }
        
        button {
            background: #2d2d2d;
            border: none;
            color: white;
            padding: 12px 24px;
            border-radius: 8px;
            cursor: pointer;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        
        button:hover {
            background: #1a1a1a;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        input, select {
            padding: 8px 12px;
            border: 1px solid #dadce0;
            border-radius: 6px;
            background: #ffffff;
            color: #2d2d2d;
        }
        
        .math-formula {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            text-align: center;
            font-size: 16px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        
        .attention-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 4px;
            max-width: 200px;
            margin: 0 auto;
            border: 2px solid #dee2e6;
            border-radius: 8px;
            padding: 8px;
            background: white;
        }
        
        .patch {
            width: 40px;
            height: 40px;
            border: 1px solid #ccc;
            border-radius: 4px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 10px;
            font-weight: bold;
            transition: all 0.3s ease;
            background: #f8f9fa;
        }
        
        .patch:hover {
            border-color: #2d2d2d;
            transform: scale(1.1);
        }
        
        .patch.selected {
            background: #28a745;
            color: white;
            border-color: #28a745;
        }
        
        .patch.attending {
            background: linear-gradient(135deg, #ff6b6b, #ee5a24);
            color: white;
            border-color: #ee5a24;
        }
        
        .attention-heatmap {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 2px;
            max-width: 200px;
            margin: 15px auto;
            border-radius: 8px;
            overflow: hidden;
        }
        
        .attention-cell {
            width: 48px;
            height: 48px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 10px;
            font-weight: bold;
            color: white;
            transition: all 0.3s ease;
        }
        
        .visual-comparison {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .visual-card {
            background: #2d2d2d;
            color: white;
            padding: 20px;
            border-radius: 12px;
            text-align: center;
        }
        
        .visual-title {
            font-weight: bold;
            margin-bottom: 15px;
            color: #28a745;
        }
        
        .visual-diagram {
            background: #4a4a4a;
            padding: 20px;
            border-radius: 8px;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            line-height: 1.4;
        }
        
        .matrix-visualization {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
            text-align: center;
        }
        
        .matrix {
            background: #f8f9fa;
            border: 2px solid #dee2e6;
            border-radius: 8px;
            padding: 15px;
        }
        
        .matrix-title {
            font-weight: bold;
            margin-bottom: 10px;
            color: #2d2d2d;
        }
        
        .matrix-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 2px;
            margin: 10px 0;
        }
        
        .matrix-cell {
            width: 30px;
            height: 30px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 8px;
            font-weight: bold;
            border: 1px solid #dee2e6;
            background: white;
        }
        
        .multihead-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        
        .head-card {
            background: white;
            border: 2px solid #e9ecef;
            border-radius: 8px;
            padding: 15px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .head-card:hover {
            border-color: #2d2d2d;
            transform: translateY(-3px);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.1);
        }
        
        .head-card.selected {
            border-color: #28a745;
            background: #d4edda;
        }
        
        .head-title {
            font-weight: bold;
            margin-bottom: 10px;
            color: #2d2d2d;
        }
        
        .head-pattern {
            font-size: 12px;
            color: #666;
            margin-bottom: 10px;
        }
        
        .complexity-calculator {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 20px;
            margin: 15px 0;
        }
        
        .complexity-results {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 15px 0;
        }
        
        .complexity-metric {
            background: white;
            border: 2px solid #e9ecef;
            border-radius: 8px;
            padding: 15px;
            text-align: center;
        }
        
        .metric-value {
            font-size: 1.5em;
            font-weight: bold;
            color: #2d2d2d;
            margin-bottom: 5px;
        }
        
        .metric-label {
            font-size: 12px;
            color: #666;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }
        
        th {
            background: #2d2d2d;
            color: white;
            padding: 12px;
            text-align: center;
            font-weight: bold;
        }
        
        td {
            padding: 12px;
            text-align: center;
            border-bottom: 1px solid #e9ecef;
        }
        
        .layer-selector {
            display: flex;
            gap: 10px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        
        .layer-button {
            padding: 8px 16px;
            border: 2px solid #e9ecef;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            font-weight: bold;
            transition: all 0.3s ease;
            color: #2d2d2d;
        }
        
        .layer-button:hover {
            border-color: #2d2d2d;
        }
        
        .layer-button.active {
            background: #28a745;
            color: white;
            border-color: #28a745;
        }
        
        .attention-evolution {
            margin: 20px 0;
            text-align: center;
        }
        
        .evolution-stage {
            display: inline-block;
            margin: 10px;
            text-align: center;
        }
        
        .stage-title {
            font-weight: bold;
            margin-bottom: 8px;
            color: #2d2d2d;
        }
        
        .stage-description {
            font-size: 12px;
            color: #666;
        }
        
        .info {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            color: #0c5460;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .success {
            background: #d4edda;
            border-left: 4px solid #28a745;
            color: #155724;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .warning {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            color: #856404;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .breakthrough-highlight {
            background: linear-gradient(135deg, #dc3545, #c82333);
            color: white;
            padding: 20px;
            border-radius: 12px;
            text-align: center;
            margin: 20px 0;
            font-size: 18px;
            font-weight: bold;
            box-shadow: 0 4px 15px rgba(220, 53, 69, 0.3);
        }
        
        .receptive-field-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .rf-visualization {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            padding: 20px;
            text-align: center;
        }
        
        .rf-title {
            font-weight: bold;
            margin-bottom: 15px;
            color: #2d2d2d;
        }
        
        .rf-demo {
            margin: 15px 0;
        }
        
        .layer-visualization {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 10px 0;
            padding: 10px;
            background: white;
            border-radius: 6px;
            border: 1px solid #dee2e6;
        }
        
        .layer-label {
            font-weight: bold;
            color: #2d2d2d;
        }
        
        .receptive-field-size {
            width: 100%;
            height: 20px;
            background: #e9ecef;
            border-radius: 10px;
            overflow: hidden;
            margin: 5px 0;
        }
        
        .rf-fill {
            height: 100%;
            background: linear-gradient(135deg, #28a745, #20c997);
            transition: width 1s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 11px;
            font-weight: bold;
        }
        
        @keyframes attention-pulse {
            0% { transform: scale(1); opacity: 0.7; }
            50% { transform: scale(1.1); opacity: 1; }
            100% { transform: scale(1); opacity: 0.7; }
        }
        
        .attention-pulse {
            animation: attention-pulse 1.5s ease-in-out infinite;
        }
        
        @keyframes matrix-multiply {
            0% { transform: translateX(0); }
            50% { transform: translateX(10px); }
            100% { transform: translateX(0); }
        }
        
        .matrix-animation {
            animation: matrix-multiply 2s ease-in-out;
        }
    </style>
</head>
<body>
    <div class="nav-bar">
        <div class="nav-title">🎯 Visual Attention Mechanisms Deep Dive</div>
        <a href="index.html" class="nav-home">🏠 Home</a>
        <a href="patch-embeddings.html" class="nav-prev">← Patch Embeddings</a>
        <a href="clip-architecture.html" class="nav-next">Next: CLIP Architecture →</a>
    </div>

    <div class="container">
        <h1>🎯 Visual Attention Mechanisms Deep Dive</h1>
        <p>Now that you understand how images become patch tokens, let's explore the <strong>core mechanism</strong> that makes Vision Transformers so powerful: <em>attention</em>. This is where the magic happens - how every patch can "see" and relate to every other patch from the very first layer.</p>
        
        <div class="info">
            <strong>🎯 What You'll Master:</strong> The complete mathematics of visual attention, why multi-head attention works, what different attention patterns mean, how global receptive fields enable superior vision understanding, and the computational trade-offs in production systems.
        </div>
    </div>

    <div class="container">
        <h2>🔍 Attention Intuition: The Search Engine Analogy</h2>
        
        <div class="step">
            <h3>💡 From Search to Vision</h3>
            
            <p>Think of attention as a <strong>search engine for image patches</strong>. When processing a patch (like "cat's eye"), the model searches through all other patches to find relevant information (like "cat's face", "whiskers", "fur texture"). The attention mechanism determines <em>how much</em> each patch should contribute to understanding the current patch.</p>
            
            <div class="visual-comparison">
                <div class="visual-card">
                    <div class="visual-title">🔍 Search Engine Process</div>
                    <div class="visual-diagram">
<strong>Query:</strong> "cat behavior"<br>
<strong>Documents:</strong> Web pages<br>
<strong>Matching:</strong> Relevance scores<br>
<strong>Result:</strong> Weighted combination<br><br>
• Query matches relevant docs<br>
• Importance weights assigned<br>
• Final answer synthesized
                    </div>
                </div>
                
                <div class="visual-card">
                    <div class="visual-title">👁️ Visual Attention Process</div>
                    <div class="visual-diagram">
<strong>Query:</strong> Current patch token<br>
<strong>Keys:</strong> All patch tokens<br>
<strong>Matching:</strong> Attention scores<br>
<strong>Result:</strong> Updated representation<br><br>
• Patch queries all other patches<br>
• Attention weights computed<br>
• Features aggregated globally
                    </div>
                </div>
            </div>
            
            <div class="breakthrough-highlight">
                🧠 Key Insight: Every patch simultaneously acts as a QUERY (asking "what's relevant to me?") and a KEY (answering "here's what I contain") for every other patch!
            </div>
        </div>
    </div>

    <div class="container">
        <h2>📐 Attention Mathematics: Step-by-Step Breakdown</h2>
        
        <div class="step">
            <h3>🧮 The Complete Attention Formula</h3>
            
            <div class="math-formula">
                <strong>Attention(Q, K, V) = softmax(QK<sup>T</sup>/√d<sub>k</sub>)V</strong><br><br>
                Where:<br>
                • Q ∈ ℝ<sup>n×d<sub>k</sub></sup> (Queries: "what am I looking for?")<br>
                • K ∈ ℝ<sup>n×d<sub>k</sub></sup> (Keys: "what do I contain?")<br>
                • V ∈ ℝ<sup>n×d<sub>v</sub></sup> (Values: "what information do I provide?")<br>
                • n = sequence length (number of patches)<br>
                • d<sub>k</sub> = key/query dimension<br>
                • d<sub>v</sub> = value dimension
            </div>
        </div>
        
        <div class="step">
            <h3>🎮 Interactive Attention Calculator</h3>
            
            <div class="interactive-demo">
                <div class="demo-title">🧮 Single-Head Attention Demonstrator</div>
                
                <p><strong>Instructions:</strong> Click on any patch to see how it attends to all other patches. Watch the Query×Key computation, softmax normalization, and final weighted aggregation!</p>
                
                <div class="controls">
                    <div class="control-group">
                        <label><strong>Temperature (√d<sub>k</sub>):</strong></label>
                        <input type="range" id="temperature" min="1" max="8" value="4" step="0.5">
                        <span id="temperatureValue">4.0</span>
                    </div>
                    <div class="control-group">
                        <label><strong>Visualization Mode:</strong></label>
                        <select id="visualMode">
                            <option value="attention">Attention Weights</option>
                            <option value="similarity">Raw Similarity (QK^T)</option>
                            <option value="final">Final Output</option>
                        </select>
                    </div>
                </div>
                
                <div style="display: grid; grid-template-columns: 1fr 2fr; gap: 30px; align-items: start;">
                    <div>
                        <h4 style="text-align: center; margin-bottom: 15px;">📷 Image Patches</h4>
                        <div class="attention-grid" id="patchGrid"></div>
                        <p style="text-align: center; font-size: 12px; margin-top: 10px;">
                            <strong>Selected:</strong> <span id="selectedPatch">None</span>
                        </p>
                    </div>
                    
                    <div>
                        <h4 style="text-align: center; margin-bottom: 15px;">🎯 Attention Visualization</h4>
                        <div class="attention-heatmap" id="attentionHeatmap"></div>
                        <div id="computationSteps" style="margin-top: 20px;"></div>
                    </div>
                </div>
                
                <button onclick="animateAttentionComputation()">🎬 Animate Full Computation</button>
                <div id="attentionResults"></div>
            </div>
        </div>
        
        <div class="step">
            <h3>🔢 Matrix Operations Breakdown</h3>
            
            <div class="matrix-visualization" id="matrixVisualization">
                <div class="matrix">
                    <div class="matrix-title">Q (Queries)</div>
                    <div class="matrix-grid" id="queryMatrix"></div>
                    <p style="font-size: 11px; margin-top: 8px;">"What am I looking for?"</p>
                </div>
                
                <div class="matrix">
                    <div class="matrix-title">K^T (Keys)</div>
                    <div class="matrix-grid" id="keyMatrix"></div>
                    <p style="font-size: 11px; margin-top: 8px;">"What do I contain?"</p>
                </div>
                
                <div class="matrix">
                    <div class="matrix-title">V (Values)</div>
                    <div class="matrix-grid" id="valueMatrix"></div>
                    <p style="font-size: 11px; margin-top: 8px;">"What info do I provide?"</p>
                </div>
            </div>
        </div>
    </div>

    <div class="container">
        <h2>🎭 Multi-Head Attention: Why Multiple Perspectives Matter</h2>
        
        <div class="step">
            <h3>🧠 The Multi-Head Advantage</h3>
            
            <p>Single-head attention is like looking at an image with one eye. <strong>Multi-head attention</strong> is like having multiple specialized visual systems - one for detecting edges, another for colors, another for spatial relationships, etc. Each "head" learns to focus on different aspects of the image.</p>
            
            <div class="math-formula">
                <strong>MultiHead(Q,K,V) = Concat(head<sub>1</sub>, head<sub>2</sub>, ..., head<sub>h</sub>)W<sup>O</sup></strong><br><br>
                Where head<sub>i</sub> = Attention(QW<sub>i</sub><sup>Q</sup>, KW<sub>i</sub><sup>K</sup>, VW<sub>i</sub><sup>V</sup>)<br><br>
                • h = number of heads (typically 8 or 16)<br>
                • Each head has its own learned projection matrices<br>
                • Final output combines all head outputs
            </div>
        </div>
        
        <div class="step">
            <h3>🎯 Interactive Multi-Head Explorer</h3>
            
            <div class="interactive-demo">
                <div class="demo-title">👁️ Multi-Head Attention Pattern Analyzer</div>
                
                <div class="controls">
                    <div class="control-group">
                        <label><strong>Image Scene:</strong></label>
                        <select id="sceneType">
                            <option value="cat">Cat Portrait</option>
                            <option value="landscape" selected>Mountain Landscape</option>
                            <option value="street">Street Scene</option>
                            <option value="food">Food Image</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label><strong>ViT Layer:</strong></label>
                        <select id="layerDepth">
                            <option value="early">Early (Layer 2)</option>
                            <option value="middle" selected>Middle (Layer 6)</option>
                            <option value="late">Late (Layer 11)</option>
                        </select>
                    </div>
                </div>
                
                <button onclick="updateMultiHeadVisualization()">🔍 Analyze Attention Heads</button>
                
                <div class="multihead-grid" id="multiHeadGrid"></div>
                
                <div id="headAnalysis" style="margin-top: 20px;"></div>
            </div>
        </div>
    </div>

    <div class="container">
        <h2>🌊 Attention Pattern Evolution Across Layers</h2>
        
        <div class="step">
            <h3>📈 From Local to Global Understanding</h3>
            
            <p>Vision Transformers learn progressively more sophisticated attention patterns as information flows through layers. Early layers focus on local patterns, middle layers discover object parts and spatial relationships, and late layers develop global semantic understanding.</p>
            
            <div class="interactive-demo">
                <div class="demo-title">🔄 Layer-wise Attention Evolution</div>
                
                <div class="layer-selector" id="layerSelector"></div>
                
                <div class="attention-evolution">
                    <div class="evolution-stage">
                        <div class="attention-grid" id="evolutionLayer"></div>
                        <div class="stage-title" id="stageTitle">Select a layer above</div>
                        <div class="stage-description" id="stageDescription">Click layer buttons to see attention patterns</div>
                    </div>
                </div>
                
                <button onclick="animateLayerEvolution()">🎬 Animate Layer Evolution</button>
                <div id="evolutionAnalysis"></div>
            </div>
        </div>
        
        <div class="step">
            <h3>🎯 What Different Layers Learn</h3>
            
            <table>
                <tr>
                    <th>Layer Range</th>
                    <th>Attention Focus</th>
                    <th>Typical Patterns</th>
                    <th>Function</th>
                </tr>
                <tr>
                    <td><strong>Early (0-3)</strong></td>
                    <td>Local neighborhoods</td>
                    <td>Adjacent patches, edges</td>
                    <td>Low-level feature detection</td>
                </tr>
                <tr>
                    <td><strong>Middle (4-8)</strong></td>
                    <td>Object parts</td>
                    <td>Spatially related regions</td>
                    <td>Part-whole relationships</td>
                </tr>
                <tr>
                    <td><strong>Late (9-11)</strong></td>
                    <td>Global semantics</td>
                    <td>Semantic similarity</td>
                    <td>Scene understanding</td>
                </tr>
            </table>
        </div>
    </div>

    <div class="container">
        <h2>🌍 Global Receptive Fields: CNN vs ViT</h2>
        
        <div class="step">
            <h3>⚡ The Global Advantage</h3>
            
            <p>This is where Vision Transformers truly shine. While CNNs gradually expand their receptive field through layers, <strong>ViTs have global receptive fields from layer 1</strong>. Every patch can immediately access information from every other patch in the image.</p>
            
            <div class="interactive-demo">
                <div class="demo-title">🔍 Receptive Field Comparison</div>
                
                <div class="controls">
                    <div class="control-group">
                        <label><strong>Image Size:</strong></label>
                        <input type="range" id="imageSize" min="224" max="512" value="224" step="32">
                        <span id="imageSizeDisplay">224×224</span>
                    </div>
                    <div class="control-group">
                        <label><strong>Animation Speed:</strong></label>
                        <select id="animationSpeed">
                            <option value="slow">Slow</option>
                            <option value="medium" selected>Medium</option>
                            <option value="fast">Fast</option>
                        </select>
                    </div>
                </div>
                
                <button onclick="compareReceptiveFields()">📊 Compare Receptive Fields</button>
                
                <div class="receptive-field-comparison">
                    <div class="rf-visualization">
                        <div class="rf-title">🔄 CNN (ResNet-50)</div>
                        <div class="rf-demo" id="cnnDemo">
                            <div class="layer-visualization">
                                <div class="layer-label">Layer 1</div>
                                <div class="receptive-field-size">
                                    <div class="rf-fill" style="width: 15%;">7×7</div>
                                </div>
                            </div>
                            <div class="layer-visualization">
                                <div class="layer-label">Layer 10</div>
                                <div class="receptive-field-size">
                                    <div class="rf-fill" style="width: 35%;">75×75</div>
                                </div>
                            </div>
                            <div class="layer-visualization">
                                <div class="layer-label">Layer 25</div>
                                <div class="receptive-field-size">
                                    <div class="rf-fill" style="width: 65%;">195×195</div>
                                </div>
                            </div>
                            <div class="layer-visualization">
                                <div class="layer-label">Layer 50</div>
                                <div class="receptive-field-size">
                                    <div class="rf-fill" style="width: 100%;">Full Image</div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="rf-visualization">
                        <div class="rf-title">⚡ ViT (All Layers)</div>
                        <div class="rf-demo" id="vitDemo">
                            <div class="layer-visualization">
                                <div class="layer-label">Layer 1</div>
                                <div class="receptive-field-size">
                                    <div class="rf-fill" style="width: 100%;">Global</div>
                                </div>
                            </div>
                            <div class="layer-visualization">
                                <div class="layer-label">Layer 6</div>
                                <div class="receptive-field-size">
                                    <div class="rf-fill" style="width: 100%;">Global</div>
                                </div>
                            </div>
                            <div class="layer-visualization">
                                <div class="layer-label">Layer 12</div>
                                <div class="receptive-field-size">
                                    <div class="rf-fill" style="width: 100%;">Global</div>
                                </div>
                            </div>
                            <div class="breakthrough-highlight" style="margin: 20px 0; padding: 15px; font-size: 14px;">
                                🚀 Every patch sees the entire image from layer 1!
                            </div>
                        </div>
                    </div>
                </div>
                
                <div id="receptiveFieldAnalysis"></div>
            </div>
        </div>
    </div>

    <div class="container">
        <h2>⚙️ Computational Complexity & Optimization</h2>
        
        <div class="step">
            <h3>📊 The O(N²) Reality</h3>
            
            <p>Attention's global connectivity comes at a cost: <strong>quadratic complexity</strong> in sequence length. For a 224×224 image with 16×16 patches, that's 196 patches, requiring a 196×196 attention matrix per head. Let's analyze the real-world implications.</p>
            
            <div class="math-formula">
                <strong>Complexity Analysis:</strong><br><br>
                • Sequence Length: N = (H × W) / P²<br>
                • Attention Matrix: N × N per head<br>
                • Memory: O(N²) for attention weights<br>
                • Compute: O(N²d + Nd²) per layer<br><br>
                For 224×224 image, 16×16 patches: N = 196
            </div>
        </div>
        
        <div class="step">
            <h3>🧮 Interactive Complexity Calculator</h3>
            
            <div class="interactive-demo">
                <div class="demo-title">📊 Attention Complexity Analyzer</div>
                
                <div class="controls">
                    <div class="control-group">
                        <label><strong>Image Height:</strong></label>
                        <input type="number" id="imageHeight" min="224" max="1024" value="224" step="32">
                    </div>
                    <div class="control-group">
                        <label><strong>Image Width:</strong></label>
                        <input type="number" id="imageWidth" min="224" max="1024" value="224" step="32">
                    </div>
                    <div class="control-group">
                        <label><strong>Patch Size:</strong></label>
                        <select id="patchSize">
                            <option value="8">8×8</option>
                            <option value="16" selected>16×16</option>
                            <option value="32">32×32</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label><strong>Attention Heads:</strong></label>
                        <select id="numHeads">
                            <option value="8">8</option>
                            <option value="12" selected>12</option>
                            <option value="16">16</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label><strong>Model Dimension:</strong></label>
                        <select id="modelDim">
                            <option value="768" selected>768 (Base)</option>
                            <option value="1024">1024 (Large)</option>
                            <option value="1280">1280 (Huge)</option>
                        </select>
                    </div>
                </div>
                
                <button onclick="calculateComplexity()">📊 Calculate Complexity</button>
                
                <div class="complexity-results" id="complexityResults"></div>
                
                <div id="optimizationSuggestions" style="margin-top: 20px;"></div>
            </div>
        </div>
    </div>

    <div class="container">
        <h2>🔬 Attention Interpretability & Debugging</h2>
        
        <div class="step">
            <h3>👁️ Seeing Through the Model's Eyes</h3>
            
            <p>One of the most powerful aspects of attention mechanisms is their <strong>interpretability</strong>. Unlike CNN feature maps, attention weights directly show us what the model is "looking at" when making decisions.</p>
            
            <div class="visual-comparison">
                <div class="visual-card">
                    <div class="visual-title">🔍 Attention Visualization Methods</div>
                    <div class="visual-diagram">
<strong>Raw Attention:</strong><br>
• Direct attention weights<br>
• Per head, per layer<br>
• Shows immediate focus<br><br>
<strong>Attention Rollout:</strong><br>
• Aggregated across layers<br>
• End-to-end attention flow<br>
• Final decision pathway<br><br>
<strong>Attention Flow:</strong><br>
• Information propagation<br>
• Layer-by-layer evolution<br>
• Dynamic attention changes
                    </div>
                </div>
                
                <div class="visual-card">
                    <div class="visual-title">⚠️ Common Attention Patterns</div>
                    <div class="visual-diagram">
<strong>Good Patterns:</strong><br>
• Object-focused attention<br>
• Semantic relationships<br>
• Contextual dependencies<br><br>
<strong>Problematic Patterns:</strong><br>
• Attention collapse<br>
• Uniform attention<br>
• Background fixation<br>
• Spurious correlations
                    </div>
                </div>
            </div>
        </div>
        
        <div class="step">
            <h3>🐛 Attention Failure Modes</h3>
            
            <div class="warning">
                <strong>⚠️ Common Issues:</strong><br><br>
                <strong>1. Attention Collapse:</strong> All heads learn similar patterns, reducing model capacity<br>
                <strong>2. Uniform Attention:</strong> Model pays equal attention to all patches, losing focus<br>
                <strong>3. Background Bias:</strong> Over-attention to irrelevant background features<br>
                <strong>4. Spurious Correlations:</strong> Attention to dataset artifacts rather than meaningful features
            </div>
            
            <div class="success">
                <strong>🔧 Debugging Strategies:</strong><br>
                • Visualize attention maps across different layers and heads<br>
                • Check for head diversity using attention distance metrics<br>
                • Monitor attention entropy (uniform = high entropy)<br>
                • Validate attention patterns align with human expectations<br>
                • Test on out-of-distribution images to check robustness
            </div>
        </div>
    </div>

    <div class="container">
        <h2>🚀 Production Considerations & Optimizations</h2>
        
        <div class="step">
            <h3>⚡ Real-World Performance Challenges</h3>
            
            <p>Moving from research to production requires careful attention to computational constraints. Here are key optimization strategies for deploying attention-based vision models at scale.</p>
            
            <table>
                <tr>
                    <th>Optimization</th>
                    <th>Memory Savings</th>
                    <th>Speed Improvement</th>
                    <th>Accuracy Impact</th>
                </tr>
                <tr>
                    <td><strong>Mixed Precision</strong></td>
                    <td>~50%</td>
                    <td>1.5-2x</td>
                    <td>Minimal</td>
                </tr>
                <tr>
                    <td><strong>Gradient Checkpointing</strong></td>
                    <td>~75%</td>
                    <td>0.8x (slower)</td>
                    <td>None</td>
                </tr>
                <tr>
                    <td><strong>Attention Sparsity</strong></td>
                    <td>10-30%</td>
                    <td>1.2-1.5x</td>
                    <td>Small</td>
                </tr>
                <tr>
                    <td><strong>Linear Attention</strong></td>
                    <td>~60%</td>
                    <td>2-3x</td>
                    <td>Moderate</td>
                </tr>
            </table>
        </div>
        
        <div class="step">
            <h3>🎯 Key Takeaways & Next Steps</h3>
            
            <div class="success">
                <strong>🧠 What You've Mastered:</strong><br>
                • Complete mathematics of visual attention mechanisms<br>
                • Why multi-head attention enables diverse pattern recognition<br>
                • How global receptive fields revolutionize vision understanding<br>
                • Computational complexity and real-world optimization strategies<br>
                • Attention interpretability for model debugging and validation<br>
                • Production deployment considerations for attention-based models
            </div>
            
            <div class="info">
                <strong>🚀 Ready for Next Level:</strong> Now that you understand attention - the core mechanism of transformers - you're ready to explore how this enables groundbreaking applications like CLIP (vision-language understanding), DALL-E (text-to-image generation), and modern multimodal AI systems.
            </div>
        </div>
    </div>

    <script>
        // Global state
        let selectedPatch = null;
        let currentLayer = 0;
        
        // Sample data for demonstrations
        const patchLabels = [
            'sky', 'sky', 'cloud', 'mountain',
            'sky', 'mountain', 'mountain', 'tree',
            'tree', 'mountain', 'tree', 'grass',
            'grass', 'grass', 'rock', 'grass'
        ];
        
        const attentionHeads = [
            { name: 'Position Head', pattern: 'Focuses on spatial neighbors', color: '#ff6b6b' },
            { name: 'Edge Head', pattern: 'Detects object boundaries', color: '#4ecdc4' },
            { name: 'Color Head', pattern: 'Groups similar colors', color: '#45b7d1' },
            { name: 'Texture Head', pattern: 'Similar surface patterns', color: '#96ceb4' },
            { name: 'Semantic Head', pattern: 'Object-level understanding', color: '#feca57' },
            { name: 'Context Head', pattern: 'Scene-level relationships', color: '#ff9ff3' },
            { name: 'Global Head', pattern: 'Uniform global attention', color: '#54a0ff' },
            { name: 'Boundary Head', pattern: 'Object-background separation', color: '#5f27cd' }
        ];
        
        const layerPatterns = [
            { layer: 0, name: 'Early', description: 'Local neighborhoods, edge detection', pattern: 'local' },
            { layer: 2, name: 'Early-Mid', description: 'Adjacent patches, textures', pattern: 'adjacent' },
            { layer: 4, name: 'Middle', description: 'Object parts, spatial groups', pattern: 'parts' },
            { layer: 6, name: 'Mid-Late', description: 'Object boundaries, shapes', pattern: 'objects' },
            { layer: 8, name: 'Late', description: 'Semantic relationships', pattern: 'semantic' },
            { layer: 11, name: 'Final', description: 'Global scene understanding', pattern: 'global' }
        ];
        
        // Initialize components
        function initializeComponents() {
            initializePatchGrid();
            initializeMatrixVisualization();
            initializeLayerSelector();
            updateSliderDisplays();
            calculateComplexity();
            updateMultiHeadVisualization();
        }
        
        function initializePatchGrid() {
            const grid = document.getElementById('patchGrid');
            grid.innerHTML = '';
            
            patchLabels.forEach((label, i) => {
                const patch = document.createElement('div');
                patch.className = 'patch';
                patch.textContent = label.substring(0, 3);
                patch.title = label;
                patch.onclick = () => selectPatch(i);
                grid.appendChild(patch);
            });
        }
        
        function selectPatch(index) {
            selectedPatch = index;
            
            // Update visual selection
            document.querySelectorAll('.patch').forEach((patch, i) => {
                patch.classList.remove('selected');
                if (i === index) {
                    patch.classList.add('selected');
                }
            });
            
            document.getElementById('selectedPatch').textContent = patchLabels[index];
            
            // Update attention visualization
            updateAttentionVisualization();
        }
        
        function updateAttentionVisualization() {
            if (selectedPatch === null) return;
            
            const temperature = parseFloat(document.getElementById('temperature').value);
            const mode = document.getElementById('visualMode').value;
            
            // Generate attention weights (simplified simulation)
            const attentionWeights = generateAttentionWeights(selectedPatch, temperature);
            
            updateAttentionHeatmap(attentionWeights, mode);
            updateComputationSteps(attentionWeights, temperature);
        }
        
        function generateAttentionWeights(queryPatch, temperature) {
            const weights = [];
            const queryLabel = patchLabels[queryPatch];
            
            patchLabels.forEach((label, i) => {
                let similarity = Math.random() * 0.3 + 0.1; // Base similarity
                
                // Add semantic similarity boost
                if (label === queryLabel) similarity += 0.8;
                else if (isSimilarCategory(label, queryLabel)) similarity += 0.4;
                
                // Add spatial proximity boost
                const spatialDistance = getSpatialDistance(queryPatch, i);
                similarity += Math.max(0, 0.3 - spatialDistance * 0.1);
                
                weights.push(similarity);
            });
            
            // Apply temperature scaling
            const scaledWeights = weights.map(w => w / temperature);
            
            // Apply softmax
            const maxWeight = Math.max(...scaledWeights);
            const expWeights = scaledWeights.map(w => Math.exp(w - maxWeight));
            const sumExp = expWeights.reduce((a, b) => a + b, 0);
            const softmaxWeights = expWeights.map(w => w / sumExp);
            
            return {
                raw: weights,
                scaled: scaledWeights,
                softmax: softmaxWeights
            };
        }
        
        function isSimilarCategory(label1, label2) {
            const categories = [
                ['sky', 'cloud'],
                ['mountain', 'rock'],
                ['tree', 'grass'],
                ['grass', 'tree']
            ];
            
            return categories.some(cat => 
                cat.includes(label1) && cat.includes(label2) && label1 !== label2
            );
        }
        
        function getSpatialDistance(patch1, patch2) {
            const row1 = Math.floor(patch1 / 4);
            const col1 = patch1 % 4;
            const row2 = Math.floor(patch2 / 4);
            const col2 = patch2 % 4;
            
            return Math.sqrt((row1 - row2) ** 2 + (col1 - col2) ** 2);
        }
        
        function updateAttentionHeatmap(weights, mode) {
            const heatmap = document.getElementById('attentionHeatmap');
            heatmap.innerHTML = '';
            
            const values = mode === 'similarity' ? weights.scaled : 
                          mode === 'attention' ? weights.softmax : weights.softmax;
            
            const maxVal = Math.max(...values);
            const minVal = Math.min(...values);
            
            values.forEach((weight, i) => {
                const cell = document.createElement('div');
                cell.className = 'attention-cell';
                
                const intensity = (weight - minVal) / (maxVal - minVal);
                const color = `rgba(${Math.round(255 * intensity)}, ${Math.round(100 * (1-intensity))}, ${Math.round(50 * (1-intensity))}, 0.8)`;
                cell.style.backgroundColor = color;
                
                cell.textContent = weight.toFixed(2);
                cell.title = `${patchLabels[i]}: ${weight.toFixed(3)}`;
                
                heatmap.appendChild(cell);
            });
        }
        
        function updateComputationSteps(weights, temperature) {
            const steps = document.getElementById('computationSteps');
            steps.innerHTML = `
                <h5>🔍 Computation Steps:</h5>
                <div style="font-size: 12px; background: white; padding: 15px; border-radius: 6px; border: 1px solid #dee2e6;">
                    <p><strong>1. Query×Key:</strong> Compute similarity scores</p>
                    <p><strong>2. Scale by √d<sub>k</sub>:</strong> Divide by temperature (${temperature.toFixed(1)})</p>
                    <p><strong>3. Softmax:</strong> Normalize to probabilities</p>
                    <p><strong>4. Weighted Sum:</strong> Aggregate value vectors</p>
                    <div style="margin-top: 10px; padding: 8px; background: #f8f9fa; border-radius: 4px;">
                        <strong>Max attention:</strong> ${Math.max(...weights.softmax).toFixed(3)} → ${patchLabels[weights.softmax.indexOf(Math.max(...weights.softmax))]}
                    </div>
                </div>
            `;
        }
        
        function initializeMatrixVisualization() {
            // Initialize Q, K, V matrices with random values
            ['queryMatrix', 'keyMatrix', 'valueMatrix'].forEach(matrixId => {
                const matrix = document.getElementById(matrixId);
                matrix.innerHTML = '';
                
                for (let i = 0; i < 16; i++) {
                    const cell = document.createElement('div');
                    cell.className = 'matrix-cell';
                    cell.textContent = (Math.random() * 2 - 1).toFixed(1);
                    matrix.appendChild(cell);
                }
            });
        }
        
    function animateAttentionComputation() {
       if (selectedPatch === null) {
           alert('Please select a patch first!');
           return;
       }
       
       const steps = [
           'Computing Q×K^T similarities...',
           'Scaling by temperature...',
           'Applying softmax normalization...',
           'Performing weighted aggregation...'
       ];
       
       const temperature = parseFloat(document.getElementById('temperature').value);
       const attentionWeights = generateAttentionWeights(selectedPatch, temperature);
       let stepIndex = 0;
       
       function animateStep() {
           if (stepIndex < steps.length) {
               document.getElementById('attentionResults').innerHTML = `
                   <div class="info" style="animation: attention-pulse 1s ease-in-out;">
                       <strong>Step ${stepIndex + 1}:</strong> ${steps[stepIndex]}
                   </div>
               `;
               
               // Add visual effects to matrices
               document.getElementById('matrixVisualization').classList.add('matrix-animation');
               
               // Update attention visualization based on current step
               const heatmap = document.getElementById('attentionHeatmap');
               const cells = heatmap.querySelectorAll('.attention-cell');
               
               // Animate attention cells based on step
               cells.forEach((cell, i) => {
                   cell.classList.add('attention-pulse');
                   cell.style.transform = 'scale(1.1)';
                   
                   setTimeout(() => {
                       cell.classList.remove('attention-pulse');
                       cell.style.transform = 'scale(1)';
                   }, 800);
               });
               
               // Show step-specific visualization
               setTimeout(() => {
                   let values, mode;
                   switch(stepIndex) {
                       case 0: // Q×K^T similarities
                           values = attentionWeights.raw;
                           mode = 'Raw Similarities';
                           break;
                       case 1: // Temperature scaling
                           values = attentionWeights.scaled;
                           mode = 'Temperature Scaled';
                           break;
                       case 2: // Softmax
                           values = attentionWeights.softmax;
                           mode = 'Softmax Normalized';
                           break;
                       case 3: // Final output
                           values = attentionWeights.softmax;
                           mode = 'Final Attention';
                           break;
                   }
                   
                   updateAttentionHeatmapWithValues(values, mode);
                   document.getElementById('matrixVisualization').classList.remove('matrix-animation');
                   stepIndex++;
                   setTimeout(animateStep, 1000);
               }, 1500);
           } else {
               document.getElementById('attentionResults').innerHTML = `
                   <div class="success">
                       <strong>✅ Attention computation complete!</strong><br>
                       The selected patch now has an updated representation incorporating information from all other patches, weighted by their attention scores. This is how transformers achieve global understanding!
                   </div>
               `;
           }
       }
       
       animateStep();
    }
    
    function updateAttentionHeatmapWithValues(values, mode) {
       const heatmap = document.getElementById('attentionHeatmap');
       heatmap.innerHTML = '';
       
       const maxVal = Math.max(...values);
       const minVal = Math.min(...values);
       
       values.forEach((weight, i) => {
           const cell = document.createElement('div');
           cell.className = 'attention-cell';
           
           const intensity = (weight - minVal) / (maxVal - minVal);
           const color = `rgba(${Math.round(255 * intensity)}, ${Math.round(100 * (1-intensity))}, ${Math.round(50 * (1-intensity))}, 0.8)`;
           cell.style.backgroundColor = color;
           
           cell.textContent = weight.toFixed(2);
           cell.title = `${patchLabels[i]}: ${weight.toFixed(3)} (${mode})`;
           
           heatmap.appendChild(cell);
       });
       
       // Update computation steps display
       const steps = document.getElementById('computationSteps');
       steps.innerHTML = `
           <h5>🔍 Current Step: ${mode}</h5>
           <div style="font-size: 12px; background: white; padding: 15px; border-radius: 6px; border: 1px solid #dee2e6;">
               <p><strong>Displaying:</strong> ${mode}</p>
               <p><strong>Values range:</strong> ${minVal.toFixed(3)} to ${maxVal.toFixed(3)}</p>
               <div style="margin-top: 10px; padding: 8px; background: #f8f9fa; border-radius: 4px;">
                   <strong>Max attention:</strong> ${Math.max(...values).toFixed(3)} → ${patchLabels[values.indexOf(Math.max(...values))]}
               </div>
           </div>
       `;
    }
        
        function compareReceptiveFields() {
            const imageSize = parseInt(document.getElementById('imageSize').value);
            const speed = document.getElementById('animationSpeed').value;
            
            const speedMap = { slow: 2000, medium: 1000, fast: 500 };
            const animationDuration = speedMap[speed];
            
            // Update display
            document.getElementById('imageSizeDisplay').textContent = `${imageSize}×${imageSize}`;
            
            // Animate CNN receptive field growth
            const cnnLayers = document.querySelectorAll('#cnnDemo .rf-fill');
            const vitLayers = document.querySelectorAll('#vitDemo .rf-fill');
            
            // Reset animations
            cnnLayers.forEach(layer => layer.style.width = '0%');
            
            // Animate CNN growth
            setTimeout(() => {
                cnnLayers.forEach((layer, i) => {
                    setTimeout(() => {
                        const widths = [15, 35, 65, 100];
                        layer.style.width = `${widths[i]}%`;
                        layer.style.transition = `width ${animationDuration/1000}s ease`;
                    }, i * animationDuration);
                });
            }, 500);
            
            // Show analysis
            setTimeout(() => {
                const patchSize = 16; // Assume 16x16 patches
                const numPatches = (imageSize / patchSize) ** 2;
                const attentionMatrixSize = numPatches * numPatches;
                
                document.getElementById('receptiveFieldAnalysis').innerHTML = `
                    <div class="step">
                        <h4>📊 Receptive Field Analysis</h4>
                        <table>
                            <tr><th>Architecture</th><th>Layer 1 RF</th><th>Final RF</th><th>Global Access</th></tr>
                            <tr>
                                <td><strong>CNN (ResNet-50)</strong></td>
                                <td>7×7 pixels</td>
                                <td>Full image (layer 50)</td>
                                <td>❌ Gradual</td>
                            </tr>
                            <tr>
                                <td><strong>ViT (All layers)</strong></td>
                                <td>Full image</td>
                                <td>Full image</td>
                                <td>✅ Immediate</td>
                            </tr>
                        </table>
                        
                        <div class="info">
                            <strong>🎯 Key Advantage:</strong> ViT's global receptive field from layer 1 enables immediate access to long-range dependencies, while CNNs must build up context gradually through many layers.
                            <br><br>
                            <strong>💾 Memory Cost:</strong> ${numPatches} patches × ${numPatches} patches = ${attentionMatrixSize.toLocaleString()} attention weights per head
                        </div>
                    </div>
                `;
            }, cnnLayers.length * animationDuration + 1000);
        }
        
        function calculateComplexity() {
            const height = parseInt(document.getElementById('imageHeight').value);
            const width = parseInt(document.getElementById('imageWidth').value);
            const patchSize = parseInt(document.getElementById('patchSize').value);
            const numHeads = parseInt(document.getElementById('numHeads').value);
            const modelDim = parseInt(document.getElementById('modelDim').value);
            
            // Calculate sequence length
            const numPatchesH = Math.floor(height / patchSize);
            const numPatchesW = Math.floor(width / patchSize);
            const sequenceLength = numPatchesH * numPatchesW;
            
            // Calculate complexity metrics
            const attentionMatrixSize = sequenceLength * sequenceLength;
            const totalAttentionWeights = attentionMatrixSize * numHeads;
            const memoryMB = (totalAttentionWeights * 4) / (1024 * 1024); // 4 bytes per float32
            const flopsPerLayer = (sequenceLength ** 2) * modelDim + sequenceLength * (modelDim ** 2);
            
            // Display results
            const resultsDiv = document.getElementById('complexityResults');
            resultsDiv.innerHTML = `
                <div class="complexity-metric">
                    <div class="metric-value">${sequenceLength}</div>
                    <div class="metric-label">Sequence Length</div>
                </div>
                <div class="complexity-metric">
                    <div class="metric-value">${attentionMatrixSize.toLocaleString()}</div>
                    <div class="metric-label">Attention Matrix Size</div>
                </div>
                <div class="complexity-metric">
                    <div class="metric-value">${memoryMB.toFixed(1)} MB</div>
                    <div class="metric-label">Attention Memory</div>
                </div>
                <div class="complexity-metric">
                    <div class="metric-value">${(flopsPerLayer / 1e9).toFixed(2)} GFLOPs</div>
                    <div class="metric-label">Compute per Layer</div>
                </div>
            `;
            
            // Optimization suggestions
            let suggestions = '';
            if (memoryMB > 100) {
                suggestions += '⚠️ High memory usage - consider gradient checkpointing<br>';
            }
            if (sequenceLength > 500) {
                suggestions += '⚠️ Long sequence - linear attention might help<br>';
            }
            if (patchSize < 16) {
                suggestions += '💡 Smaller patches increase quality but quadratic cost<br>';
            }
            
            document.getElementById('optimizationSuggestions').innerHTML = suggestions ? `
                <div class="warning">
                    <strong>🔧 Optimization Suggestions:</strong><br>
                    ${suggestions}
                </div>
            ` : `
                <div class="success">
                    <strong>✅ Reasonable complexity</strong> - This configuration should work well for most hardware.
                </div>
            `;
        }
        
        function updateSliderDisplays() {
            const temperature = document.getElementById('temperature');
            const temperatureValue = document.getElementById('temperatureValue');
            
            if (temperature && temperatureValue) {
                temperature.addEventListener('input', () => {
                    temperatureValue.textContent = temperature.value;
                    updateAttentionVisualization();
                });
            }
            
            const imageSize = document.getElementById('imageSize');
            const imageSizeDisplay = document.getElementById('imageSizeDisplay');
            
            if (imageSize && imageSizeDisplay) {
                imageSize.addEventListener('input', () => {
                    imageSizeDisplay.textContent = `${imageSize.value}×${imageSize.value}`;
                });
            }
        }
        
        // Event listeners
        document.addEventListener('DOMContentLoaded', function() {
            initializeComponents();
            
            // Add event listeners for dropdowns
            ['visualMode', 'sceneType', 'layerDepth'].forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.addEventListener('change', () => {
                        if (id === 'visualMode') updateAttentionVisualization();
                        else if (id === 'sceneType' || id === 'layerDepth') updateMultiHeadVisualization();
                    });
                }
            });
            
            // Add event listeners for complexity calculator
            ['imageHeight', 'imageWidth', 'patchSize', 'numHeads', 'modelDim'].forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.addEventListener('change', calculateComplexity);
                    element.addEventListener('input', calculateComplexity);
                }
            });
            
            // Initialize with first patch selected
            selectPatch(0);
        });
        
        function updateMultiHeadVisualization() {
            const grid = document.getElementById('multiHeadGrid');
            grid.innerHTML = '';
            
            const scene = document.getElementById('sceneType').value;
            const layer = document.getElementById('layerDepth').value;
            
            attentionHeads.forEach((head, i) => {
                const card = document.createElement('div');
                card.className = 'head-card';
                card.onclick = () => selectAttentionHead(i);
                
                card.innerHTML = `
                    <div class="head-title">${head.name}</div>
                    <div class="attention-grid" style="transform: scale(0.6); margin: 5px auto;">
                        ${generateHeadVisualization(head, scene, layer)}
                    </div>
                    <div class="head-pattern">${head.pattern}</div>
                `;
                
                grid.appendChild(card);
            });
        }
        
        function generateHeadVisualization(head, scene, layer) {
            let html = '';
            for (let i = 0; i < 16; i++) {
                const intensity = getHeadIntensity(head.name, i, scene, layer);
                const color = `rgba(${parseInt(head.color.slice(1, 3), 16)}, ${parseInt(head.color.slice(3, 5), 16)}, ${parseInt(head.color.slice(5, 7), 16)}, ${intensity})`;
                html += `<div class="patch" style="background: ${color}; font-size: 8px;">${Math.round(intensity * 100)}</div>`;
            }
            return html;
        }
        
        function getHeadIntensity(headName, patchIndex, scene, layer) {
            // Simulate different attention patterns based on head type
            const baseIntensity = Math.random() * 0.3 + 0.1;
            
            if (headName.includes('Position')) {
                // Position heads focus on spatial neighbors
                const center = Math.floor(Math.random() * 16);
                const distance = getSpatialDistance(center, patchIndex);
                return Math.max(0.1, 0.9 - distance * 0.2);
            } else if (headName.includes('Edge')) {
                // Edge heads focus on boundaries
                const row = Math.floor(patchIndex / 4);
                const col = patchIndex % 4;
                return (row === 0 || row === 3 || col === 0 || col === 3) ? 0.8 : 0.2;
            } else if (headName.includes('Global')) {
                // Global heads have uniform attention
                return 0.25;
            } else {
                // Other heads have varied patterns
                return baseIntensity + Math.sin(patchIndex) * 0.3 + 0.3;
            }
        }
        
        function selectAttentionHead(index) {
            document.querySelectorAll('.head-card').forEach((card, i) => {
                card.classList.remove('selected');
                if (i === index) {
                    card.classList.add('selected');
                }
            });
            
            const head = attentionHeads[index];
            document.getElementById('headAnalysis').innerHTML = `
                <div class="info">
                    <strong>🎯 ${head.name} Analysis:</strong><br>
                    <strong>Specialization:</strong> ${head.pattern}<br>
                    <strong>Function:</strong> ${getHeadFunction(head.name)}<br>
                    <strong>Typical Use:</strong> ${getHeadUseCase(head.name)}
                </div>
            `;
        }
        
        function getHeadFunction(headName) {
            const functions = {
                'Position Head': 'Maintains spatial relationships and locality biases',
                'Edge Head': 'Detects boundaries between different regions or objects',
                'Color Head': 'Groups patches with similar visual appearance',
                'Texture Head': 'Identifies similar surface patterns and materials',
                'Semantic Head': 'Associates semantically related image regions',
                'Context Head': 'Provides broad scene context and global understanding',
                'Global Head': 'Aggregates information uniformly across the image',
                'Boundary Head': 'Separates foreground objects from background'
            };
            return functions[headName] || 'Specialized pattern recognition';
        }
        
        function getHeadUseCase(headName) {
            const useCases = {
                'Position Head': 'Preserving object structure and spatial coherence',
                'Edge Head': 'Object detection and segmentation tasks',
                'Color Head': 'Scene classification and visual similarity matching',
                'Texture Head': 'Material recognition and surface analysis',
                'Semantic Head': 'Object recognition and classification',
                'Context Head': 'Scene understanding and contextual reasoning',
                'Global Head': 'Feature pooling and global representations',
                'Boundary Head': 'Instance segmentation and object extraction'
            };
            return useCases[headName] || 'General vision tasks';
        }
        
        function initializeLayerSelector() {
            const selector = document.getElementById('layerSelector');
            selector.innerHTML = '';
            
            layerPatterns.forEach((pattern, i) => {
                const button = document.createElement('div');
                button.className = 'layer-button';
                button.textContent = `L${pattern.layer}`;
                button.title = `${pattern.name}: ${pattern.description}`;
                button.onclick = () => selectLayer(i);
                
                if (i === 0) button.classList.add('active');
                
                selector.appendChild(button);
            });
            
            selectLayer(0);
        }
        
        function selectLayer(index) {
            document.querySelectorAll('.layer-button').forEach((btn, i) => {
                btn.classList.remove('active');
                if (i === index) {
                    btn.classList.add('active');
                }
            });
            
            const pattern = layerPatterns[index];
            currentLayer = index;
            
            updateLayerVisualization(pattern);
        }
        
        function updateLayerVisualization(pattern) {
            const grid = document.getElementById('evolutionLayer');
            grid.innerHTML = '';
            
            for (let i = 0; i < 16; i++) {
                const patch = document.createElement('div');
                patch.className = 'patch';
                
                const intensity = getLayerAttentionPattern(pattern.pattern, i);
                const color = `rgba(40, 167, 69, ${intensity})`;
                patch.style.background = color;
                patch.style.color = intensity > 0.5 ? 'white' : '#2d2d2d';
                patch.textContent = Math.round(intensity * 100);
                
                grid.appendChild(patch);
            }
            
            document.getElementById('stageTitle').textContent = `Layer ${pattern.layer}: ${pattern.name}`;
            document.getElementById('stageDescription').textContent = pattern.description;
        }
        
        function getLayerAttentionPattern(patternType, patchIndex) {
            const row = Math.floor(patchIndex / 4);
            const col = patchIndex % 4;
            
            switch (patternType) {
                case 'local':
                    // Early layers: local attention
                    return Math.max(0.1, 0.8 - getSpatialDistance(6, patchIndex) * 0.3);
                case 'adjacent':
                    // Adjacent patches
                    const center = 5;
                    const dist = getSpatialDistance(center, patchIndex);
                    return dist <= 1 ? 0.9 : 0.2;
                case 'parts':
                    // Object parts (simulate with clusters)
                    return (row < 2 && col < 2) || (row >= 2 && col >= 2) ? 0.8 : 0.3;
                case 'objects':
                    // Object-level attention
                    return col < 2 ? 0.7 : 0.4;
                case 'semantic':
                    // Semantic relationships
                    return patchIndex % 2 === 0 ? 0.6 : 0.8;
                case 'global':
                    // Global uniform attention
                    return 0.5 + Math.random() * 0.2;
                default:
                    return Math.random() * 0.5 + 0.3;
            }
        }
        
        function animateLayerEvolution() {
            let currentIndex = 0;
            
            function animateNextLayer() {
                if (currentIndex < layerPatterns.length) {
                    selectLayer(currentIndex);
                    document.getElementById('evolutionAnalysis').innerHTML = `
                        <div class="info" style="animation: attention-pulse 1s ease-in-out;">
                            <strong>Layer ${layerPatterns[currentIndex].layer}:</strong> ${layerPatterns[currentIndex].description}
                        </div>
                    `;
                    currentIndex++;
                    setTimeout(animateNextLayer, 2000);
                } else {
                    document.getElementById('evolutionAnalysis').innerHTML = `
                        <div class="success">
                            <strong>✅ Layer evolution complete!</strong><br>
                            Notice how attention patterns evolve from local neighborhoods in early layers to global semantic understanding in later layers. This progressive refinement is key to ViT's success.
                        </div>
                    `;
                }
            }
            
            animateNextLayer();
        }
        
        // Event listeners
        document.addEventListener('DOMContentLoaded', function() {
            initializeComponents();
            
            // Add event listeners for dropdowns
            ['visualMode', 'sceneType', 'layerDepth'].forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.addEventListener('change', () => {
                        if (id === 'visualMode') updateAttentionVisualization();
                        else if (id === 'sceneType' || id === 'layerDepth') updateMultiHeadVisualization();
                    });
                }
            });
            
            // Add event listeners for complexity calculator
            ['imageHeight', 'imageWidth', 'patchSize', 'numHeads', 'modelDim'].forEach(id => {
                const element = document.getElementById(id);
                if (element) {
                    element.addEventListener('change', calculateComplexity);
                    element.addEventListener('input', calculateComplexity);
                }
            });
            
            // Initialize with first patch selected
            selectPatch(0);
        });
    </script>
</body>
</html>
